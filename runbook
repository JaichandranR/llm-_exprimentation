import unittest
import sys
import types
from unittest.mock import patch, MagicMock, mock_open

# --- Full mock of awsglue and pyspark module tree ---
awsglue_mock = types.ModuleType("awsglue")
context_mock = types.ModuleType("awsglue.context")
utils_mock = types.ModuleType("awsglue.utils")
pyspark_context_mock = types.ModuleType("pyspark.context")

# Mock necessary classes/functions
context_mock.GlueContext = MagicMock()
pyspark_context_mock.SparkContext = MagicMock()
utils_mock.getResolvedOptions = MagicMock(return_value={
    "JOB_NAME": "dummy_job",
    "raw_bucket": "dummy",
    "dq_bucket": "dummy",
    "output_bucket": "dummy",
    "base_dir": "dummy",
    "data_set": "dummy",
    "history_data_set": "dummy",
    "ignore_threshold": "false",
    "threshold_info": "none",
    "schema_file_name": "dummy",
    "TempDir": "/tmp",
    "multiple_source_dataset": "false"
})

# Inject mocks
sys.modules["awsglue"] = awsglue_mock
sys.modules["awsglue.context"] = context_mock
sys.modules["awsglue.utils"] = utils_mock
sys.modules["pyspark.context"] = pyspark_context_mock

# --- Actual test cases ---
from src.main.python.wis_etl import (
    get_path_for_data_set,
    configure_spark,
    do_worker_etl,
    do_job_etl,
    do_local_cost_center_etl,
    do_corporate_hierarchy_etl,
)

class TestDoCorporateHierarchyEtl(unittest.TestCase):
    @patch("builtins.open", new_callable=mock_open, read_data='{}')
    @patch("src.main.python.wis_etl.perform_full_load_etl")
    def test_do_corporate_hierarchy_etl(self, mock_perform_etl, mock_file):
        mock_spark = MagicMock()
        mock_perform_etl.return_value = "etl_result"

        result = do_corporate_hierarchy_etl(
            mock_spark, "source_dir", "target_dir", "history_dir", "dq_dir", "schema_loc_info"
        )

        mock_perform_etl.assert_called()
        self.assertEqual(result, "etl_result")

class TestDoLocalCostCenterEtl(unittest.TestCase):
    @patch("builtins.open", new_callable=mock_open, read_data='{}')
    @patch("src.main.python.wis_etl.perform_full_load_etl")
    def test_do_local_cost_center_etl(self, mock_perform_etl, mock_file):
        mock_spark = MagicMock()
        mock_perform_etl.return_value = "etl_result"

        result = do_local_cost_center_etl(
            mock_spark, "source_dir", "target_dir", "history_dir", "dq_dir", "schema_loc_info"
        )

        mock_perform_etl.assert_called()
        self.assertEqual(result, "etl_result")

class TestDoJobEtl(unittest.TestCase):
    @patch("builtins.open", new_callable=mock_open, read_data='{}')
    @patch("src.main.python.wis_etl.perform_full_load_etl")
    def test_do_job_etl(self, mock_perform_etl, mock_file):
        mock_spark = MagicMock()
        mock_perform_etl.return_value = "etl_result"

        result = do_job_etl(
            mock_spark, "source_dir", "target_dir", "history_dir", "dq_dir", "schema_loc_info"
        )

        mock_perform_etl.assert_called()
        self.assertEqual(result, "etl_result")

class TestDoWorkerEtl(unittest.TestCase):
    @patch("builtins.open", new_callable=mock_open, read_data='{}')
    @patch("src.main.python.wis_etl.get_path_for_data_set")
    @patch("src.main.python.wis_etl.perform_full_load_etl")
    def test_do_worker_etl(self, mock_perform_etl, mock_get_path, mock_file):
        mock_spark = MagicMock()
        mock_get_path.return_value = "s3://dummy-path"
        mock_perform_etl.return_value = "etl_result"

        result = do_worker_etl(
            mock_spark, "source_dir", "target_dir", "history_dir", "dq_dir", "schema_loc_info"
        )

        mock_get_path.assert_called()
        mock_perform_etl.assert_called()
        self.assertIsNotNone(result)

class TestConfigureSpark(unittest.TestCase):
    @patch("src.main.python.wis_etl.SparkSession")
    def test_configure_spark(self, mock_spark_session):
        mock_builder = MagicMock()
        mock_spark_session.builder.appName.return_value = mock_builder
        mock_builder.getOrCreate.return_value = MagicMock()

        result = configure_spark("test_dataset", verbose=True)

        mock_spark_session.builder.appName.assert_called()
        self.assertIsNotNone(result)

if __name__ == '__main__':
    unittest.main()
