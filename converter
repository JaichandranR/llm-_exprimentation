import sys
import traceback
from pyspark.sql import SparkSession

# -------- Configurable Inputs --------
catalog_nm = "cosmos_nonhcd_iceberg"
output_bucket = "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/105130_tpc_products/"
db_name = "common_data"
table_name = "105130_tpc_products"

# -------- Spark Session Setup --------
try:
    spark = (
        SparkSession.builder
        .appName("IcebergPropertySetTest")
        .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config(f"spark.sql.catalog.{catalog_nm}.warehouse", output_bucket)
        .getOrCreate()
    )
    print("✅ Spark session created.")
except Exception as e:
    print("❌ Spark session creation failed.")
    print(traceback.format_exc())
    sys.exit(1)

# -------- Read Table via Spark SQL --------
full_table = f"{catalog_nm}.{db_name}.{table_name}"
try:
    df = spark.sql(f"SELECT * FROM {full_table} LIMIT 5")
    df.show()
    print("✅ Table read successfully.")
except Exception as e:
    print("❌ Failed to read table via SQL:")
    print(traceback.format_exc())
    sys.exit(1)

# -------- Write Table with TBLPROPERTIES --------
try:
    df.writeTo(full_table) \
        .using("iceberg") \
        .tableProperty("write.metadata.relative-path", "true") \
        .createOrReplace()
    print("✅ TBLPROPERTIES updated successfully.")
except Exception as e:
    print("❌ Failed to update Iceberg table properties:")
    print(traceback.format_exc())
    sys.exit(1)

spark.stop()
