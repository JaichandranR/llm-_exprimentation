import sys
import traceback
from pyspark.sql import SparkSession

# Configurable parameters
catalog_nm = "cosmos_nonhcd_iceberg"
warehouse_path = "s3://your-bucket/warehouse/"  # Replace with your actual Iceberg root path
db_name = "common_data"
table_name = "105130_tpc_products"

# Compose full table name
full_table_path = f"{catalog_nm}.{db_name}.{table_name}"

try:
    # Start Spark session with Iceberg config
    spark = (
        SparkSession.builder
        .appName("IcebergPropertyUpdatePOC")
        .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config(f"spark.sql.catalog.{catalog_nm}.warehouse", warehouse_path)
        .getOrCreate()
    )

    print("✅ Spark session created successfully.")

    # Read Iceberg table
    df = spark.read.format("iceberg").load(full_table_path)
    df.show(5)
    print("✅ Iceberg table read successfully.")

    # Update table property
    spark.sql(
        f"""
        ALTER TABLE {full_table_path}
        SET TBLPROPERTIES ('write.metadata.relative-path' = 'true')
        """
    )
    print(f"✅ Table property updated for {full_table_path}.")

except Exception as e:
    print("❌ Failed to update Iceberg table.")
    traceback.print_exc()

finally:
    spark.stop()
