from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, sha2, concat_ws
from boto3 import client, session
from uuid import uuid4

# -----------------------------
# Configuration
# -----------------------------
catalog_nm = "cosmos_nonhcd_iceberg"
database_nm = "common_data"
inventory_table_nm = "iceberg_metadata_inventory"
inventory_table_full = f"{database_nm}.{inventory_table_nm}"
warehouse_path = "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/"  # ‚úÖ update if needed

# -----------------------------
# SparkSession with Iceberg Support
# -----------------------------
spark = (
    SparkSession.builder
    .appName("IcebergMetadataInventory")
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config(f"spark.sql.catalog.{catalog_nm}.warehouse", warehouse_path)
    .getOrCreate()
)

region = session.Session().region_name
run_id = str(uuid4())

# -----------------------------
# Helper: Get all tables from Glue
# -----------------------------
def get_all_table_names_from_glue(database: str) -> list:
    glue = client("glue")
    paginator = glue.get_paginator("get_tables")
    pages = paginator.paginate(DatabaseName=database)

    table_names = []
    for page in pages:
        for table in page["TableList"]:
            table_names.append(table["Name"])
    return table_names

# -----------------------------
# Helper: Check if table is Iceberg
# -----------------------------
def is_iceberg_table(catalog: str, db: str, table: str) -> bool:
    try:
        tbl = spark._jsparkSession.catalog().getTable(f"{catalog}.{db}.{table}")
        props = tbl.properties()
        is_iceberg = props.containsKey("table_type") and props.get("table_type") == "ICEBERG"
        if not is_iceberg:
            print(f"‚ùå Not an Iceberg table: {catalog}.{db}.{table}")
        return is_iceberg
    except Exception as e:
        print(f"‚ö†Ô∏è Error verifying table {catalog}.{db}.{table}: {e}")
        return False

# -----------------------------
# Create metadata inventory table
# -----------------------------
spark.sql(f"USE {catalog_nm}")
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {inventory_table_full} (
    table_name STRING,
    region STRING,
    run_id STRING,
    snapshot_id BIGINT,
    committed_at TIMESTAMP,
    operation STRING,
    parent_id BIGINT,
    manifest_list STRING,
    manifest_path STRING,
    partition_spec_id INT,
    partition STRUCT<partition_key: STRING>,
    record_count BIGINT,
    file_path STRING,
    file_size_in_bytes BIGINT,
    table_location STRING,
    summary STRING,
    snapshot_checksum STRING
)
PARTITIONED BY (table_name, region)
TBLPROPERTIES ('format'='iceberg')
""")

# -----------------------------
# Discover and filter Iceberg tables
# -----------------------------
all_tables = get_all_table_names_from_glue(database_nm)
iceberg_tables = []

for table in all_tables:
    if is_iceberg_table(catalog_nm, database_nm, table):
        iceberg_tables.append(table)
    else:
        print(f"üõë Skipping non-Iceberg table: {table}")

print(f"\n‚úÖ Total Iceberg tables: {len(iceberg_tables)} ‚Üí {iceberg_tables}")

# -----------------------------
# Loop over tables and capture metadata
# -----------------------------
for table_name in iceberg_tables:
    print(f"\nüîç Processing: {table_name}")
    qualified = f"`{catalog_nm}.{database_nm}.{table_name}`"

    try:
        # Iceberg-specific system tables
        snapshots_df = spark.sql(f"SELECT snapshot_id, committed_at, operation, parent_id, manifest_list, summary FROM {qualified}$snapshots")
        manifests_df = spark.sql(f"SELECT * FROM {qualified}$manifests")
        files_df = spark.sql(f"SELECT * FROM {qualified}$files")

        table_location = spark._jsparkSession.catalog().getTable(f"{catalog_nm}.{database_nm}.{table_name}").location()

        joined_df = snapshots_df \
            .join(manifests_df, "snapshot_id", "left") \
            .join(files_df, manifests_df.path == files_df.file_path, "left") \
            .withColumn("table_name", lit(table_name)) \
            .withColumn("region", lit(region)) \
            .withColumn("run_id", lit(run_id)) \
            .withColumn("table_location", lit(table_location)) \
            .withColumn("snapshot_checksum", sha2(concat_ws("||",
                snapshots_df.snapshot_id.cast("string"),
                snapshots_df.committed_at.cast("string"),
                snapshots_df.operation,
                snapshots_df.manifest_list
            ), 256))

        # Write to inventory table
        joined_df.selectExpr(
            "table_name", "region", "run_id", "snapshot_id", "committed_at", "operation", "parent_id",
            "manifest_list", "path as manifest_path", "partition_spec_id", "partition", "record_count",
            "file_path", "file_size_in_bytes", "table_location", "summary", "snapshot_checksum"
        ).writeTo(f"{catalog_nm}.{inventory_table_full}").append()

        print(f"‚úÖ Metadata captured: {table_name}")

    except Exception as e:
        print(f"‚ùå Failed to capture {table_name}: {e}")
