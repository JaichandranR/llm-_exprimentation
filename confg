from pyspark.sql import SparkSession
from pyspark.sql.functions import col, row_number, when
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql.utils import AnalysisException

# Configuration
catalog_nm = "cosmos_nonhcd_iceberg"
database_nm = "common_data"
inventory_table = "iceberg_snapshot_inventory"
state_table = "iceberg_inventory_state"
s3_warehouse_path = "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/"

# Initialize Spark session
spark = (
    SparkSession.builder
    .appName("IcebergMetadataInventoryLoop")
    .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config(f"spark.sql.catalog.{catalog_nm}.warehouse", s3_warehouse_path)
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .getOrCreate()
)

# Load all table names
tables_df = spark.sql(f"SHOW TABLES IN {catalog_nm}.{database_nm}")
table_names = [row.tableName for row in tables_df.collect()]

# Load or initialize state table
state_schema = StructType([
    StructField("table_name", StringType(), True),
    StructField("snapshot_id", StringType(), True),
])
try:
    state_df = spark.sql(f"SELECT * FROM {catalog_nm}.{database_nm}.{state_table}")
except AnalysisException:
    empty_state_df = spark.createDataFrame([], schema=state_schema)
    empty_state_df.writeTo(f"{catalog_nm}.{database_nm}.{state_table}").createOrReplace()
    state_df = empty_state_df

# Initialize result collector
all_snapshots = []

for table in table_names:
    print(f"üìò Processing table: {table}")
    try:
        # Read snapshot metadata
        snapshots_df = spark.sql(
            f"""SELECT '{table}' AS table_name, snapshot_id, committed_at, operation, manifest_list, summary
                 FROM {catalog_nm}.{database_nm}.{table}.snapshots"""
        )

        # Determine latest snapshot
        window_spec = Window.orderBy(col("committed_at").desc())
        snapshots_df = snapshots_df.withColumn("rn", row_number().over(window_spec))
        snapshots_df = snapshots_df.withColumn(
            "is_latest_snapshot", when(col("rn") == 1, "Yes").otherwise("No")
        ).drop("rn")

        # Get latest snapshot_id
        latest_row = snapshots_df.filter(col("is_latest_snapshot") == "Yes").collect()[0]
        latest_snapshot_id = latest_row.snapshot_id

        # Check against state table
        prev_state = state_df.filter(col("table_name") == table).collect()
        prev_snapshot_id = prev_state[0]["snapshot_id"] if prev_state else None

        if prev_snapshot_id != latest_snapshot_id:
            print(f"üîÑ Snapshot updated for: {table}")
            all_snapshots.append(snapshots_df)
        else:
            print(f"‚úÖ No change for: {table}")
    except Exception as e:
        print(f"‚ùå Error reading {table}: {str(e)}")

# Merge all updated metadata and write output
if all_snapshots:
    merged_df = all_snapshots[0]
    for df in all_snapshots[1:]:
        merged_df = merged_df.unionByName(df)

    # Write inventory table
    merged_df.writeTo(f"{catalog_nm}.{database_nm}.{inventory_table}").createOrReplace()

    # Update state table
    latest_state_df = merged_df.filter("is_latest_snapshot = 'Yes'") \
                               .select("table_name", "snapshot_id")
    latest_state_df.writeTo(f"{catalog_nm}.{database_nm}.{state_table}").overwritePartitions()

    print("‚úÖ Inventory and state tables updated.")
else:
    print("‚úÖ No snapshot changes detected. Nothing updated.")
