from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Minimal Spark session setup for Iceberg
catalog_nm = "cosmos_nonhcd_iceberg"
database_nm = "common_data"

spark = (
    SparkSession.builder
    .appName("IcebergSnapshotDryRun")
    .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config(f"spark.sql.catalog.{catalog_nm}.warehouse", "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/")
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .getOrCreate()
)

# Use just a few known Iceberg tables for dry run
sample_tables = ["your_table_1", "your_table_2", "your_table_3"]  # replace with real table names

for table in sample_tables:
    print(f"\nüîç Reading snapshot metadata for: {table}")
    try:
        snapshots = spark.sql(
            f"""SELECT snapshot_id, committed_at
                 FROM {catalog_nm}.{database_nm}.{table}.snapshots
                 ORDER BY committed_at DESC"""
        ).limit(5)

        for row in snapshots.collect():
            print(f"üìå snapshot_id = {row.snapshot_id}, committed_at = {row.committed_at}")
    except Exception as e:
        print(f"‚ùå Failed reading {table}: {str(e)}")
