import sys
import boto3
from datetime import datetime
from pyspark.sql import SparkSession

# ─── CONFIG ────────────────────────────────────────────────────────────────
DATABASE             = "common_data"
TABLE                = "metadata_table"
SNAPSHOT_ID_TO_EXPIRE = 4525018059108774355  # replace with one you saw

# ─── SPARK SETUP ───────────────────────────────────────────────────────────
spark = (
    SparkSession.builder
      .appName("Iceberg_Load_Scheme_Test")
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()
)

# ─── 1) fetch the catalog‐defined S3 path ───────────────────────────────────
glue = boto3.client("glue")
tbl  = glue.get_table(DatabaseName=DATABASE, Name=TABLE)["Table"]
base_s3 = tbl["StorageDescriptor"]["Location"].rstrip("/") + "/"  # e.g. "s3://bucket/.../metadata_table/"

print("▶ Raw catalog path:", base_s3)

# ─── 2) try loading under both schemes ─────────────────────────────────────
hadoopTables = spark._jvm.org.apache.iceberg.hadoop.HadoopTables()
for scheme in ("s3a://", "s3://"):
    test_uri = base_s3.replace("s3://", scheme)
    print(f"\n▶ Attempting load with URI = {test_uri}")
    try:
        tbl_obj = hadoopTables.load(test_uri)
        print("  ✔️ SUCCESS loading with", scheme)
        break
    except Exception as e:
        print("  ❌ failed:", e)

else:
    print("\n‼️ Could not load the Iceberg table under either URI!")
    spark.stop()
    sys.exit(1)

# ─── 3) (Optional) expire a single snapshot to test delete perms ────────────
print(f"\n▶ Expiring one snapshot ID {SNAPSHOT_ID_TO_EXPIRE} …")
try:
    tbl_obj.expireSnapshots() \
           .expireSnapshotId(SNAPSHOT_ID_TO_EXPIRE) \
           .commit()
    print("  ✔️ Successfully expired snapshot", SNAPSHOT_ID_TO_EXPIRE)
except Exception as e:
    print("  ❌ expireSnapshotId failed:", e)

spark.stop()
