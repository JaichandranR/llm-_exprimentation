import sys
from datetime import datetime, timedelta

from pyspark.context import SparkContext
from pyspark.sql import SparkSession as sparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
catalog                = "cosmos_nonhcd_iceberg"
db                     = "common_data"
tbl                    = "metadata_table"
warehouse              = "s3://app-id-90177-dep-id-114232-uu-id-pee895Fr5knp/"
full_table             = f"{catalog}.{db}.{tbl}"
target_file_size       = 256 * 1024 * 1024  # 256 MB
RETENTION_DAYS         = 7

# â”€â”€â”€ SPARK + ICEBERG SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark = (
    sparkSession.builder
      .appName("GlueIcebergCompactionViaSQL_SizeBased")
      .config(f"spark.sql.catalog.{catalog}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{catalog}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{catalog}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .config(f"spark.sql.catalog.{catalog}.warehouse",    warehouse)
      .config("spark.sql.extensions",                      "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    # â”€â”€â”€ 1) Read & detect partitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("â–¶ Loading Iceberg table:", full_table)
    df = spark.read.format("iceberg").load(full_table)

    try:
        meta          = spark.sql(f"SELECT partition FROM {full_table}.partitions")
        struct_type   = meta.schema["partition"].dataType
        partition_cols = [fld.name for fld in struct_type.fields]
        is_part       = True
        print(f"  â€¢ Found partition columns: {partition_cols}")
    except AnalysisException:
        is_part = False
        print("  â€¢ No partitions detected (fullâ€table)")

    # â”€â”€â”€ 2) Size-based compaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_part:
        select_expr = ", ".join(f"partition.{c} AS {c}" for c in partition_cols)
        flat = (
            spark.sql(f"SELECT {select_expr} FROM {full_table}.partitions")
                 .distinct()
                 .orderBy(*[col(c).desc() for c in partition_cols])
        )
        values = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(values) > 2:
            to_compact = values[2:]
            print(f"â–¶ Compacting {len(to_compact)} older partitions (skipping newest 2)")
            for v in to_compact:
                cond = " AND ".join(f"{c} = {repr(x)}" for c, x in zip(partition_cols, v))
                print("   â€¢ WHERE", cond)
                df.filter(expr(cond)) \
                  .writeTo(full_table) \
                  .option("write.target-file-size-bytes", str(target_file_size)) \
                  .overwritePartitions()
        else:
            print("â–¶ â‰¤2 partitions found; full-table compaction")
            df.writeTo(full_table) \
              .option("write.target-file-size-bytes", str(target_file_size)) \
              .overwritePartitions()
    else:
        print("â–¶ Unpartitioned; full-table compaction")
        df.writeTo(full_table) \
          .option("write.target-file-size-bytes", str(target_file_size)) \
          .overwritePartitions()

    print("âœ… Compaction complete.")

    # â”€â”€â”€ 3) Expire snapshots older than 7 days â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cutoff_ts = (datetime.utcnow() - timedelta(days=RETENTION_DAYS)) \
                   .strftime("%Y-%m-%d %H:%M:%S")
    print(f"â–¶ Expiring snapshots older than {RETENTION_DAYS} days (before {cutoff_ts} UTC)")

    expire_sql = f"""
      CALL {catalog}.system.expire_snapshots(
        table      => '{db}.{tbl}',
        older_than => TIMESTAMP '{cutoff_ts}'
      )
    """
    spark.sql(expire_sql)
    print("âœ… expire_snapshots procedure completed.")

    spark.stop()
    print("ğŸ‰ Job finished successfully.")

except Exception as e:
    print("âŒ Job failed:", e)
    sys.exit(1)
