import sys
from datetime import datetime, timedelta

from pyspark.context import SparkContext
from pyspark.sql import SparkSession as sparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

# ─── CONFIG ───────────────────────────────────────────────────────────────────
catalog                = "cosmos_nonhcd_iceberg"
db                     = "common_data"
tbl                    = "metadata_table"
warehouse              = "s3://app-id-90177-dep-id-114232-uu-id-pee895Fr5knp/"
full_table             = f"{catalog}.{db}.{tbl}"
target_file_size_bytes = 256 * 1024 * 1024    # 256 MB
skip_partitions        = 4                    # skip newest 4
retention_days         = 7

# ─── START SPARK ──────────────────────────────────────────────────────────────
spark = (
    sparkSession.builder
      .appName("Iceberg_Compact_Expire_RemoveOrphans")
      .config("spark.sql.extensions",
              "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .config(f"spark.sql.catalog.{catalog}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{catalog}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{catalog}.type",         "glue")
      .config(f"spark.sql.catalog.{catalog}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .config(f"spark.sql.catalog.{catalog}.warehouse",    warehouse)
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    # 1) load + detect partitions
    df = spark.read.format("iceberg").load(full_table)
    try:
        meta          = spark.sql(f"SELECT partition FROM {full_table}.partitions")
        struct_type   = meta.schema["partition"].dataType
        partition_cols = [f.name for f in struct_type.fields]
        is_part = True
    except AnalysisException:
        is_part = False

    # 2) compact by file size
    if is_part:
        flat = (spark.sql(
                  "SELECT " + ", ".join(f"partition.{c} AS {c}" for c in partition_cols) +
                  f" FROM {full_table}.partitions")
                .distinct()
                .orderBy(*[col(c).desc() for c in partition_cols]))
        vals = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(vals) > skip_partitions:
            for part_vals in vals[skip_partitions:]:
                cond = " AND ".join(f"{c} = {repr(v)}" for c, v in zip(partition_cols, part_vals))
                df.filter(expr(cond)) \
                  .writeTo(full_table) \
                  .option("write.target-file-size-bytes", str(target_file_size_bytes)) \
                  .overwritePartitions()
        else:
            df.writeTo(full_table) \
              .option("write.target-file-size-bytes", str(target_file_size_bytes)) \
              .overwritePartitions()
    else:
        df.writeTo(full_table) \
          .option("write.target-file-size-bytes", str(target_file_size_bytes)) \
          .overwritePartitions()

    # 3) expire snapshots
    cutoff = (datetime.utcnow() - timedelta(days=retention_days)) \
                .strftime("%Y-%m-%d %H:%M:%S")
    spark.sql(f"""
      CALL {catalog}.system.expire_snapshots(
        table      => '{db}.{tbl}',
        older_than => TIMESTAMP '{cutoff}'
      )
    """)

    # 4) remove orphan files
    spark.sql(f"""
      CALL {catalog}.system.remove_orphan_files(
        table      => '{db}.{tbl}',
        older_than => TIMESTAMP '{cutoff}'
      )
    """)

    spark.stop()
    print("✅ Compaction + snapshot expiration + orphan cleanup done.")

except Exception as e:
    print("❌ Job failed:", e)
    sys.exit(1)
