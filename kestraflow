# ─── 5) Read snapshot metadata via Spark SQL ──────────────────────────────
snap_df = spark.sql(f"""
  SELECT snapshot_id, committed_at
  FROM {full_table}.snapshots
  ORDER BY committed_at DESC
""")
snaps = snap_df.collect()  # rows with .snapshot_id and .committed_at (a Python datetime)

# ─── 6) Determine which snapshots to expire ──────────────────────────────
cutoff_dt = datetime.utcnow() - timedelta(days=RETENTION_DAYS)

# skip the first two (newest) and only keep those older than cutoff_dt
to_expire = [
  row.snapshot_id
  for idx, row in enumerate(snaps)
  if idx >= 2 and row.committed_at < cutoff_dt
]

print(f"Snapshots found: {[r.snapshot_id for r in snaps]}")
print(f"Snapshots selected to expire: {to_expire}")

# ─── 7) Expire those snapshots via HadoopTables API ────────────────────────
jvm           = spark._jvm
hadoopTables  = jvm.org.apache.iceberg.hadoop.HadoopTables()
icebergTable  = hadoopTables.load(s3a_loc)

builder = icebergTable.expireSnapshots()
for sid in to_expire:
    builder = builder.expireSnapshotId(sid)
builder.commit()

print(f"✅ Expired {len(to_expire)} snapshots older than {RETENTION_DAYS} days, skipping the newest two.")
