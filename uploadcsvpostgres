import pandas as pd
import requests
import json
import numpy as np
import psycopg2
from ecdpawssession.session_builder import SessionBuilder

# Step 1: PostgreSQL Connection Setup
USERNAME = ''  # Leave blank for IAM authentication
DOMAIN = 'naeast'  # ENTER DOMAIN
ROLE_ARN = 'arn:aws:iam::ROLE/90177-aurora-app-admin'  # ENTER ROLE ARN
ENDPOINT = 'proxy-postgres-nlb.elb.us-east-1.amazonaws.com'  # ENTER DB NLB PROXY
CLUSTER = 'aurora-cluster.cluster-c1wayscsgqcd.us-east-1.rds.amazonaws.com'  # ENTER PG Cluster
PORT = '6160'  # ENTER PORT
USER = 'llmdbAuroraAppAdmin'  # ENTER USER
REGION = 'us-east-1'  # ENTER REGION
DBNAME = 'llmdb'  # ENTER DB

# Generate RDS token for authentication
session = SessionBuilder.cli(
    role_arn=ROLE_ARN,
    region=REGION,
    username=USERNAME,
    domain=DOMAIN,
    store_password=True
).with_auto_renew().build()

client = session.client('rds')
token = client.generate_db_auth_token(
    DBHostname=CLUSTER,
    Port=PORT,
    DBUsername=USER,
    Region=REGION
)

# Establish database connection
def get_postgres_connection():
    try:
        conn = psycopg2.connect(
            host=ENDPOINT,
            user=USER,
            password=token,
            port=PORT,
            database=DBNAME
        )
        print("Database connection successful.")
        return conn
    except Exception as e:
        print(f"Database connection failed due to {e}")
        raise

# Step 2: Hosted API Setup
API_URL = "http://your-eks-hosted-url"  # Replace with the actual URL of your hosted API
API_KEY = "3g7h1e9a5b0d2f81a4c6"  # Replace with your actual API key

# Step 3: Chunking Text
def chunk_text(text, max_length=512):
    sentences = text.split(". ")
    chunks = []
    chunk = []
    length = 0

    for sentence in sentences:
        sentence_length = len(sentence.split())
        if length + sentence_length <= max_length:
            chunk.append(sentence)
            length += sentence_length
        else:
            chunks.append(". ".join(chunk))
            chunk = [sentence]
            length = sentence_length

    if chunk:
        chunks.append(". ".join(chunk))
    return chunks

# Step 4: Normalize the Embedding
def normalize_embedding(embedding):
    norm = np.linalg.norm(embedding)
    if norm == 0:
        return embedding  # Avoid division by zero
    return [float(i) / norm for i in embedding]

# Step 5: Populate application.mitre_embeddings Table
def populate_mitre_embeddings_table(file_path, id_column, text_column, source, max_length=512):
    conn = get_postgres_connection()
    cursor = conn.cursor()

    try:
        df = pd.read_csv(file_path)
        print(f"Processing {len(df)} rows from {file_path}...")

        for _, row in df.iterrows():
            control_id = row[id_column]
            document = row[text_column]

            chunks = chunk_text(document, max_length=max_length)
            for chunk_id, chunk in enumerate(chunks):
                if not chunk.strip():
                    print(f"Empty context for control_id {control_id}, skipping chunk_id {chunk_id}...")
                    continue

                headers = {
                    "Content-Type": "application/json",
                    "x-api-key": API_KEY
                }
                data = {
                    "model": "MetaLlama318BInstruct",
                    "prompt": (
                        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|> "
                        f"You are an embedding assistant. Generate an embedding for the provided text. "
                        f"<|eot_id|><|start_header_id|>context<|end_header_id|> {chunk} "
                        f"<|eot_id|><|start_header_id|>assistant<|end_header_id|> Generate an embedding. <|end_header_id|>"
                    ),
                    "options": {"seed": 1, "temperature": 0},
                    "stream": False
                }
                response = requests.post(API_URL, data=json.dumps(data), headers=headers, timeout=1000)

                print(f"Full API Response for control_id {control_id}, chunk_id {chunk_id}: {response.json()}")

                if response.status_code == 200:
                    result = response.json()
                    embedding = result.get("context")
                    if embedding is None or not isinstance(embedding, list) or len(embedding) == 0:
                        print(f"Invalid embedding received for control_id {control_id}, chunk_id {chunk_id}: {embedding}")
                        continue

                    if len(embedding) > 768:
                        embedding = embedding[:768]
                    elif len(embedding) < 768:
                        embedding.extend([0.0] * (768 - len(embedding)))

                    embedding = [round(float(value), 10) for value in embedding]
                    embedding = normalize_embedding(embedding)
                    embedding = [round(float(value), 10) for value in embedding]

                    cursor.execute(
                        """
                        INSERT INTO application.mitre_embeddings (control_id, chunk_id, source, documents, embedding)
                        VALUES (%s, %s, %s, %s, %s)
                        ON CONFLICT (control_id, chunk_id)
                        DO UPDATE SET 
                            source = EXCLUDED.source,
                            documents = EXCLUDED.documents,
                            embedding = EXCLUDED.embedding;
                        """,
                        (control_id, chunk_id, source, chunk, embedding)
                    )
                    conn.commit()
                else:
                    print(f"Failed to compute embedding for control_id {control_id}, chunk_id {chunk_id}. API Response: {response.text}")

    except Exception as e:
        print(f"Error while processing {file_path}: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

# Step 6: Process All CSV Files
def process_all_files():
    csv_files = [
        {"file": "MITRE_Mitigation.csv", "id_column": "MitigationID", "text_column": "MitigationDesc", "source": "mitre_mitigation"},
        {"file": "MITRE_SubTechnique.csv", "id_column": "SubTechniqueID", "text_column": "SubTechniqueDesc", "source": "mitre_subtechnique"},
        {"file": "MITRE_ID_Mapping.csv", "id_column": "TechniqueID", "text_column": "TechniqueID", "source": "mitre_id_mapping"},
        {"file": "MITRE_Tactic.csv", "id_column": "TacticID", "text_column": "TacticDescription", "source": "mitre_tactic"},
        {"file": "MITRE_Technique.csv", "id_column": "TechniqueID", "text_column": "TechniqueDesc", "source": "mitre_technique"}
    ]

    for csv_file in csv_files:
        populate_mitre_embeddings_table(
            file_path=csv_file["file"],
            id_column=csv_file["id_column"],
            text_column=csv_file["text_column"],
            source=csv_file["source"],
            max_length=512
        )

# Run the script
if __name__ == "__main__":
    process_all_files()
