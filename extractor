import yaml
import json
import os
import re
import logging
import subprocess
import pandas as pd
from datetime import datetime

from trino.dbapi import connect
from trino.auth import JWTAuthentication

# -------------------------------------------------------------------
# Logging setup
# -------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# -------------------------------------------------------------------
# Load DBT profile & Trino connection
# -------------------------------------------------------------------
def load_dbt_trino_profile(profile_name='dbt-trino-cosmos', target_name='prototype', profile_path=None):
    if profile_path is None:
        profile_path = os.path.expanduser("~/.dbt/profiles.yml")

    if not os.path.exists(profile_path):
        raise FileNotFoundError(f"profiles.yml not found: {profile_path}")

    with open(profile_path, 'r') as f:
        profiles = yaml.safe_load(f)

    profile = profiles[profile_name]['outputs'][target_name]

    token_file = "prod_token.txt" if target_name == "prototype" else "sit_token.txt"
    base_dir = os.path.dirname(os.path.dirname(profile_path))
    jwt_path = os.path.join(base_dir, "lib", "jwt", token_file)

    if not os.path.exists(jwt_path):
        raise FileNotFoundError(f"JWT token file missing: {jwt_path}")

    with open(jwt_path, "r") as f:
        jwt = f.read().strip()

    auth = JWTAuthentication(jwt)

    conn = connect(
        host=profile['host'],
        port=profile['port'],
        user=None,
        catalog=profile['catalog'],
        schema=profile['schema'],
        http_scheme=profile.get('http_scheme', 'https'),
        auth=auth
    )

    return conn, profile['catalog'], profile['schema']


# -------------------------------------------------------------------
# Fetch manifest.json via curl
# -------------------------------------------------------------------
def fetch_manifest(team_name, out_path):
    url = (
        "https://cronos-cao-app2app.prod.aws.jpmchase.net/"
        f"git-webhook-consumer/defer-manifest?teamName={team_name}"
    )

    result = subprocess.run(
        ["curl", "-s", "-o", out_path, "-X", "GET", url],
        capture_output=True
    )

    if result.returncode != 0:
        logging.error(f"Failed fetching {team_name}: {result.stderr}")
        return False

    if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:
        logging.error(f"Manifest empty for {team_name}")
        return False

    logging.info(f"Fetched manifest for {team_name}")
    return True


# -------------------------------------------------------------------
# Build registry from all manifests
# -------------------------------------------------------------------
def build_registry(manifest_paths):
    registry = {}

    for mp in manifest_paths:
        if not os.path.exists(mp):
            continue

        try:
            with open(mp, "r") as f:
                mf = json.load(f)
        except:
            logging.error(f"Invalid JSON: {mp}")
            continue

        nodes = mf.get("nodes", {})
        sources = mf.get("sources", {})

        # MODELS
        for node_id, node in nodes.items():
            if not node_id.startswith("model."):
                continue

            parts = node_id.split(".")
            if len(parts) < 3:
                continue

            project = parts[1]
            model = parts[2]

            registry[model] = {
                "project": project,
                "resource_type": "model",
                "schema": node.get("schema"),
                "fqn": node.get("fqn", []),
                "file": node.get("original_file_path", "")
            }

        # SOURCES
        for sid, src in sources.items():
            parts = sid.split(".")
            if len(parts) < 3:
                continue

            project = parts[1]
            source_name = parts[2]

            registry[source_name] = {
                "project": project,
                "resource_type": "source",
                "schema": src.get("schema"),
                "fqn": src.get("fqn", []),
                "file": ""
            }

    logging.info(f"Registry size = {len(registry)}")
    return registry


# -------------------------------------------------------------------
# Dependency extraction patterns
# -------------------------------------------------------------------
REF_PATTERN = re.compile(r"ref\(\s*'([^']+)'\s*\)")
SRC_PATTERN = re.compile(r"source\(\s*'[^']*'\s*,\s*'([^']+)'\s*\)")
SQL_TABLE_PATTERN = re.compile(
    r"(?:from|join)\s+[a-zA-Z0-9_]+\.[a-zA-Z0-9_]+\.([a-zA-Z0-9_]+)",
    re.IGNORECASE
)


# -------------------------------------------------------------------
# Dependency extraction
# -------------------------------------------------------------------
def parse_dependencies(manifest_paths, registry):
    records = []

    for mp in manifest_paths:
        if not os.path.exists(mp):
            continue

        try:
            with open(mp, "r") as f:
                mf = json.load(f)
        except:
            continue

        nodes = mf.get("nodes", {})

        for node_id, node in nodes.items():
            if not node_id.startswith("model."):
                continue

            parts = node_id.split(".")
            child_project = parts[1]
            child_model = parts[2]
            child_schema = node.get("schema")
            child_fqn = ".".join(node.get("fqn", []))
            sql = node.get("raw_code", "")

            ts = datetime.now().isoformat()

            # 1. ref()
            for name in REF_PATTERN.findall(sql):
                if name in registry:
                    p = registry[name]
                    records.append({
                        "parent_model": name,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "child_model": child_model,
                        "child_project": child_project,
                        "child_schema": child_schema,
                        "relation_type": "ref",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": child_fqn,
                        "timestamp": ts
                    })

            # 2. source()
            for name in SRC_PATTERN.findall(sql):
                if name in registry:
                    p = registry[name]
                    records.append({
                        "parent_model": name,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "child_model": child_model,
                        "child_project": child_project,
                        "child_schema": child_schema,
                        "relation_type": "source",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": child_fqn,
                        "timestamp": ts
                    })

            # 3. Fully qualified SQL: db.schema.table
            for tbl in SQL_TABLE_PATTERN.findall(sql):
                if tbl in registry:
                    p = registry[tbl]
                    records.append({
                        "parent_model": tbl,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "child_model": child_model,
                        "child_project": child_project,
                        "child_schema": child_schema,
                        "relation_type": "direct_sql",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": child_fqn,
                        "timestamp": ts
                    })

    df = pd.DataFrame(records)
    logging.info(f"Parsed dependencies = {len(df)}")
    return df


# -------------------------------------------------------------------
# Create table
# -------------------------------------------------------------------
def create_lineage_table(cursor, catalog, schema):
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {catalog}.{schema}.global_lineage_purge_engine (
            parent_model VARCHAR,
            parent_project VARCHAR,
            parent_schema VARCHAR,
            child_model VARCHAR,
            child_project VARCHAR,
            child_schema VARCHAR,
            relation_type VARCHAR,
            parent_fqn VARCHAR,
            child_fqn VARCHAR,
            timestamp VARCHAR
        )
    """)
    logging.info("Ensured lineage table exists.")


# -------------------------------------------------------------------
# BATCH MERGE of 100 rows (like before)
# -------------------------------------------------------------------
def merge_records(cursor, catalog, schema, df, batch_size=100):

    if df.empty:
        logging.info("No records to merge.")
        return

    total = len(df)
    logging.info(f"Merging {total} records in batches of {batch_size}")

    cols = [
        "parent_model", "parent_project", "parent_schema",
        "child_model", "child_project", "child_schema",
        "relation_type", "parent_fqn", "child_fqn", "timestamp"
    ]

    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = df.iloc[start:end]

        values_sql = []
        for _, row in batch.iterrows():
            vals = [str(row[c]).replace("'", "''") for c in cols]
            values_sql.append(f"('{vals[0]}','{vals[1]}','{vals[2]}','{vals[3]}','{vals[4]}','{vals[5]}','{vals[6]}','{vals[7]}','{vals[8]}','{vals[9]}')")

        values_block = ",\n".join(values_sql)

        merge_sql = f"""
        MERGE INTO {catalog}.{schema}.global_lineage_purge_engine t
        USING (
            VALUES
                {values_block}
        ) AS s (
            parent_model, parent_project, parent_schema,
            child_model, child_project, child_schema,
            relation_type, parent_fqn, child_fqn, timestamp
        )
        ON (
            t.parent_model = s.parent_model AND
            t.parent_project = s.parent_project AND
            t.child_model = s.child_model AND
            t.child_project = s.child_project AND
            t.relation_type = s.relation_type
        )
        WHEN MATCHED THEN UPDATE SET
            timestamp = s.timestamp
        WHEN NOT MATCHED THEN INSERT VALUES (
            s.parent_model, s.parent_project, s.parent_schema,
            s.child_model, s.child_project, s.child_schema,
            s.relation_type, s.parent_fqn, s.child_fqn, s.timestamp
        )
        """

        cursor.execute(merge_sql)
        logging.info(f"Merged rows {start+1} â†’ {end}")

    logging.info("Batch merge complete.")


# -------------------------------------------------------------------
# MAIN WORKFLOW
# -------------------------------------------------------------------
if __name__ == "__main__":

    teams = [
        "core-services", "core-services-hcd", "cyber-ops", "dfas", "gt-metrics",
        "intel", "resiliency", "techgrc", "aa", "assurance", "dlp", "ccm", "tpr"
    ]

    manifest_paths = [f"manifests/manifest_{t}.json" for t in teams]

    for t, p in zip(teams, manifest_paths):
        fetch_manifest(t, p)

    registry = build_registry(manifest_paths)
    df = parse_dependencies(manifest_paths, registry)

    conn, catalog, schema = load_dbt_trino_profile(
        profile_path="C:/Users/R732325/ds/projects/dbt-jules/dbt-models-core-services/.dbt/profiles.yml"
    )
    cursor = conn.cursor()

    create_lineage_table(cursor, catalog, schema)
    merge_records(cursor, catalog, schema, df, batch_size=100)

    logging.info("Lineage extraction completed successfully.")
