import os
import json
import yaml
import logging
import subprocess
import pandas as pd
from datetime import datetime

from trino.dbapi import connect
from trino.auth import JWTAuthentication


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ============================================================
# 1. Load dbt profiles.yml and connect to Trino
# ============================================================
def load_dbt_trino_profile(profile_path, profile_name, target_name):
    profile_path = os.path.expanduser(profile_path)

    if not os.path.exists(profile_path):
        raise FileNotFoundError(f"profiles.yml not found at {profile_path}")

    with open(profile_path, "r") as f:
        profiles = yaml.safe_load(f)

    if profile_name not in profiles:
        raise KeyError(f"Profile '{profile_name}' not found")

    profile = profiles[profile_name]["outputs"][target_name]

    # Load JWT token
    base = os.path.dirname(profile_path)
    token_file = "sit_token.txt" if target_name != "prototype" else "prod_token.txt"
    jwt_path = os.path.join(base, "lib", "jwt", token_file)

    if not os.path.exists(jwt_path):
        raise FileNotFoundError(f"JWT token file {jwt_path} not found")

    with open(jwt_path, "r") as f:
        jwt_token = f.read().strip()

    auth = JWTAuthentication(jwt_token)

    # Override catalog for prototype
    catalog = profile["catalog"]
    if target_name == "prototype":
        catalog = "cosmos_nonhcd_iceberg_prototype"

    # Connect to Trino
    conn = connect(
        host=profile["host"],
        port=profile["port"],
        catalog=catalog,
        schema=profile["schema"],
        user=None,
        http_scheme=profile.get("http_scheme", "https"),
        auth=auth,
    )

    logging.info(f"Connected to Trino â€” catalog={catalog}, schema={profile['schema']}")
    return conn, catalog, profile["schema"]


# ============================================================
# 2. Fetch manifest.json via curl
# ============================================================
def fetch_manifest(team_name, out_dir):
    out_path = os.path.join(out_dir, f"manifest_{team_name}.json")

    url = (
        "https://cronos-cao-app2app.prod.aws.jpmchase.net/"
        f"git-webhook-consumer/defer-manifest?teamName={team_name}"
    )

    result = subprocess.run(
        ["curl", "-s", "-o", out_path, "-X", "GET", url],
        capture_output=True
    )

    if result.returncode != 0:
        logging.error(f"Failed to fetch manifest for {team_name}: {result.stderr.decode()}")
        return None

    if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:
        logging.warning(f"Manifest for {team_name} is empty. Skipping.")
        return None

    logging.info(f"Fetched manifest for {team_name} ({os.path.getsize(out_path)} bytes)")
    return out_path


# ============================================================
# 3. Build global registry for all models/sources
# ============================================================
def build_registry(manifest_paths):
    registry = {}

    for mpath in manifest_paths:

        # NEW FIX: Skip invalid or empty manifest
        if not os.path.exists(mpath) or os.path.getsize(mpath) == 0:
            logging.warning(f"Skipping invalid or empty manifest: {mpath}")
            continue

        try:
            with open(mpath, "r") as f:
                manifest = json.load(f)
        except json.JSONDecodeError:
            logging.error(f"Manifest file {mpath} is not valid JSON. Skipping.")
            continue

        project = manifest["metadata"]["project_name"]

        # Models
        for _, node in manifest.get("nodes", {}).items():
            if node["resource_type"] == "model":
                name = node["name"].lower()
                registry[name] = {
                    "project": project,
                    "schema": node.get("schema"),
                    "fqn": node.get("fqn"),
                    "type": "model"
                }

        # Sources
        for _, src in manifest.get("sources", {}).items():
            name = src["name"].lower()
            registry[name] = {
                "project": project,
                "schema": src.get("schema"),
                "fqn": src.get("fqn"),
                "type": "source"
            }

    logging.info(f"Registry built with {len(registry)} items.")
    return registry


# ============================================================
# 4. Parse parent-child dependencies (ref + source)
# ============================================================
def parse_dependencies(manifest_paths, registry):
    records = []

    for mpath in manifest_paths:

        # NEW FIX: Skip invalid manifest
        if not os.path.exists(mpath) or os.path.getsize(mpath) == 0:
            logging.warning(f"Skipping empty manifest during dependency parse: {mpath}")
            continue

        try:
            with open(mpath, "r") as f:
                manifest = json.load(f)
        except json.JSONDecodeError:
            logging.error(f"Manifest file {mpath} is invalid JSON. Skipping.")
            continue

        child_project = manifest["metadata"]["project_name"]

        for _, node in manifest["nodes"].items():
            if node["resource_type"] != "model":
                continue

            child_name = node["name"].lower()
            child_schema = node["schema"]
            child_fqn = ".".join(node["fqn"])
            ts = datetime.now().isoformat()

            # ref() parents
            for pid in node.get("depends_on", {}).get("nodes", []):
                parent_name = manifest["nodes"].get(pid, {}).get("name")
                if not parent_name:
                    continue

                parent_info = registry.get(parent_name.lower())
                if not parent_info:
                    logging.warning(f"Missing parent info for model {parent_name}")
                    continue

                records.append({
                    "parent_model": parent_name,
                    "parent_project": parent_info["project"],
                    "parent_schema": parent_info["schema"],
                    "child_model": child_name,
                    "child_project": child_project,
                    "child_schema": child_schema,
                    "relation_type": "ref",
                    "parent_fqn": ".".join(parent_info["fqn"]),
                    "child_fqn": child_fqn,
                    "timestamp": ts
                })

            # source() parents
            for sid in node.get("depends_on", {}).get("sources", []):
                source_name = manifest["sources"][sid]["name"]
                parent_info = registry.get(source_name.lower())

                if not parent_info:
                    logging.warning(f"Missing parent info for source {source_name}")
                    continue

                records.append({
                    "parent_model": source_name,
                    "parent_project": parent_info["project"],
                    "parent_schema": parent_info["schema"],
                    "child_model": child_name,
                    "child_project": child_project,
                    "child_schema": child_schema,
                    "relation_type": "source",
                    "parent_fqn": ".".join(parent_info["fqn"]),
                    "child_fqn": child_fqn,
                    "timestamp": ts
                })

    df = pd.DataFrame(records)
    logging.info(f"Parsed {len(df)} dependency records.")
    return df


# ============================================================
# 5. Create lineage table in Trino (idempotent)
# ============================================================
def create_lineage_table(cursor, catalog, schema, table):
    sql = f"""
    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table} (
        parent_model VARCHAR,
        parent_project VARCHAR,
        parent_schema VARCHAR,
        child_model VARCHAR,
        child_project VARCHAR,
        child_schema VARCHAR,
        relation_type VARCHAR,
        parent_fqn VARCHAR,
        child_fqn VARCHAR,
        timestamp VARCHAR
    )
    """
    cursor.execute(sql)
    logging.info(f"Ensured table {table} exists.")


# ============================================================
# 6. Insert data incrementally
# ============================================================
def merge_records(cursor, catalog, schema, table, df):
    for _, row in df.iterrows():
        sql = f"""
        INSERT INTO {catalog}.{schema}.{table}
        (
            parent_model, parent_project, parent_schema,
            child_model, child_project, child_schema,
            relation_type, parent_fqn, child_fqn, timestamp
        )
        VALUES (
            '{row.parent_model}', '{row.parent_project}', '{row.parent_schema}',
            '{row.child_model}', '{row.child_project}', '{row.child_schema}',
            '{row.relation_type}', '{row.parent_fqn}', '{row.child_fqn}',
            '{row.timestamp}'
        )
        """
        cursor.execute(sql)

    logging.info("Incremental insert completed.")


# ============================================================
# 7. MAIN WORKFLOW
# ============================================================
if __name__ == "__main__":

    with open("teams.txt", "r") as f:
        TEAMS = [t.strip() for t in f.readlines()]

    OUT_DIR = "manifests"
    os.makedirs(OUT_DIR, exist_ok=True)

    manifest_paths = []

    for team in TEAMS:
        logging.info(f"Fetching manifest for {team}")
        path = fetch_manifest(team, OUT_DIR)
        if path:
            manifest_paths.append(path)

    registry = build_registry(manifest_paths)
    df = parse_dependencies(manifest_paths, registry)

    conn, catalog, schema = load_dbt_trino_profile(
        profile_path="C:/Users/R732325/ds/projects/dbt-jules/dbt-models-core-services/.dbt/profiles.yml",
        profile_name="dbt-trino-cosmos",
        target_name="prototype"
    )
    cursor = conn.cursor()

    table_name = "global_lineage_purge_engine"
    create_lineage_table(cursor, catalog, schema, table_name)

    merge_records(cursor, catalog, schema, table_name, df)

    logging.info("All manifests processed and lineage table populated.")
