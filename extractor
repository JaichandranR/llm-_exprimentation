import os
import json
import yaml
import logging
import subprocess
import pandas as pd
from datetime import datetime

from trino.dbapi import connect
from trino.auth import JWTAuthentication


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ============================================================
# 1. Load dbt profiles.yml and connect to Trino
# ============================================================
def load_dbt_trino_profile(profile_path, profile_name, target_name):
    profile_path = os.path.expanduser(profile_path)

    if not os.path.exists(profile_path):
        raise FileNotFoundError(f"profiles.yml not found at {profile_path}")

    with open(profile_path, "r") as f:
        profiles = yaml.safe_load(f)

    if profile_name not in profiles:
        raise KeyError(f"Profile '{profile_name}' not found")

    profile = profiles[profile_name]["outputs"][target_name]

    # ---------------------------
    # FIXED JWT PATH LOGIC
    # ---------------------------
    dbt_dir = os.path.dirname(profile_path)             # .../.dbt
    project_dir = os.path.abspath(os.path.join(dbt_dir, ".."))  # .../project-root

    token_file = "sit_token.txt" if target_name != "prototype" else "prod_token.txt"
    jwt_path = os.path.join(project_dir, "lib", "jwt", token_file)

    if not os.path.exists(jwt_path):
        raise FileNotFoundError(f"JWT token file {jwt_path} not found")

    with open(jwt_path, "r") as f:
        jwt_token = f.read().strip()

    auth = JWTAuthentication(jwt_token)

    # Override catalog for prototype
    catalog = profile["catalog"]
    if target_name == "prototype":
        catalog = "cosmos_nonhcd_iceberg_prototype"

    # Connect
    conn = connect(
        host=profile["host"],
        port=profile["port"],
        catalog=catalog,
        schema=profile["schema"],
        user=None,
        http_scheme=profile.get("http_scheme", "https"),
        auth=auth,
    )

    logging.info(f"Connected to Trino — catalog={catalog}, schema={profile['schema']}")
    return conn, catalog, profile["schema"]


# ============================================================
# 2. Fetch manifest.json via curl
# ============================================================
def fetch_manifest(team_name, out_dir):
    out_path = os.path.join(out_dir, f"manifest_{team_name}.json")

    url = (
        "https://cronos-cao-app2app.prod.aws.jpmchase.net/"
        f"git-webhook-consumer/defer-manifest?teamName={team_name}"
    )

    result = subprocess.run(
        ["curl", "-s", "-o", out_path, "-X", "GET", url],
        capture_output=True
    )

    if result.returncode != 0:
        logging.error(f"Failed to fetch manifest for {team_name}: {result.stderr.decode()}")
        return None

    if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:
        logging.warning(f"Manifest for {team_name} is empty. Skipping.")
        return None

    size = os.path.getsize(out_path)
    logging.info(f"Fetched manifest for {team_name} ({size} bytes)")
    return out_path


# ============================================================
# 3. Build global model + source registries
# ============================================================
def build_registries(manifest_paths):

    registry_models = {}   # ONLY true model origins
    registry_sources = {}  # All sources

    for mpath in manifest_paths:

        if not os.path.exists(mpath) or os.path.getsize(mpath) == 0:
            logging.warning(f"Skipping invalid or empty manifest: {mpath}")
            continue

        try:
            with open(mpath, "r") as f:
                manifest = json.load(f)
        except json.JSONDecodeError:
            logging.error(f"Manifest file {mpath} invalid JSON. Skipping.")
            continue

        project = manifest["metadata"]["project_name"]

        # MODELS (true definitions)
        for _, node in manifest.get("nodes", {}).items():
            if node["resource_type"] == "model":
                name = node["name"].lower()
                key = name  # model name uniquely identifies model across projects

                registry_models[key] = {
                    "name": name,
                    "project": project,
                    "schema": node.get("schema"),
                    "fqn": node.get("fqn"),
                    "type": "model"
                }

        # SOURCES (secondary definitions)
        for _, src in manifest.get("sources", {}).items():
            name = src["name"].lower()
            key = name

            registry_sources[key] = {
                "name": name,
                "project": project,
                "schema": src.get("schema"),
                "fqn": src.get("fqn"),
                "type": "source"
            }

    logging.info(f"Models registry size     : {len(registry_models)}")
    logging.info(f"Sources registry size    : {len(registry_sources)}")

    return registry_models, registry_sources


# ============================================================
# 4. Parse Dependencies using new correct model-first logic
# ============================================================
def parse_dependencies(manifest_paths, registry_models, registry_sources):
    records = []

    for mpath in manifest_paths:

        if not os.path.exists(mpath) or os.path.getsize(mpath) == 0:
            logging.warning(f"Skipping empty manifest during parsing: {mpath}")
            continue

        try:
            with open(mpath, "r") as f:
                manifest = json.load(f)
        except json.JSONDecodeError:
            logging.error(f"Skipping invalid JSON manifest: {mpath}")
            continue

        child_project = manifest["metadata"]["project_name"]

        for _, node in manifest["nodes"].items():
            if node["resource_type"] != "model":
                continue

            child_name = node["name"].lower()
            child_schema = node["schema"]
            child_fqn = ".".join(node["fqn"])
            ts = datetime.now().isoformat()

            # ----------------------------
            # HANDLE REF() dependencies
            # ----------------------------
            for pid in node.get("depends_on", {}).get("nodes", []):
                parent_name = manifest["nodes"].get(pid, {}).get("name", "").lower()
                if not parent_name:
                    continue

                # PRIORITY 1 — true model definitions
                if parent_name in registry_models:
                    parent_info = registry_models[parent_name]
                    relation_type = "ref"

                # PRIORITY 2 — source definition
                elif parent_name in registry_sources:
                    parent_info = registry_sources[parent_name]
                    relation_type = "source"

                else:
                    logging.warning(f"No parent found for {parent_name}")
                    continue

                records.append({
                    "parent_model": parent_info["name"],
                    "parent_project": parent_info["project"],
                    "parent_schema": parent_info["schema"],
                    "child_model": child_name,
                    "child_project": child_project,
                    "child_schema": child_schema,
                    "relation_type": relation_type,
                    "parent_fqn": ".".join(parent_info["fqn"]),
                    "child_fqn": child_fqn,
                    "timestamp": ts
                })

    df = pd.DataFrame(records)
    logging.info(f"Parsed {len(df)} dependency records.")
    return df


# ============================================================
# 5. Create lineage table in Trino
# ============================================================
def create_lineage_table(cursor, catalog, schema, table):
    sql = f"""
    CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table} (
        parent_model VARCHAR,
        parent_project VARCHAR,
        parent_schema VARCHAR,
        child_model VARCHAR,
        child_project VARCHAR,
        child_schema VARCHAR,
        relation_type VARCHAR,
        parent_fqn VARCHAR,
        child_fqn VARCHAR,
        timestamp VARCHAR
    )
    """
    cursor.execute(sql)
    logging.info(f"Ensured table {table} exists.")


# ============================================================
# 6. Batched Insert (100 per batch)
# ============================================================
def merge_records(cursor, catalog, schema, table, df, batch_size=100):

    total = len(df)
    if total == 0:
        logging.info("No records to insert.")
        return

    columns = [
        "parent_model", "parent_project", "parent_schema",
        "child_model", "child_project", "child_schema",
        "relation_type", "parent_fqn", "child_fqn",
        "timestamp"
    ]

    for start in range(0, total, batch_size):
        batch = df.iloc[start:start+batch_size]

        values_sql = []
        for _, row in batch.iterrows():
            vals = [str(row[col]).replace("'", "''") for col in columns]
            values_sql.append("(" + ",".join(f"'{v}'" for v in vals) + ")")

        insert_sql = f"""
        INSERT INTO {catalog}.{schema}.{table}
        ({",".join(columns)})
        VALUES
        {",".join(values_sql)}
        """

        cursor.execute(insert_sql)
        logging.info(f"Inserted {len(batch)} rows ({start+1} → {start + len(batch)})")


# ============================================================
# 7. MAIN WORKFLOW
# ============================================================
if __name__ == "__main__":

    # Read team names from file
    with open("teams.txt", "r") as f:
        TEAMS = [t.strip() for t in f.readlines()]

    OUT_DIR = "manifests"
    os.makedirs(OUT_DIR, exist_ok=True)

    # Fetch manifests
    manifest_paths = []
    for team in TEAMS:
        logging.info(f"Fetching manifest for {team}")
        path = fetch_manifest(team, OUT_DIR)
        if path:
            manifest_paths.append(path)

    # Build global registries
    registry_models, registry_sources = build_registries(manifest_paths)

    # Parse dependencies using new model-first logic
    df = parse_dependencies(manifest_paths, registry_models, registry_sources)

    # Connect to Trino
    conn, catalog, schema = load_dbt_trino_profile(
        profile_path="C:/Users/R732325/ds/projects/dbt-jules/dbt-models-core-services/.dbt/profiles.yml",
        profile_name="dbt-trino-cosmos",
        target_name="prototype"
    )
    cursor = conn.cursor()

    # Create lineage table
    table_name = "global_lineage_purge_engine"
    create_lineage_table(cursor, catalog, schema, table_name)

    # Insert records in batches
    merge_records(cursor, catalog, schema, table_name, df)

    logging.info("All manifests processed and lineage table populated.")
