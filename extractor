import yaml
import json
import os
import re
import logging
import subprocess
import pandas as pd
from datetime import datetime

from trino.dbapi import connect
from trino.auth import JWTAuthentication


# ---------------------------------------------------------------
# Logging setup
# ---------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# ---------------------------------------------------------------
# LOCAL TESTING OVERRIDE (core-services only)
# ---------------------------------------------------------------
USE_LOCAL_OVERRIDE = True   # Set to False after testing
LOCAL_MANIFEST_PATH = r"C:\Users\R732325\ds\projects\dbt-jules\dbt-models-core-services\target\manifest.json"


# ---------------------------------------------------------------
# Load DBT profile & Trino connection
# ---------------------------------------------------------------
def load_dbt_trino_profile(profile_name='dbt-trino-cosmos', target_name='prototype', profile_path=None):
    if profile_path is None:
        profile_path = os.path.expanduser("~/.dbt/profiles.yml")

    if not os.path.exists(profile_path):
        raise FileNotFoundError(f"profiles.yml not found: {profile_path}")

    with open(profile_path, 'r') as f:
        profiles = yaml.safe_load(f)

    profile = profiles[profile_name]['outputs'][target_name]

    token_file = "prod_token.txt" if target_name == "prototype" else "sit_token.txt"
    base_dir = os.path.dirname(os.path.dirname(profile_path))
    jwt_path = os.path.join(base_dir, "lib", "jwt", token_file)

    if not os.path.exists(jwt_path):
        raise FileNotFoundError(f"JWT token file missing: {jwt_path}")

    with open(jwt_path, "r") as f:
        jwt = f.read().strip()

    auth = JWTAuthentication(jwt)

    conn = connect(
        host=profile['host'],
        port=profile['port'],
        user=None,
        catalog=profile['catalog'],
        schema=profile['schema'],
        http_scheme=profile.get('http_scheme', 'https'),
        auth=auth
    )

    return conn, profile['catalog'], profile['schema']


# ---------------------------------------------------------------
# Fetch manifest.json via curl (with override)
# ---------------------------------------------------------------
def fetch_manifest(team_name, out_path):

    # LOCAL OVERRIDE FOR TESTING RCC EXTRACTION
    if USE_LOCAL_OVERRIDE and team_name == "core-services":
        logging.info("Using LOCAL manifest.json for core-services")
        if not os.path.exists(LOCAL_MANIFEST_PATH):
            logging.error("Local manifest file NOT found")
            return False
        import shutil
        shutil.copyfile(LOCAL_MANIFEST_PATH, out_path)
        return True

    # Otherwise use curl (unchanged)
    url = (
        "https://cronos-cao-app2app.prod.aws.jpmchase.net/"
        f"git-webhook-consumer/defer-manifest?teamName={team_name}"
    )

    result = subprocess.run(
        ["curl", "-s", "-o", out_path, "-X", "GET", url],
        capture_output=True
    )

    if result.returncode != 0:
        logging.error(f"Failed fetching {team_name}: {result.stderr}")
        return False

    if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:
        logging.error(f"Manifest empty for {team_name}")
        return False

    logging.info(f"Fetched manifest for {team_name}")
    return True


# ---------------------------------------------------------------
# Build registry from manifest files
# ---------------------------------------------------------------
def build_registry(manifest_paths):
    registry = {}

    for mp in manifest_paths:
        if not os.path.exists(mp):
            continue

        try:
            with open(mp, "r") as f:
                mf = json.load(f)
        except:
            logging.error(f"Invalid JSON: {mp}")
            continue

        nodes = mf.get("nodes", {})
        sources = mf.get("sources", {})

        # MODELS
        for node_id, node in nodes.items():
            if not node_id.startswith("model."):
                continue

            parts = node_id.split(".")
            if len(parts) < 3:
                continue

            project = parts[1]
            model = parts[2]

            # Correct RCC extraction
            rcc_code = node.get("config", {}).get("rcc_code")

            registry[model] = {
                "project": project,
                "resource_type": "model",
                "schema": node.get("schema"),
                "fqn": node.get("fqn", []),
                "file": node.get("original_file_path", ""),
                "rcc_code": rcc_code
            }

        # SOURCES (no rcc_code for sources)
        for sid, src in sources.items():
            parts = sid.split(".")
            if len(parts) < 3:
                continue

            project = parts[1]
            source_name = parts[2]

            registry[source_name] = {
                "project": project,
                "resource_type": "source",
                "schema": src.get("schema"),
                "fqn": src.get("fqn", []),
                "file": None,
                "rcc_code": None   # NO RCC for sources
            }

    logging.info(f"Registry size = {len(registry)}")
    return registry


# ---------------------------------------------------------------
# Dependency extraction patterns
# ---------------------------------------------------------------
REF_PATTERN = re.compile(r"ref\(\s*['\"]([^'\"]+)['\"]\s*\)")
SRC_PATTERN = re.compile(r"source\(\s*['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]+)['\"]\s*\)")
SQL_TABLE_PATTERN = re.compile(
    r"(?:from|join)\s+([a-zA-Z_0-9]+\.[a-zA-Z_0-9_]+\.[a-zA-Z_0-9_]+)",
    re.IGNORECASE
)


# ---------------------------------------------------------------
# Parse dependencies
# ---------------------------------------------------------------
def parse_dependencies(manifest_paths, registry):

    records = []
    all_models = set()
    child_models = set()

    for mp in manifest_paths:
        if not os.path.exists(mp):
            continue

        try:
            with open(mp, "r") as f:
                mf = json.load(f)
        except:
            continue

        nodes = mf.get("nodes", {})

        for node_id, node in nodes.items():
            if not node_id.startswith("model."):
                continue

            parts = node_id.split(".")
            child_model = parts[2]
            child_schema = node.get("schema")

            # Correct RCC extraction for child
            child_rcc_code = node.get("config", {}).get("rcc_code")

            sql = node.get("raw_code", "")

            ts = datetime.now().isoformat()
            all_models.add(child_model)

            # 1. ref()
            for name in REF_PATTERN.findall(sql):
                if name in registry:
                    p = registry[name]
                    records.append({
                        "parent_model": name,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "parent_rcc_code": p["rcc_code"],

                        "child_model": child_model,
                        "child_project": parts[1],
                        "child_schema": child_schema,
                        "child_rcc_code": child_rcc_code,

                        "relation_type": "ref",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": None,
                        "timestamp": ts
                    })
                    child_models.add(child_model)

            # 2. source()
            for src_table, src_col in SRC_PATTERN.findall(sql):
                if src_table in registry:
                    p = registry[src_table]
                    records.append({
                        "parent_model": src_table,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "parent_rcc_code": None,  # sources do NOT have RCC

                        "child_model": child_model,
                        "child_project": parts[1],
                        "child_schema": child_schema,
                        "child_rcc_code": child_rcc_code,

                        "relation_type": "source",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": None,
                        "timestamp": ts
                    })
                    child_models.add(child_model)

            # 3. Direct SQL references
            for tbl in SQL_TABLE_PATTERN.findall(sql):
                if tbl in registry:
                    p = registry[tbl]
                    records.append({
                        "parent_model": tbl,
                        "parent_project": p["project"],
                        "parent_schema": p["schema"],
                        "parent_rcc_code": p["rcc_code"],

                        "child_model": child_model,
                        "child_project": parts[1],
                        "child_schema": child_schema,
                        "child_rcc_code": child_rcc_code,

                        "relation_type": "direct_sql",
                        "parent_fqn": ".".join(p["fqn"]),
                        "child_fqn": None,
                        "timestamp": ts
                    })
                    child_models.add(child_model)

    # Add no-dependency parent-only records
    for model in all_models:
        if model not in child_models:
            p = registry.get(model)
            if p:
                records.append({
                    "parent_model": model,
                    "parent_project": p["project"],
                    "parent_schema": p["schema"],
                    "parent_rcc_code": p["rcc_code"],

                    "child_model": None,
                    "child_project": None,
                    "child_schema": None,
                    "child_rcc_code": None,

                    "relation_type": "no_dependency",
                    "parent_fqn": ".".join(p["fqn"]),
                    "child_fqn": None,
                    "timestamp": datetime.now().isoformat()
                })

    df = pd.DataFrame(records)
    logging.info(f"Parsed dependencies = {len(df)}")
    return df


# ---------------------------------------------------------------
# Create lineage table
# ---------------------------------------------------------------
def create_lineage_table(cursor, catalog, schema):
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {catalog}.{schema}.global_lineage_purge_engine (
            parent_model VARCHAR,
            parent_project VARCHAR,
            parent_schema VARCHAR,
            parent_rcc_code VARCHAR,
            child_model VARCHAR,
            child_project VARCHAR,
            child_schema VARCHAR,
            child_rcc_code VARCHAR,
            relation_type VARCHAR,
            parent_fqn VARCHAR,
            child_fqn VARCHAR,
            timestamp VARCHAR
        )
    """)
    logging.info("Ensured lineage table exists.")


# ---------------------------------------------------------------
# Dedupe logic
# ---------------------------------------------------------------
def dedupe_records(df):
    if df.empty:
        return df

    df = df.drop_duplicates(
        subset=[
            "parent_model", "parent_project", "child_model",
            "child_project", "relation_type", "parent_fqn"
        ],
        keep="last"
    )

    logging.info(f"After dedupe: {len(df)} rows")
    return df


# ---------------------------------------------------------------
# Batch merge (100 rows)
# ---------------------------------------------------------------
def merge_records(cursor, catalog, schema, df, batch_size=100):

    if df.empty:
        logging.info("No records to merge.")
        return

    total = len(df)
    logging.info(f"Merging {total} records in batches of {batch_size}")

    cols = [
        "parent_model", "parent_project", "parent_schema", "parent_rcc_code",
        "child_model", "child_project", "child_schema", "child_rcc_code",
        "relation_type", "parent_fqn", "child_fqn", "timestamp"
    ]

    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = df.iloc[start:end]

        values_sql = []
        for _, row in batch.iterrows():
            vals = [row[c] if pd.notnull(row[c]) else "NULL" for c in cols]
            vals = [str(v).replace("'", "") for v in vals]
            values_sql.append(
                f"('{vals[0]}','{vals[1]}','{vals[2]}','{vals[3]}','{vals[4]}','{vals[5]}','{vals[6]}','{vals[7]}','{vals[8]}','{vals[9]}','{vals[10]}','{vals[11]}')"
            )

        values_block = ",\n".join(values_sql)

        merge_sql = f"""
        MERGE INTO {catalog}.{schema}.global_lineage_purge_engine t
        USING (
            VALUES
            {values_block}
        ) AS s (
            parent_model, parent_project, parent_schema, parent_rcc_code,
            child_model, child_project, child_schema, child_rcc_code,
            relation_type, parent_fqn, child_fqn, timestamp
        )
        ON (
            t.parent_model = s.parent_model AND
            t.child_model = s.child_model AND
            t.relation_type = s.relation_type
        )
        WHEN MATCHED THEN UPDATE SET
            timestamp = s.timestamp,
            parent_rcc_code = s.parent_rcc_code,
            child_rcc_code = s.child_rcc_code
        WHEN NOT MATCHED THEN INSERT VALUES (
            s.parent_model, s.parent_project, s.parent_schema, s.parent_rcc_code,
            s.child_model, s.child_project, s.child_schema, s.child_rcc_code,
            s.relation_type, s.parent_fqn, s.child_fqn, s.timestamp
        )
        """

        cursor.execute(merge_sql)
        logging.info(f"Merged rows {start+1} â†’ {end}")

    logging.info("Batch merge complete.")


# ---------------------------------------------------------------
# MAIN WORKFLOW
# ---------------------------------------------------------------
if __name__ == "__main__":

    teams = [
        "core-services", "core-services-hecd", "cyber-ops", "dfas", "tgt-metrics",
        "intel", "resiliency", "techgrc", "aa", "assurance", "dlp", "ccm", "tpgr"
    ]

    manifest_paths = [f"manifests/manifest_{t}.json" for t in teams]

    for t, p in zip(teams, manifest_paths):
        fetch_manifest(t, p)

    registry = build_registry(manifest_paths)
    df = parse_dependencies(manifest_paths, registry)

    conn, catalog, schema = load_dbt_trino_profile(
        profile_path="C:/Users/R732325/ds/projects/dbt-jules/dbt-models-core-services/.dbt/profiles.yml"
    )

    cursor = conn.cursor()

    if schema == "prototype":
        catalog = catalog + "_prototype"

    create_lineage_table(cursor, catalog, schema)
    df = dedupe_records(df)
    merge_records(cursor, catalog, schema, df, batch_size=100)

    logging.info("Lineage extraction completed successfully!")
