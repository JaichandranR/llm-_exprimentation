import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import psycopg2
from ecdpawssession.session_builder import SessionBuilder

# Postgres connection setup
USERNAME = ''  # Leave blank for IAM authentication
DOMAIN = 'naeast'  # ENTER DOMAIN
ROLE_ARN = 'arn:aws:iam::ROLE/90177-aurora-app-admin'  # ENTER ROLE ARN
ENDPOINT = 'proxy-postgres-nlb.elb.us-east-1.amazonaws.com'  # ENTER DB NLB PROXY
CLUSTER = 'aurora-cluster.cluster-c1wayscsgqcd.us-east-1.rds.amazonaws.com'  # ENTER PG Cluster
PORT = '6160'  # ENTER PORT
USER = 'llmdbAuroraAppAdmin'  # ENTER USER
REGION = 'us-east-1'  # ENTER REGION
DBNAME = 'llmdb'  # ENTER DB

# Generate RDS token for authentication
session = SessionBuilder.cli(
    role_arn=ROLE_ARN, 
    region=REGION, 
    username=USERNAME, 
    domain=DOMAIN, 
    store_password=True
).with_auto_renew().build()

client = session.Client('rds')
token = client.generate_db_auth_token(
    DBHostname=CLUSTER, 
    Port=PORT, 
    DBUsername=USER, 
    Region=REGION
)

# Connect to the database
def get_postgres_connection():
    try:
        conn = psycopg2.connect(
            host=ENDPOINT,
            user=USER,
            password=token,
            port=PORT,
            database=DBNAME
        )
        print("Database connection successful.")
        return conn
    except Exception as e:
        print(f"Database connection failed due to {e}")
        raise

# Hugging Face model setup for embeddings
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Function to compute embeddings
def compute_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embeddings

# Function to load data, compute embeddings, and update the embeddings table
def update_embeddings(file_path, id_column, description_column, source_table):
    conn = get_postgres_connection()
    cursor = conn.cursor()
    
    try:
        # Load data
        df = pd.read_csv(file_path)
        print(f"Loaded {len(df)} rows from {file_path}")

        # Compute embeddings
        df["embedding"] = df[description_column].apply(
            lambda x: compute_embedding(x) if pd.notnull(x) else np.zeros(768)
        )
        
        # Insert or update embeddings in the database
        insert_query = """
            INSERT INTO embeddings (data_id, embedding, metadata)
            VALUES (%s, %s, %s)
            ON CONFLICT (data_id)
            DO UPDATE SET embedding = EXCLUDED.embedding, metadata = EXCLUDED.metadata;
        """
        for _, row in df.iterrows():
            metadata = {"source_table": source_table, description_column: row[description_column]}
            cursor.execute(insert_query, (row[id_column], row["embedding"].tolist(), json.dumps(metadata)))
        
        conn.commit()
        print(f"Embeddings for {source_table} updated successfully.")
    except Exception as e:
        print(f"Failed to update embeddings for {source_table}: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

# Manually call this function for each table
update_embeddings("MITRE_Technique.csv", "TechniqueID", "TechniqueDesc", "mitre_technique")
update_embeddings("MITRE_SubTechnique.csv", "SubTechniqueID", "SubTechniqueDesc", "mitre_subtechnique")
update_embeddings("MITRE_Mitigation.csv", "MitigationID", "MitigationDesc", "mitre_mitigation")
update_embeddings("MITRE_Tactic.csv", "TacticID", "TacticDescription", "mitre_tactic")
update_embeddings("MITRE_ID_Mapping.csv", "TechniqueID", "TechniqueID", "mitre_id_mapping")
