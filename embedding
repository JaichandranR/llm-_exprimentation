import pandas as pd
from sqlalchemy import create_engine, text
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

# Step 1: Set up Postgres connection
DATABASE_URI = "postgresql+psycopg2://username:password@localhost/llm_lookup_db"
engine = create_engine(DATABASE_URI)

# Step 2: Hugging Face model setup for embeddings
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Function to compute embeddings
def compute_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # Average the token embeddings to create sentence embeddings
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embeddings

# Step 3: Load CSV Data and Generate Embeddings
def load_and_store_embeddings(file_path, id_column, description_column, source_table):
    df = pd.read_csv(file_path)
    # Compute embeddings
    df["embedding"] = df[description_column].apply(
        lambda x: compute_embedding(x) if pd.notnull(x) else np.zeros(768)
    )

    # Store embeddings in the embeddings table
    insert_query = text("""
        INSERT INTO embeddings (data_id, embedding, metadata)
        VALUES (:data_id, :embedding, :metadata)
    """)
    with engine.connect() as conn:
        for _, row in df.iterrows():
            conn.execute(insert_query, {
                "data_id": row[id_column],
                "embedding": row["embedding"].tolist(),
                "metadata": {"source_table": source_table, description_column: row[description_column]}
            })
    print(f"Embeddings for {source_table} stored successfully.")

# Load all 5 tables into embeddings
load_and_store_embeddings("MITRE_Technique.csv", "TechniqueID", "TechniqueDesc", "mitre_technique")
load_and_store_embeddings("MITRE_SubTechnique.csv", "SubTechniqueID", "SubTechniqueDesc", "mitre_subtechnique")
load_and_store_embeddings("MITRE_Mitigation.csv", "MitigationID", "MitigationDesc", "mitre_mitigation")
load_and_store_embeddings("MITRE_Tactic.csv", "TacticID", "TacticDescription", "mitre_tactic")
load_and_store_embeddings("MITRE_ID_Mapping.csv", "TechniqueID", "TechniqueID", "mitre_id_mapping")  # Use TechniqueID as description for mapping

# Step 4: Relational Query to Fetch Sub-Techniques
def get_related_subtechniques(engine, technique_id):
    query = text("""
        SELECT s.SubTechniqueID, s.SubTechniqueName, s.SubTechniqueDesc
        FROM mitre_subtechnique s
        INNER JOIN mitre_id_mapping m ON s.SubTechniqueID = m.SubTechniqueID
        WHERE m.TechniqueID = :technique_id
    """)
    with engine.connect() as conn:
        result = conn.execute(query, {"technique_id": technique_id}).fetchall()
    return result

# Step 5: Search Similar Embeddings Using pgvector
def search_similar_embeddings(query_embedding, top_k=5):
    query = text("""
        SELECT data_id, metadata, embedding <-> :query_embedding AS distance
        FROM embeddings
        ORDER BY distance ASC
        LIMIT :top_k
    """)
    with engine.connect() as conn:
        result = conn.execute(query, {
            "query_embedding": query_embedding.tolist(),
            "top_k": top_k
        }).fetchall()
    return result

# Step 6: Combine Results for LLM Context
def create_context_for_llm_combined(engine, query_embedding, technique_id):
    # Relational query: Get directly related sub-techniques
    related_subtechniques = get_related_subtechniques(engine, technique_id)

    # Embedding-based lookup: Find semantically similar entries
    similar_embeddings = search_similar_embeddings(query_embedding)

    # Format the context
    context = "Related Sub-Techniques (Relational Query):\n"
    for subtech in related_subtechniques:
        context += f"SubTechniqueID: {subtech['SubTechniqueID']}, Name: {subtech['SubTechniqueName']}, Description: {subtech['SubTechniqueDesc']}\n"

    context += "\nRelated Entries (Embedding Lookup):\n"
    for row in similar_embeddings:
        context += f"ID: {row['data_id']}, Metadata: {row['metadata']}\n"

    return context

# Step 7: Pass Context to LLM
def llm_infer(api_url, api_key, prompt):
    import requests
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key
    }
    data = {
        "model": "your-llm-model",
        "prompt": prompt,
        "temperature": 0.7
    }
    response = requests.post(api_url, headers=headers, json=data)
    return response.json()

# Example Usage
query_text = "Detect suspicious activity in user accounts."
query_embedding = compute_embedding(query_text)
technique_id = "T1566"

# Create LLM context
context = create_context_for_llm_combined(engine, query_embedding, technique_id)

# Prepare prompt and call LLM
llm_prompt = f"""
You are an expert in cybersecurity. Analyze the following context and provide insights:

{context}
"""
api_url = "https://your-llm-api-endpoint"
api_key = "your-api-key"
response = llm_infer(api_url, api_key, llm_prompt)

print("LLM Response:")
print(response)
