@patch('src.main.python.iceberg_compaction.SparkSession')
def test_expiry_only_updates_existing_compacted_partitions(self, mock_spark):
    from src.main.python.iceberg_compaction import update_after_expiry_metrics
    from pyspark.sql import Row
    from pyspark.sql.types import StructType, StructField, IntegerType, LongType

    # --- Setup test DataFrame with real values for filtering ---
    schema = StructType([
        StructField("partition", StructType([
            StructField("time_hour", IntegerType(), True)
        ]), True),
        StructField("record_count", IntegerType(), True),
        StructField("file_count", IntegerType(), True),
        StructField("total_data_file_size_in_bytes", LongType(), True)
    ])

    partition_data = [
        Row(partition={"time_hour": 100}, record_count=100, file_count=1, total_data_file_size_in_bytes=10485760),
        Row(partition={"time_hour": 101}, record_count=200, file_count=2, total_data_file_size_in_bytes=20971520)
    ]

    mock_df = mock_spark.createDataFrame(partition_data, schema)
    mock_spark.table.return_value = mock_df

    # Mock metric table
    metric_df = mock_spark.createDataFrame([
        Row(
            table_name="mock_table",
            partition_name=str({"time_hour": 100}),
            record_count_before_compact=120,
            file_count_before_compact=2,
            total_size_before_compact=15.0,
            compaction_status="completed",
            compaction_datetime=None,
            record_count_after_expiry=None,
            file_count_after_expiry=None,
            total_size_after_expiry=None,
            snapshot_expired_status=None,
            snapshot_expired_datetime=None
        )
    ])
    metric_df.createOrReplaceTempView("mock_metrics_table")

    # Patch SQL call
    def mock_sql(query):
        if "SELECT * FROM mock_metrics_table" in query:
            return metric_df
        return MagicMock()

    mock_spark.sql.side_effect = mock_sql

    update_after_expiry_metrics(
        spark=mock_spark,
        full_table="mock_table",
        metric_table="mock_metrics_table",
        current_hour=300  # expiry cutoff is 108 (300 - 192)
    )

    result = mock_spark.sql("SELECT * FROM mock_metrics_table").collect()
    assert len(result) == 1
    assert result[0]["partition_name"] == str({"time_hour": 100})
    assert result[0]["snapshot_expired_status"] == "expired"
