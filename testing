import pytest
from unittest.mock import MagicMock

from src.main.python.iceberg_compaction import update_after_expiry_metrics, capture_metrics

@pytest.fixture

def mock_spark():
    mock_spark = MagicMock()
    mock_df = MagicMock()

    mock_spark.createDataFrame.return_value = mock_df

    mock_df.createOrReplaceTempView.return_value = None
    mock_df.filter.return_value = mock_df
    mock_df.withColumn.return_value = mock_df
    mock_df.select.return_value = mock_df
    mock_df.collect.return_value = [
        {
            "partition_name": str({"time_hour": 100}),
            "snapshot_expired_status": "expired"
        }
    ]
    mock_df.__getitem__.side_effect = lambda key: MagicMock()

    return mock_spark

def test_expiry_only_updates_existing_compacted_partitions(mock_spark):
    df = mock_spark.createDataFrame([
        {"partition": {"time_hour": 100}, "record_count": 100, "file_count": 1, "total_data_file_size_in_bytes": 10485760},
        {"partition": {"time_hour": 101}, "record_count": 200, "file_count": 2, "total_data_file_size_in_bytes": 20971520}
    ])
    df.createOrReplaceTempView("mock_table.partitions")

    metric_df = mock_spark.createDataFrame([
        {
            "table_name": "mock_table",
            "partition_name": str({"time_hour": 100}),
            "record_count_before_compact": 120,
            "file_count_before_compact": 2,
            "total_size_before_compact": 15.0,
            "compaction_status": "completed",
            "compaction_datetime": None,
            "record_count_after_expiry": None,
            "file_count_after_expiry": None,
            "total_size_after_expiry": None,
            "snapshot_expired_status": None,
            "snapshot_expired_datetime": None
        }
    ])
    metric_df.createOrReplaceTempView("mock_metrics_table")

    update_after_expiry_metrics(
        spark=mock_spark,
        full_table="mock_table",
        metric_table="mock_metrics_table",
        current_hour=300
    )

    result = mock_spark.sql("SELECT * FROM mock_metrics_table").collect()
    assert len(result) == 1
    assert result[0]['partition_name'] == str({"time_hour": 100})
    assert result[0]['snapshot_expired_status'] == "expired"

def test_expiry_skips_untracked_partitions(mock_spark):
    df = mock_spark.createDataFrame([
        {"partition": {"time_hour": 101}, "record_count": 200, "file_count": 2, "total_data_file_size_in_bytes": 20971520}
    ])
    df.createOrReplaceTempView("mock_table.partitions")

    metric_df = mock_spark.createDataFrame([])
    metric_df.createOrReplaceTempView("mock_metrics_table")

    update_after_expiry_metrics(
        spark=mock_spark,
        full_table="mock_table",
        metric_table="mock_metrics_table",
        current_hour=300
    )

    result = mock_spark.sql("SELECT * FROM mock_metrics_table").collect()
    assert result == []

def test_update_after_expiry_includes_compaction_metadata(mock_spark):
    df = mock_spark.createDataFrame([
        {"partition": {"time_hour": 100}, "record_count": 100, "file_count": 1, "total_data_file_size_in_bytes": 10485760}
    ])
    df.createOrReplaceTempView("mock_table.partitions")

    metric_df = mock_spark.createDataFrame([
        {
            "table_name": "mock_table",
            "partition_name": str({"time_hour": 100}),
            "record_count_before_compact": 120,
            "file_count_before_compact": 2,
            "total_size_before_compact": 15.0,
            "compaction_status": "completed",
            "compaction_datetime": None,
            "record_count_after_expiry": None,
            "file_count_after_expiry": None,
            "total_size_after_expiry": None,
            "snapshot_expired_status": None,
            "snapshot_expired_datetime": None
        }
    ])
    metric_df.createOrReplaceTempView("mock_metrics_table")

    update_after_expiry_metrics(
        spark=mock_spark,
        full_table="mock_table",
        metric_table="mock_metrics_table",
        current_hour=300
    )

    result = mock_spark.sql("SELECT * FROM mock_metrics_table").collect()
    assert len(result) == 1
    assert result[0]['partition_name'] == str({"time_hour": 100})
    assert result[0]['snapshot_expired_status'] == "expired"

def test_no_partitions_to_compact(mock_spark):
    with pytest.raises(Exception):
        capture_metrics(mock_spark, "mock_table", "mock_metrics", ["time_hour"], 300, [])

def test_snapshot_expiration(mock_spark):
    mock_df = mock_spark.createDataFrame([
        {"time_hour_val": 276},  # current_hour = 300, within range
        {"time_hour_val": 40}    # outside range
    ])
    mock_df.filter.return_value = mock_df
    mock_df.__getitem__.side_effect = lambda key: [row[key] for row in mock_df.collect.return_value]
    mock_df.collect.return_value = mock_df.collect.return_value

    before_df = mock_df.filter(
        (mock_df["time_hour_val"] >= 276) & (mock_df["time_hour_val"] < 300)
    )
    assert before_df is not None
