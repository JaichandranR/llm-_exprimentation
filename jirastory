Story 1 – Design & Build the RCC Retention Purge Macro (purge_by_rcc)

As a Data Engineer,
I need to design and develop a reusable dbt macro that can identify and purge records older than the defined retention period (based on RCC codes),
so that our Iceberg tables comply with corporate retention rules and storage stays optimized.

Description:

Create macro purge_by_rcc under macros/purge_by_rcc.sql.

Macro accepts parameters model_name and execute_delete.

Reads RCC configuration from model’s schema.yml (rcc_code, purge_date_field).

Looks up retention period from 88057_jade_data_retention.

Maps units (D, M, Y) → (DAY, MONTH, YEAR).

Supports both dry-run (audit) and delete execution modes.

Provides detailed dbt log output (RCC code, ruleperiod, count, purge result).

Business Value:
Ensures data is retained only for the defined duration and removes stale data automatically, reducing storage costs.

Acceptance Criteria:

✅ Runs with:
dbt run-operation purge_by_rcc --args "{\"model_name\": \"110930_ame_co_driver\"}"

✅ Accurately fetches ruleperiod from 88057_jade_data_retention.

✅ Correctly maps unit codes.

✅ Works for Trino 1.8.0 without syntax errors.

✅ Logs all purge actions and errors clearly.

Testing Notes:

Validate dry-run vs delete outputs.

Verify SQL statements in Trino history.

Confirm purge_date_field used correctly.

Dependencies: None (first implementation).

Story 2 – Define RCC Configuration in schema.yml Files

As a Data Engineer,
I need to add RCC-related configuration to each model’s schema.yml,
so that the purge macro knows which column to evaluate for retention and which RCC rule to apply.

Description:

Add to eligible models:

config:
  rcc_code: DBU090D
  purge_date_field: cosmosingestedat


Ensure only incremental or snapshot models have this config.

Exclude models with on_table_exists: drop or replace.

Business Value:
Centralizes purge configuration per model and aligns dbt metadata with retention governance.

Acceptance Criteria:

✅ All eligible models have rcc_code + purge_date_field.

✅ Ineligible (drop-replace) models skip purge config.

✅ dbt compile and parse validate schema successfully.

Testing Notes:
Run dbt parse → no config errors.
Inspect logs for correct config retrieval.

Dependencies: Story 1.

Story 3 – Extend Macro to Skip Transient (Drop/Replace) Models

As a Data Engineer,
I need the purge macro to automatically skip models that recreate full tables each run (on_table_exists: drop or replace),
so that unnecessary purge queries aren’t executed on ephemeral or transient tables.

Description:

In macro, detect materialized == 'table' and on_table_exists in ('drop','replace').

Log a clear message and skip purge execution.

Continue normal purge for incremental and snapshot models.

Business Value:
Prevents redundant purge operations and avoids potential locking issues on recreated tables.

Acceptance Criteria:

✅ Skips transient models automatically.

✅ Logs:
“⚠️ Skipping purge for transient model (on_table_exists=drop): <model_name>”

✅ Still runs for incremental/snapshot models.

Testing Notes:
Simulate both transient and incremental models.
Validate skip behavior and log output.

Dependencies: Story 1 & 2.

Story 4 – Implement Audit-Only (Dry-Run) Mode for Purge

As a Data Engineer,
I want to run the purge macro in a dry-run (audit) mode,
so that I can preview how many rows would be deleted before executing the actual purge.

Description:

Extend macro to support execute_delete=False (default).

Generate report/log showing models and eligible row counts.

Optionally store audit summary in an audit_purge_summary table.

Business Value:
Allows safe validation of purge scope before deletion — reducing production risk.

Acceptance Criteria:

✅ Dry-run lists all eligible records count per model.

✅ No delete query runs when execute_delete=False.

✅ Logs summary clearly in dbt output.

Testing Notes:
Run macro in dry mode.
Compare reported counts vs manual SQL count.

Dependencies: Story 1 & 3.

Story 5 – Apply Purge Logic to Incremental and Snapshot Models

As a Data Engineer,
I want the purge macro to handle incremental and snapshot tables correctly,
so that historical records are cleaned based on retention while active records remain intact.

Description:

Support purge for incremental/merge models with history.

For snapshot models, purge rows where valid_to or purge_date_field exceeds retention period.

Preserve active snapshot versions (current_flag = true).

Business Value:
Keeps long-running snapshot and incremental tables compact, improving query performance.

Acceptance Criteria:

✅ Purge runs correctly for snapshot and incremental models.

✅ Excludes current snapshot rows.

✅ Verified via dbt run-operation + Trino logs.

Testing Notes:
Use a sample snapshot with historical versions.
Validate that only expired records are purged.

Dependencies: Story 3.

Story 6 – End-to-End Testing and Documentation

As a Data Engineer,
I need to perform integrated testing and create documentation for the purge framework,
so that both developers and operations teams can understand, use, and troubleshoot the process.

Description:

Run full POC validation across all model types:

Drop/replace

Incremental

Snapshot

Document usage examples, known issues, limitations.

Create docs/purge_by_rcc.md and update README.md.

Capture before/after row counts in audit log.

Business Value:
Provides clarity and operational readiness for production rollout.

Acceptance Criteria:

✅ Tested on Trino 1.8.0 and dbt 1.8.0.

✅ Documentation includes commands, dry-run vs delete, troubleshooting.

✅ Audit output validated.

Testing Notes:
Run through orchestrated job flow and validate logs.

Dependencies: Stories 1–5.

Story 7 – Schedule and Monitor RCC Purge via Kestra/Jenkins

As a Data Engineer,
I need to automate the purge process through our orchestration layer (Kestra/Jenkins),
so that purging happens periodically without manual intervention.

Description:

Create Kestra/Jenkins job to call dbt run-operation purge_by_rcc.

Schedule dry-run daily, purge weekly.

Send Slack/email notifications when deletes occur.

Retain execution history for audits.

Business Value:
Automates maintenance, ensures consistent compliance, and improves observability.

Acceptance Criteria:

✅ Job triggers purge macro automatically.

✅ Alerts include model name and row counts purged.

✅ Logs archived for 90 days.

Testing Notes:
Run test pipeline and confirm alert delivery and retries.

Dependencies: Story 6.

Deliverables Summary
Deliverable	Description
purge_by_rcc.sql	Final purge macro with unit-mapping and skip logic
schema.yml updates	RCC + purge_date_field configs
audit_purge_summary	Optional audit table
docs/purge_by_rcc.md	Documentation & runbook
CI/CD job (Kestra/Jenkins)	Automated purge schedule

Would you like me to convert this into a Jira-importable CSV or Markdown table (each story as one row with columns like Summary, As a, I want, So that, Acceptance Criteria, Dependencies)?
That would let you paste or bulk-import directly into Jira.
