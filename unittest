from pyspark.sql import SparkSession
import boto3
import json
from datetime import datetime

# Set catalog/database/table config
catalog_name = "cosmos_nonhcd_iceberg"
database_name = "common_data"
output_table = "iceberg_snapshot_inventory"
s3_warehouse_path = "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/"

# Init Spark session
spark = (SparkSession.builder
         .appName("IcebergSnapshotInventory")
         .config(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
         .config(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
         .config(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
         .config(f"spark.sql.catalog.{catalog_name}.warehouse", s3_warehouse_path)
         .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
         .getOrCreate())

# Boto3 Glue client
glue = boto3.client("glue")

def is_valid_iceberg_table(database, table_name):
    try:
        response = glue.get_table(DatabaseName=database, Name=table_name)
        params = response['Table'].get('Parameters', {})
        iceberg = params.get('table_type', '').upper() == 'ICEBERG'
        metadata_location = params.get('metadata_location')
        return iceberg and metadata_location is not None
    except Exception as e:
        print(f"Failed to check type for {table_name}: {e}")
        return False

# Get list of tables from Glue
response = glue.get_tables(DatabaseName=database_name)
table_names = [table['Name'] for table in response['TableList']]

inventory_rows = []

# Process Iceberg tables only
for table_name in table_names:
    if not is_valid_iceberg_table(database_name, table_name):
        print(f"Skipping non-Iceberg or invalid Iceberg table: {table_name}")
        continue

    try:
        snapshots_df = spark.sql(f"SELECT * FROM {catalog_name}.{database_name}.{table_name}__snapshots")
        metadata_location_df = spark.sql(f"SELECT * FROM {catalog_name}.{database_name}.{table_name}__metadata_log_entries")

        latest_metadata = metadata_location_df.orderBy(metadata_location_df.timestamp_ms.desc()).first()
        metadata_path = latest_metadata.metadata_file if latest_metadata else None

        for row in snapshots_df.collect():
            inventory_rows.append({
                "table_name": table_name,
                "snapshot_id": row.get("snapshot_id"),
                "timestamp_ms": row.get("committed_at"),
                "parent_id": row.get("parent_id"),
                "operation": row.get("operation"),
                "manifest_list": row.get("manifest_list"),
                "metadata_file": metadata_path,
                "ingested_at": datetime.utcnow().isoformat()
            })

    except Exception as e:
        print(f"❌ Error processing {table_name}: {e}")

# Save to Iceberg table
if inventory_rows:
    inventory_df = spark.createDataFrame(inventory_rows)
    inventory_df.writeTo(f"{catalog_name}.{database_name}.{output_table}").using("iceberg").createOrReplace()
    print("✅ Iceberg snapshot inventory written.")
else:
    print("⚠️ No Iceberg snapshots found.")
