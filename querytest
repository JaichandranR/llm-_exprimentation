// --- BEGIN: metric table enhancement ---

from pyspark.sql.functions import col, lit, current_timestamp

def create_metric_table_if_not_exists(spark, metric_table: str):
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {metric_table} (
            table_name STRING,
            partition_name STRING,
            record_count_before_compact INT,
            file_count_before_compact INT,
            total_size_before_compact DOUBLE,
            record_count_after_expiry INT,
            file_count_after_expiry INT,
            total_size_after_expiry DOUBLE,
            compaction_status STRING,
            compaction_datetime TIMESTAMP,
            snapshot_expired_status STRING,
            snapshot_expired_datetime TIMESTAMP
        )
    """)

def capture_metrics(spark, full_table: str, metric_table: str, partition_fields: list, current_hour: int, compacted_partition_hours: list):
    df = spark.table(full_table + ".partitions")
    for field in partition_fields:
        df = df.withColumn(f"{field}_val", df[f"partition.{field}"].cast("int"))
    df = df.withColumn("total_size_mb", df["total_data_file_size_in_bytes"] / (1024 * 1024))
    df = df.withColumn("partition_name", df["partition"].cast("string"))

    before_df = df.filter((df["time_hour_val"] >= current_hour - 24) & (df["time_hour_val"] < current_hour)) \
        .withColumnRenamed("record_count", "record_count_before_compact") \
        .withColumnRenamed("file_count", "file_count_before_compact") \
        .withColumnRenamed("total_size_mb", "total_size_before_compact") \
        .withColumn("table_name", lit(full_table))

    compacted_df = df.filter(col("time_hour_val").isin(compacted_partition_hours)) \
        .select("partition_name").distinct() \
        .withColumn("table_name", lit(full_table)) \
        .withColumn("compaction_status", lit("completed")) \
        .withColumn("compaction_datetime", current_timestamp())

    combined_df = before_df.join(compacted_df, on=["table_name", "partition_name"], how="outer")
    combined_df.createOrReplaceTempView("temp_metrics")

    spark.sql(f"""
        MERGE INTO {metric_table} t
        USING temp_metrics s
        ON t.table_name = s.table_name AND t.partition_name = s.partition_name
        WHEN MATCHED THEN UPDATE SET
            t.record_count_before_compact = s.record_count_before_compact,
            t.file_count_before_compact = s.file_count_before_compact,
            t.total_size_before_compact = s.total_size_before_compact,
            t.compaction_status = s.compaction_status,
            t.compaction_datetime = s.compaction_datetime
        WHEN NOT MATCHED THEN INSERT *
    """)

def update_after_expiry_metrics(spark, full_table: str, metric_table: str, current_hour: int):
    df = spark.table(full_table + ".partitions")
    df = df.withColumn("time_hour_val", df["partition.time_hour"].cast("int"))
    df = df.withColumn("total_size_mb", df["total_data_file_size_in_bytes"] / (1024 * 1024))
    df = df.withColumn("partition_name", df["partition"].cast("string"))

    expiry_df = df.filter(df["time_hour_val"] < current_hour - 192) \
        .withColumnRenamed("record_count", "record_count_after_expiry") \
        .withColumnRenamed("file_count", "file_count_after_expiry") \
        .withColumnRenamed("total_size_mb", "total_size_after_expiry") \
        .withColumn("snapshot_expired_status", lit("expired")) \
        .withColumn("snapshot_expired_datetime", current_timestamp()) \
        .withColumn("table_name", lit(full_table))

    expiry_df.createOrReplaceTempView("temp_expiry_metrics")

    spark.sql(f"""
        MERGE INTO {metric_table} t
        USING temp_expiry_metrics s
        ON t.table_name = s.table_name AND t.partition_name = s.partition_name
        WHEN MATCHED THEN UPDATE SET
            t.record_count_after_expiry = s.record_count_after_expiry,
            t.file_count_after_expiry = s.file_count_after_expiry,
            t.total_size_after_expiry = s.total_size_after_expiry,
            t.snapshot_expired_status = s.snapshot_expired_status,
            t.snapshot_expired_datetime = s.snapshot_expired_datetime
    """)

// --- END: metric table enhancement ---


# === BEGIN: metric integration ===

# Place near top of main()
metric_table = f"{catalog_nm}.{source_db}.table_compaction_metric"
create_metric_table_if_not_exists(spark, metric_table)

# After computing partition_hours
capture_metrics(spark, full_table, metric_table, partition_fields, current_hour, partition_hours)

# After snapshot expiration block success
update_after_expiry_metrics(spark, full_table, metric_table, current_hour)

# === END: metric integration ===
