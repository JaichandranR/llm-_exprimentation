1. Create a similarity search function
The similarity search involves querying the embeddings stored in the database and comparing them to the input query embedding using a similarity metric (e.g., cosine similarity).

Here's the function to perform similarity search:

python
Copy code
import psycopg2
import numpy as np

def get_similar_mitre_embeddings(query_embedding, top_k=5):
    """
    Perform similarity search on the mitre_embeddings table and return the top-k similar rows.
    
    Args:
        query_embedding (list): The embedding of the input query.
        top_k (int): The number of similar results to return.

    Returns:
        List of dictionaries containing the most similar rows.
    """
    conn = get_postgres_connection()
    cursor = conn.cursor()
    
    try:
        # Normalize the query embedding
        query_embedding = normalize_embedding(query_embedding)

        # SQL query to compute cosine similarity
        query = f"""
        SELECT 
            control_id,
            chunk_id,
            source,
            documents,
            embedding,
            (embedding <=> cube(%s)) AS distance  -- Postgres's "<=>" operator computes similarity
        FROM 
            application.mitre_embeddings
        ORDER BY 
            distance ASC  -- Lower distance is more similar
        LIMIT {top_k};
        """
        # Execute query
        cursor.execute(query, (query_embedding,))
        results = cursor.fetchall()
        
        # Process results
        similar_rows = []
        for row in results:
            similar_rows.append({
                "control_id": row[0],
                "chunk_id": row[1],
                "source": row[2],
                "documents": row[3],
                "embedding": row[4],
                "distance": row[5]
            })

        return similar_rows
    except Exception as e:
        print(f"Error during similarity search: {e}")
        return []
    finally:
        cursor.close()
        conn.close()
2. Integrate the LLM for embeddings
To find the most similar MITRE technique or sub-technique ID, follow these steps:

Generate embedding for the input query using the LLM API:
python
Copy code
def generate_query_embedding(query_text):
    """
    Generate embedding for the input query using the hosted LLM API.
    
    Args:
        query_text (str): The query for which to generate the embedding.

    Returns:
        List of floats representing the embedding.
    """
    headers = {
        "Content-Type": "application/json",
        "x-api-key": API_KEY
    }
    data = {
        "model": "MetaLlama318BInstruct",
        "prompt": (
            f"<|begin_of_text|><|start_header_id|>system<|end_header_id|> Generate an embedding for the following query: {query_text} "
            f"<|end_of_text|>"
        ),
        "options": {
            "seed": 1,
            "temperature": 0
        },
        "stream": False
    }
    response = requests.post(API_URL, data=json.dumps(data), headers=headers, timeout=1000)

    if response.status_code == 200:
        result = response.json()
        embedding = result.get("embedding")
        if embedding and isinstance(embedding, list) and len(embedding) > 0:
            return normalize_embedding(embedding)
        else:
            print(f"Invalid embedding received for query: {query_text}")
            return None
    else:
        print(f"Failed to generate embedding for query: {query_text}. API Response: {response.text}")
        return None
Perform similarity search using the generated embedding:
python
Copy code
def find_similar_mitre_techniques(query_text, top_k=5):
    """
    Find similar MITRE techniques and sub-techniques for a given query.
    
    Args:
        query_text (str): The query text to search for similar techniques.
        top_k (int): The number of similar results to return.

    Returns:
        List of dictionaries containing similar MITRE techniques.
    """
    # Step 1: Generate query embedding
    query_embedding = generate_query_embedding(query_text)
    if not query_embedding:
        print(f"Failed to generate embedding for query: {query_text}")
        return []

    # Step 2: Perform similarity search
    similar_results = get_similar_mitre_embeddings(query_embedding, top_k=top_k)

    if similar_results:
        print(f"Top-{top_k} similar results for query '{query_text}':")
        for result in similar_results:
            print(f"Control ID: {result['control_id']}, Source: {result['source']}, Distance: {result['distance']}")
    else:
        print(f"No similar results found for query '{query_text}'.")

    return similar_results
3. Use the function in your workflow
Here's how you can use the above function:

python
Copy code
if __name__ == "__main__":
    # Example query
    user_query = "How to mitigate unauthorized access to critical systems?"

    # Find similar MITRE techniques
    top_similar_techniques = find_similar_mitre_techniques(user_query, top_k=5)

    # Display results
    for technique in top_similar_techniques:
        print(f"Control ID: {technique['control_id']}, Source: {technique['source']}, Document: {technique['documents']}, Distance: {technique['distance']}")
Key Notes:
Embedding and Normalization:

Ensure the query embedding is normalized before performing similarity search to match the embeddings stored in the database.
The stored embeddings in the application.mitre_embeddings table are already normalized.
Distance Calculation:

The <=> operator in Postgres computes cosine distance if the embedding vectors are normalized.
Top-K Results:

Adjust the top_k parameter to retrieve more or fewer similar results.
Indexing for Performance:

Create an index on the embedding column of the mitre_embeddings table to speed up similarity searches:
sql
Copy code
CREATE INDEX ON application.mitre_embeddings USING ivfflat (embedding);
Ensure Postgres is configured with the pgvector extension.
