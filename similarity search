import psycopg2
import numpy as np
from transformers import AutoTokenizer, AutoModel
from scipy.spatial.distance import cosine

# Database connection setup
USERNAME = ''  # Leave blank for IAM authentication
DOMAIN = 'naeast'  # ENTER DOMAIN
ROLE_ARN = 'arn:aws:iam::ROLE/90177-aurora-app-admin'  # ENTER ROLE ARN
ENDPOINT = 'proxy-postgres-nlb.elb.us-east-1.amazonaws.com'  # ENTER DB NLB PROXY
CLUSTER = 'aurora-cluster.cluster-c1wayscsgqcd.us-east-1.rds.amazonaws.com'  # ENTER PG Cluster
PORT = '6160'  # ENTER PORT
USER = 'llmdbAuroraAppAdmin'  # ENTER USER
REGION = 'us-east-1'  # ENTER REGION
DBNAME = 'llmdb'  # ENTER DB

# Generate RDS token for authentication
from ecdpawssession.session_builder import SessionBuilder

session = SessionBuilder.cli(
    role_arn=ROLE_ARN, 
    region=REGION, 
    username=USERNAME, 
    domain=DOMAIN, 
    store_password=True
).with_auto_renew().build()

client = session.client('rds')
token = client.generate_db_auth_token(
    DBHostname=CLUSTER, 
    Port=PORT, 
    DBUsername=USER, 
    Region=REGION
)

def get_postgres_connection():
    try:
        conn = psycopg2.connect(
            host=ENDPOINT,
            user=USER,
            password=token,
            port=PORT,
            database=DBNAME
        )
        print("Database connection successful.")
        return conn
    except Exception as e:
        print(f"Database connection failed due to {e}")
        raise

# Load the tokenizer and model
MODEL_NAME = "BAAI/bge-large-en-v1.5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)

# Generate embedding for the query
def generate_query_embedding(query):
    inputs = tokenizer(query, return_tensors="pt", max_length=512, truncation=True, padding=True)
    outputs = model(**inputs)
    # Use the mean of the last hidden state as the embedding
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()
    # Normalize the embedding
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm
    return embedding

# Perform similarity search in the database
def similarity_search(query, top_k=5):
    conn = get_postgres_connection()
    cursor = conn.cursor()

    try:
        # Generate the query embedding
        query_embedding = generate_query_embedding(query)

        # Fetch embeddings from the database
        cursor.execute("SELECT control_id, chunk_id, source, embedding FROM application.mitre_embeddings")
        rows = cursor.fetchall()

        # Calculate cosine similarity for each embedding
        similarities = []
        for control_id, chunk_id, source, embedding in rows:
            db_embedding = np.array(embedding)
            similarity = 1 - cosine(query_embedding, db_embedding)
            similarities.append((control_id, chunk_id, source, similarity))

        # Sort by similarity in descending order
        similarities = sorted(similarities, key=lambda x: x[3], reverse=True)

        # Return the top-k most similar entries
        return similarities[:top_k]

    except Exception as e:
        print(f"Error during similarity search: {e}")
        return []

    finally:
        cursor.close()
        conn.close()

# Example usage
if __name__ == "__main__":
    query = "Explain directory traversal attack"
    top_results = similarity_search(query, top_k=5)

    print("Top similar entries:")
    for result in top_results:
        print(f"Control ID: {result[0]}, Chunk ID: {result[1]}, Source: {result[2]}, Similarity: {result[3]:.4f}")
