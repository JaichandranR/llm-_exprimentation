def test_get_backlog_partition_hours_filters_correctly(self):
    mock_df = MagicMock(spec=DataFrame)
    
    # Setup Spark read and load return values
    self.mock_spark.read.format.return_value.load.return_value = mock_df
    
    # Setup DataFrame chainable mocks
    mock_df.withColumn.return_value = mock_df
    mock_df.filter.return_value = mock_df
    mock_df.select.return_value = mock_df
    mock_df.rdd.map.return_value.collect.return_value = [485759, 485760]

    # Patch rdd access pattern
    mock_df.rdd = MagicMock()
    mock_df.rdd.map = MagicMock()
    mock_df.rdd.map.return_value.collect.return_value = [485759, 485760]

    result = iceberg_compaction.get_backlog_partition_hours(
        spark=self.mock_spark,
        full_table="test_catalog.test_db.test_table",
        partition_fields=["hour"],
        last_hour_done=485750,
        current_hour=485765,
        batch_size=10
    )

    self.assertEqual(result, [485759, 485760])
