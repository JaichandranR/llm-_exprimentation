    def test_snapshot_expiration_and_orphan_file_removal_called(self):
        with patch("builtins.print") as mock_print:
            iceberg_compaction.args = {
                'catalog_nm': 'test_catalog',
                'table_nm': 'test_table',
                'source_db': 'test_db',
                'expire_snapshots_day': '7',
                'skip_newest_partitions': '0'
            }

            self.mock_spark.sql = MagicMock()
            self.mock_spark.catalog.refreshTable = MagicMock()

            mock_df = MagicMock()
            self.mock_spark.table.return_value = mock_df
            self.mock_spark.read.format.return_value.load.return_value = mock_df

            mock_df.select.return_value = mock_df
            mock_df.filter.return_value = mock_df
            mock_df.withColumn.return_value = mock_df
            mock_df.distinct.return_value = mock_df
            mock_df.join.return_value = mock_df

            # Simulate .first() returning a dict-like row with 'last_compacted_hour'
            mock_df.first.return_value = {'last_compacted_hour': 485760}

            # Support for df['col'] >= value and < value
            def ge_mock(val): return MagicMock(name=f'>={val}')
            def lt_mock(val): return MagicMock(name=f'<{val}')
            mock_df.__getitem__.side_effect = lambda key: MagicMock(__ge__=ge_mock, __lt__=lt_mock)

            mock_df.schema.fields = []  # Avoid schema error

            iceberg_compaction.main()

            self.assertTrue(self.mock_spark.sql.called)
