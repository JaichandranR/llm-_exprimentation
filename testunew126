import sys
import unittest
from unittest.mock import patch, MagicMock
from types import SimpleNamespace
from pyspark.sql import SparkSession, DataFrame
from pyspark.context import SparkContext

# Mock the awsglue module and its submodules
sys.modules['awsglue'] = MagicMock()
sys.modules['awsglue.utils'] = MagicMock()
sys.modules['awsglue.context'] = MagicMock()

# Import target after mocking
from src.main.python import iceberg_compaction

def get_current_hour():
    return 485762

iceberg_compaction.get_current_hour = get_current_hour

# Define AnalysisException for testing
class AnalysisException(Exception):
    pass

iceberg_compaction.AnalysisException = AnalysisException

class TestDataFrameOperations(unittest.TestCase):

    def setUp(self):
        self.mock_spark = MagicMock(spec=SparkSession)
        self.mock_spark_context = MagicMock(spec=SparkContext)
        self.mock_spark._jsc = MagicMock()
        SparkContext._active_spark_context = self.mock_spark_context
        self.mock_spark_context._jsc = MagicMock()

        mock_builder = MagicMock()
        mock_builder.getOrCreate.return_value = self.mock_spark
        patcher = patch('src.main.python.iceberg_compaction.SparkSession.builder', return_value=mock_builder)
        self.addCleanup(patcher.stop)
        patcher.start()

    def test_snapshot_expiration_and_orphan_file_removal_called(self):
        with patch("builtins.print") as mock_print:
            iceberg_compaction.args = {
                'catalog_nm': 'test_catalog',
                'table_nm': 'test_table',
                'source_db': 'test_db',
                'expire_snapshots_day': '7',
                'skip_newest_partitions': '0'
            }

            self.mock_spark.sql = MagicMock()
            self.mock_spark.catalog.refreshTable = MagicMock()

            mock_df = MagicMock()
            self.mock_spark.table.return_value = mock_df
            self.mock_spark.read.format.return_value.load.return_value = mock_df

            mock_df.select.return_value = mock_df
            mock_df.filter.return_value = mock_df
            mock_df.withColumn.return_value = mock_df
            mock_df.distinct.return_value = mock_df
            mock_df.join.return_value = mock_df
            mock_df.first.return_value = {"last_compacted_hour": 485760}

            mock_df.__getitem__.return_value.__ge__.return_value = MagicMock()
            mock_df.__getitem__.return_value.__lt__.return_value = MagicMock()
            mock_df.schema.fields = []  # Prevent schema-related errors

            iceberg_compaction.main()

            self.assertTrue(self.mock_spark.sql.called)

    def test_get_backlog_partition_hours_filters_correctly(self):
        mock_df = MagicMock()
        mock_df.withColumn.return_value = mock_df
        mock_df.filter.return_value = mock_df
        mock_df.select.return_value = mock_df
        mock_df.distinct.return_value = mock_df
        mock_df.orderBy.return_value = mock_df
        mock_df.limit.return_value = mock_df
        mock_df.rdd.flatMap.return_value.collect.return_value = [485760, 485761]

        self.mock_spark.read.format.return_value.load.return_value = mock_df

        result = iceberg_compaction.get_backlog_partition_hours(
            self.mock_spark,
            full_table="db.tbl",
            partition_fields=["time_hour"],
            last_hour_done=485760,
            current_hour=485762,
            batch_size=10
        )

        self.assertIsInstance(result, list)
        self.assertEqual(result, [485760, 485761])

if __name__ == '__main__':
    unittest.main()
