import sys
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

#######################################################################
# Configuration constants                                             #
#######################################################################
DEFAULT_BATCH_SIZE = 100  # compact 100 partitions per run
#######################################################################


def get_backlog_partition_hours(spark, full_table: str,
                                partition_field: str,
                                last_hour_done: int, current_hour: int,
                                batch_size: int) -> list:
    """Extract up to *batch_size* partition-hour integers between 
    *last_hour_done* (inclusive) and *current_hour* (exclusive) using
    the `time_hour` field inside the partition STRUCT."""
    df = spark.read.format("iceberg").load(full_table + ".partitions")
    df = df.withColumn("hour_val", df[f"partition.{partition_field}"].cast("int"))
    return (
        df.filter((df.hour_val >= last_hour_done) & (df.hour_val < current_hour))
          .select("hour_val").distinct().orderBy("hour_val")
          .limit(batch_size)
          .rdd.flatMap(lambda r: r).collect()
    )


def get_last_compacted_index(spark, status_table: str, full_table: str) -> int:
    try:
        df = spark.read.format("iceberg").load(status_table)
        row = df.filter(df.table_name == full_table).select("last_compacted_hour").first()
        return row[0] if row else 0
    except AnalysisException:
        return 0


def persist_last_compacted_index(spark, status_table: str, full_table: str, hour_val: int) -> None:
    spark.sql(f"""
        MERGE INTO {status_table} t
        USING (SELECT '{full_table}' AS table_name, {hour_val} AS last_compacted_hour) s
        ON t.table_name = s.table_name
        WHEN MATCHED THEN UPDATE SET last_compacted_hour = s.last_compacted_hour
        WHEN NOT MATCHED THEN INSERT *
    """)


def call_rewrite_for_hour(spark, catalog_nm: str, source_db: str, table_nm: str,
                           target_file_size: int, epoch_hour: int) -> None:
    window_start = datetime(1970, 1, 1) + timedelta(hours=epoch_hour)
    window_end   = window_start + timedelta(hours=1)
    spark.sql(f"""
        CALL {catalog_nm}.system.rewrite_data_files(
          table => '{source_db}.{table_nm}',
          where => "time >= TIMESTAMP '{window_start}' AND time < TIMESTAMP '{window_end}'",
          options => map('target-file-size-bytes', '{target_file_size}', 'min-input-files', '1')
        )
    """)


def main():
    args = getResolvedOptions(sys.argv, [
        'file_size_for_optimization',
        'catalog_nm',
        'table_nm',
        'source_db',
        'expire_snapshots_day',
        'skip_newest_partitions'
    ])

    glue_job_name          = "90177_common_data_compaction"
    target_file_size_bytes = int(args['file_size_for_optimization']) * 1024 * 1024
    catalog_nm             = args['catalog_nm']
    table_nm               = args['table_nm']
    source_db              = args['source_db']
    expire_snapshots_day   = int(args['expire_snapshots_day'])
    skip_newest_partitions = int(args['skip_newest_partitions'])

    full_table   = f"{catalog_nm}.{source_db}.{table_nm}"
    status_table = f"{catalog_nm}.{source_db}.table_compaction_status"

    spark = (
        SparkSession.builder
        .appName(glue_job_name)
        .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .enableHiveSupport()
        .getOrCreate()
    )
    GlueContext(SparkContext.getOrCreate())

    spark.sql(f"CREATE TABLE IF NOT EXISTS {status_table} (table_name STRING, last_compacted_hour INT) USING iceberg")

    last_hour_done = get_last_compacted_index(spark, status_table, full_table)
    print(f"Starting compaction from epoch-hour {last_hour_done} (~ {datetime(1970,1,1)+timedelta(hours=last_hour_done)})")

    current_hour = int((datetime.utcnow() - timedelta(hours=skip_newest_partitions) - datetime(1970,1,1)).total_seconds() // 3600)

    partition_hours = []
    if last_hour_done < current_hour:
        partition_hours = get_backlog_partition_hours(
        spark, full_table, 'time_hour',
            spark, full_table, last_hour_done, current_hour, DEFAULT_BATCH_SIZE)

    print(f"⏳ Will compact {len(partition_hours)} partition(s): {partition_hours}")

    for hour in partition_hours:
        window_start = datetime(1970, 1, 1) + timedelta(hours=hour)
        window_end   = window_start + timedelta(hours=1)
        try:
            print(f"   → Compacting time_hour={hour} ({window_start} to {window_end})")
            call_rewrite_for_hour(spark, catalog_nm, source_db, table_nm, target_file_size_bytes, hour)
            print(f"      ✔ Completed hour {hour}")
        except Exception as e:
            print(f"      ⚠ Failed hour {hour}: {e}")

    if partition_hours:
        persist_last_compacted_index(spark, status_table, full_table, max(partition_hours) + 1)

    latest_hour_allowed = current_hour
    current_index       = get_last_compacted_index(spark, status_table, full_table)

    if current_index >= (latest_hour_allowed - 24):
        print("🔄 Snapshot expiration & orphan cleanup …")
        cutoff_snap = (datetime.utcnow() - timedelta(days=expire_snapshots_day)).strftime('%Y-%m-%d %H:%M:%S')
        spark.sql(f"""
            CALL {catalog_nm}.system.expire_snapshots(
              table => '{source_db}.{table_nm}',
              older_than => TIMESTAMP '{cutoff_snap}'
            )
        """)
        spark.sql(f"""
            CALL {catalog_nm}.system.remove_orphan_files(
              table => '{source_db}.{table_nm}',
              older_than => TIMESTAMP '{cutoff_snap}'
            )
        """)
    else:
        print("⚠ Backlog still exists; snapshot expiration skipped.")

    spark.stop()
    print("✅ Compaction job finished.")


if __name__ == "__main__":
    main()
