import sys
from datetime import datetime, timedelta

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext
from pyspark.context import SparkContext

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATALOG                = "cosmos_nonhcd_iceberg"
DATABASE               = "common_data"
TABLE                  = "metadata_table"
FULL_TABLE             = f"{CATALOG}.{DATABASE}.{TABLE}"
TARGET_FILE_SIZE_BYTES = 256 * 1024 * 1024  # 256 MB
RETENTION_DAYS         = 7

# â”€â”€â”€ SPARK + ICEBERG SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark = (
    SparkSession.builder
      .appName("Iceberg_Optimize_And_Expire")
      .config("spark.sql.extensions",
              "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      # register a Glue-backed Iceberg catalog
      .config(f"spark.sql.catalog.{CATALOG}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{CATALOG}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{CATALOG}.type",         "glue")
      .config(f"spark.sql.catalog.{CATALOG}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    print("â–¶ Loading Iceberg table for compaction:", FULL_TABLE)
    df = spark.read.format("iceberg").load(FULL_TABLE)

    # â”€â”€â”€ detect partitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        meta          = spark.sql(f"SELECT partition FROM {FULL_TABLE}.partitions")
        struct_type   = meta.schema["partition"].dataType
        partition_cols = [fld.name for fld in struct_type.fields]
        is_partitioned = True
        print(f"  â€¢ Detected partition columns: {partition_cols}")
    except AnalysisException:
        is_partitioned = False
        print("  â€¢ No partitions detected")

    # â”€â”€â”€ compaction logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_partitioned:
        # flatten and sort newest â†’ oldest
        select_cols = ", ".join(f"partition.{c} AS {c}" for c in partition_cols)
        flat = (
            spark.sql(f"SELECT {select_cols} FROM {FULL_TABLE}.partitions")
                 .distinct()
                 .orderBy(*[col(c).desc() for c in partition_cols])
        )
        part_vals = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(part_vals) > 2:
            print(f"â–¶ Compacting {len(part_vals)-2} partitions (skipping newest 2)")
            for vals in part_vals[2:]:
                cond = " AND ".join(f"{c} = {repr(v)}" for c, v in zip(partition_cols, vals))
                print("   â€¢ WHERE", cond)
                df.filter(expr(cond)) \
                  .writeTo(FULL_TABLE) \
                  .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
                  .overwritePartitions()
        else:
            print("â–¶ â‰¤2 partitions found; doing full-table compaction")
            df.writeTo(FULL_TABLE) \
              .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
              .overwritePartitions()
    else:
        print("â–¶ Unpartitioned table; doing full-table compaction")
        df.writeTo(FULL_TABLE) \
          .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
          .overwritePartitions()

    print("âœ… Compaction complete.")

    # â”€â”€â”€ expire snapshots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cutoff_ts = (datetime.utcnow() - timedelta(days=RETENTION_DAYS)) \
                   .strftime("%Y-%m-%d %H:%M:%S")
    print(f"â–¶ Expiring snapshots older than {RETENTION_DAYS} days (before {cutoff_ts} UTC)")

    expire_sql = f"""
      CALL {CATALOG}.system.expire_snapshots(
        table => '{DATABASE}.{TABLE}',
        older_than => TIMESTAMP '{cutoff_ts}'
      )
    """
    spark.sql(expire_sql)
    print("âœ… expire_snapshots procedure completed.")

    spark.stop()
    print("ğŸ‰ Job finished successfully.")

except Exception as e:
    print("âŒ Job failed:", e)
    sys.exit(1)
