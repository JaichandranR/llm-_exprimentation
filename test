from pyspark.sql import SparkSession
from pyspark.sql.functions import col, row_number, when
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType
from pyspark.sql.utils import AnalysisException

# Configuration
catalog_nm = "cosmos_nonhcd_iceberg"
database_nm = "common_data"
inventory_table = "iceberg_snapshot_inventory"
state_table = "iceberg_inventory_state"
s3_warehouse_path = "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/"

# Initialize Spark session with Iceberg support
spark = (
    SparkSession.builder
    .appName("IcebergMetadataCapture")
    .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config(f"spark.sql.catalog.{catalog_nm}.warehouse", s3_warehouse_path)
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .getOrCreate()
)

# Fetch all tables from the Iceberg database
tables_df = spark.sql(f"SHOW TABLES IN {catalog_nm}.{database_nm}")
tables = tables_df.collect()

# Define schema for the state table
state_schema = StructType([
    StructField("table_name", StringType(), True),
    StructField("snapshot_id", StringType(), True),
])

# Try loading the state table or initialize it
try:
    state_df = spark.sql(f"SELECT * FROM {catalog_nm}.{database_nm}.{state_table}")
except AnalysisException:
    print(f"State table {state_table} not found. Creating it.")
    empty_state_df = spark.createDataFrame([], schema=state_schema)
    empty_state_df.writeTo(f"{catalog_nm}.{database_nm}.{state_table}").createOrReplace()
    state_df = empty_state_df

# Collect new snapshot metadata
inventory_metadata_df = None

for table in tables:
    table_name = table.tableName
    print(f"Processing table: {table_name}")
    
    try:
        # Query Iceberg snapshots
        snapshots_df = spark.sql(
            f"""SELECT '{table_name}' AS table_name, snapshot_id, committed_at, operation, manifest_list, summary
                FROM {catalog_nm}.{database_nm}.{table_name}.snapshots"""
        )

        # Mark the latest snapshot
        window_spec = Window.orderBy(col("committed_at").desc())
        snapshots_df = snapshots_df.withColumn("rn", row_number().over(window_spec))
        snapshots_df = snapshots_df.withColumn("is_latest_snapshot", when(col("rn") == 1, "Yes").otherwise("No")).drop("rn")

        # Get the current snapshot ID
        latest_snapshot_id = snapshots_df.filter("is_latest_snapshot = 'Yes'").select("snapshot_id").first()["snapshot_id"]

        # Check state table for previous snapshot
        previous = state_df.filter(col("table_name") == table_name).collect()
        prev_snapshot_id = previous[0]["snapshot_id"] if previous else None

        # Compare and collect metadata if changed
        if prev_snapshot_id != latest_snapshot_id:
            print(f"Snapshot updated for {table_name}")
            inventory_metadata_df = snapshots_df if inventory_metadata_df is None else inventory_metadata_df.unionByName(snapshots_df)
        else:
            print(f"No snapshot change for {table_name}, skipping.")
    
    except Exception as e:
        print(f"Skipping table {table_name} due to error: {e}")

# Write updated metadata and new state
if inventory_metadata_df is not None:
    # Update inventory table
    inventory_metadata_df.writeTo(f"{catalog_nm}.{database_nm}.{inventory_table}").createOrReplace()

    # Update state table with latest snapshot IDs
    latest_state_df = inventory_metadata_df.filter("is_latest_snapshot = 'Yes'").select("table_name", "snapshot_id")
    latest_state_df.writeTo(f"{catalog_nm}.{database_nm}.{state_table}").overwritePartitions()

    print("Inventory and state tables updated successfully.")
else:
    print("No snapshot changes detected. No updates performed.")
