import sys
from datetime import datetime, timedelta

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext
from pyspark.context import SparkContext

# ─── CONFIG ───────────────────────────────────────────────────────────────────
CATALOG                = "cosmos_nonhcd_iceberg"
DATABASE               = "common_data"
TABLE                  = "metadata_table"
FULL_TABLE             = f"{CATALOG}.{DATABASE}.{TABLE}"
TARGET_FILE_SIZE_BYTES = 256 * 1024 * 1024  # 256 MB
RETENTION_DAYS         = 7

# ─── SPARK + ICEBERG SETUP ────────────────────────────────────────────────────
spark = (
    SparkSession.builder
      .appName("Iceberg_Optimize_And_Expire")
      .config("spark.sql.extensions",
              "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      # register a Glue-backed Iceberg catalog
      .config(f"spark.sql.catalog.{CATALOG}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{CATALOG}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{CATALOG}.type",         "glue")
      .config(f"spark.sql.catalog.{CATALOG}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    print("▶ Loading Iceberg table for compaction:", FULL_TABLE)
    df = spark.read.format("iceberg").load(FULL_TABLE)

    # ─── detect partitions ──────────────────────────────────────────────────────
    try:
        meta          = spark.sql(f"SELECT partition FROM {FULL_TABLE}.partitions")
        struct_type   = meta.schema["partition"].dataType
        partition_cols = [fld.name for fld in struct_type.fields]
        is_partitioned = True
        print(f"  • Detected partition columns: {partition_cols}")
    except AnalysisException:
        is_partitioned = False
        print("  • No partitions detected")

    # ─── compaction logic ───────────────────────────────────────────────────────
    if is_partitioned:
        # flatten and sort newest → oldest
        select_cols = ", ".join(f"partition.{c} AS {c}" for c in partition_cols)
        flat = (
            spark.sql(f"SELECT {select_cols} FROM {FULL_TABLE}.partitions")
                 .distinct()
                 .orderBy(*[col(c).desc() for c in partition_cols])
        )
        part_vals = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(part_vals) > 2:
            print(f"▶ Compacting {len(part_vals)-2} partitions (skipping newest 2)")
            for vals in part_vals[2:]:
                cond = " AND ".join(f"{c} = {repr(v)}" for c, v in zip(partition_cols, vals))
                print("   • WHERE", cond)
                df.filter(expr(cond)) \
                  .writeTo(FULL_TABLE) \
                  .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
                  .overwritePartitions()
        else:
            print("▶ ≤2 partitions found; doing full-table compaction")
            df.writeTo(FULL_TABLE) \
              .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
              .overwritePartitions()
    else:
        print("▶ Unpartitioned table; doing full-table compaction")
        df.writeTo(FULL_TABLE) \
          .option("write.target-file-size-bytes", str(TARGET_FILE_SIZE_BYTES)) \
          .overwritePartitions()

    print("✅ Compaction complete.")

    # ─── expire snapshots ───────────────────────────────────────────────────────
    cutoff_ts = (datetime.utcnow() - timedelta(days=RETENTION_DAYS)) \
                   .strftime("%Y-%m-%d %H:%M:%S")
    print(f"▶ Expiring snapshots older than {RETENTION_DAYS} days (before {cutoff_ts} UTC)")

    expire_sql = f"""
      CALL {CATALOG}.system.expire_snapshots(
        table => '{DATABASE}.{TABLE}',
        older_than => TIMESTAMP '{cutoff_ts}'
      )
    """
    spark.sql(expire_sql)
    print("✅ expire_snapshots procedure completed.")

    spark.stop()
    print("🎉 Job finished successfully.")

except Exception as e:
    print("❌ Job failed:", e)
    sys.exit(1)
