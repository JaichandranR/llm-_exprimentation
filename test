import sys
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

"""
Iceberg incremental compaction job
----------------------------------
* Compacts only partitions whose **time partition** is older than a configurable age (default 24â€¯h).
* Batches compaction by a limited number of partitions per run.
* Snapshot expiration & orphan cleanup is delayed until all old partitions are compacted.
"""

ALL_PARTITIONS_COMPACTED_FLAG = "/tmp/iceberg_compaction_complete.flag"

# ---------------------------------------------------------------------------
# ENTRYPOINT
# ---------------------------------------------------------------------------

def main():
    # â”€â”€ 1. Resolve CLI parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    args = getResolvedOptions(
        sys.argv,
        [
            "file_size_for_optimization",  # MB
            "catalog_nm",
            "table_nm",
            "source_db",
            "time_partition_col",         # e.g., event_date or ingestion_ts
            "min_file_age_hours",         # typically 24
            "batch_size",                 # max partitions to compact per run
            "expire_snapshots_day"        # retention window
        ]
    )

    FILE_SIZE_BYTES    = int(args["file_size_for_optimization"]) * 1024 * 1024
    CATALOG            = args["catalog_nm"]
    DB                 = args["source_db"]
    TABLE              = args["table_nm"]
    TP_COL             = args["time_partition_col"]
    AGE_HR             = int(args.get("min_file_age_hours", 24))
    MAX_PARTS          = int(args.get("batch_size", 200))
    RETAIN_DAYS        = int(args.get("expire_snapshots_day", 7))

    FULL_TABLE = f"{CATALOG}.{DB}.{TABLE}"

    # â”€â”€ 2. Spark + Iceberg bootstrap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    spark = (
        SparkSession.builder
        .appName("iceberg_incremental_compaction")
        .config(f"spark.sql.catalog.{CATALOG}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .enableHiveSupport()
        .getOrCreate()
    )
    GlueContext(SparkContext.getOrCreate())

    try:
        # â”€â”€ 3. Determine cutoff timestamp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        cutoff_ts  = datetime.utcnow() - timedelta(hours=AGE_HR)
        cutoff_str = cutoff_ts.strftime("%Y-%m-%d %H:%M:%S")
        print(f"Compacting partitions where {TP_COL} < {cutoff_str} â€¦")

        # â”€â”€ 4. Fetch eligible partitions from Iceberg metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        part_query = (
            f"SELECT DISTINCT partition.{TP_COL} AS tp_val, partition \n"
            f"FROM   {FULL_TABLE}.partitions \n"
            f"WHERE  partition.{TP_COL} < TIMESTAMP '{cutoff_str}'"
        )
        parts_df = spark.sql(part_query).orderBy(col("tp_val").asc()).limit(MAX_PARTS)
        parts = parts_df.collect()
        print(f"Found {len(parts)} partitions older than cutoff (processing up to {MAX_PARTS}).")

        # â”€â”€ 5. Compact each selected partition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for p in parts:
            part_struct = p["partition"]
            cond = " AND ".join([f"{k} = {repr(v)}" for k, v in part_struct.asDict().items()])
            print(f"Compacting partition: {cond}")
            (
                spark.read.format("iceberg").load(FULL_TABLE)
                .filter(cond)
                .writeTo(FULL_TABLE)
                .option("write.target-file-size-bytes", str(FILE_SIZE_BYTES))
                .overwritePartitions()
            )

        # â”€â”€ 6. Check if all old partitions have been compacted â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        remaining_query = (
            f"SELECT COUNT(*) AS remaining FROM {FULL_TABLE}.partitions \n"
            f"WHERE partition.{TP_COL} < TIMESTAMP '{cutoff_str}'"
        )
        remaining = spark.sql(remaining_query).collect()[0]["remaining"]

        if remaining == 0:
            print("âœ… All eligible partitions compacted â€” writing flag.")
            with open(ALL_PARTITIONS_COMPACTED_FLAG, "w") as f:
                f.write("done\n")
        else:
            print(f"ğŸ•’ Still {remaining} partitions left to compact.")

        # â”€â”€ 7. Expire snapshots only when all partitions have been compacted â”€
        if remaining == 0:
            snaps = (
                spark.read.format("iceberg").load(f"{CATALOG}.{DB}.{TABLE}.snapshots")
                .filter("operation = 'overwrite'")
                .filter("element_at(summary, 'replace-partitions') = 'true'")
                .orderBy(col("committed_at").desc())
                .limit(1)
                .collect()
            )
            if snaps:
                print("âœ… Compaction snapshot detected â†’ Expiring old snapshots â€¦")
                exp_cutoff = (datetime.utcnow() - timedelta(days=RETAIN_DAYS)).strftime("%Y-%m-%d %H:%M:%S")
                spark.sql(
                    f"CALL {CATALOG}.system.expire_snapshots(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}',\n"
                    f"        retain_last => 5\n"
                    f")"
                )
                print("Snapshot expiration complete.")

                print("Removing orphan files â€¦")
                spark.sql(
                    f"CALL {CATALOG}.system.remove_orphan_files(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}'\n"
                    f")"
                )
                print("Orphan removal complete.")
        else:
            print("â­ Skipping expiration â€” full compaction not complete yet.")

    except Exception as exc:
        print("âŒ Job failed:", exc)
        sys.exit(1)
    finally:
        spark.stop()
        print("Job finished â€“ OK.")


if __name__ == "__main__":
    main()
