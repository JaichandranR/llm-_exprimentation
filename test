import sys
from datetime import datetime, timedelta

from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
catalog = "cosmos_nonhcd_iceberg"
db      = "common_data"
tbl     = "metadata_table"
warehouse = "s3://app-id-90177-dep-id-114232-uu-id-pee895Fr5knp/"
full_table = f"{catalog}.{db}.{tbl}"

# tuning params
target_num_files   = 8
target_file_size   = 256 * 1024 * 1024  # 256 MB

# ‚îÄ‚îÄ‚îÄ SPARK / ICEBERG SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
spark = (
    SparkSession.builder
      .appName("GlueIcebergCompactionViaSQL")
      .config(f"spark.sql.catalog.{catalog}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{catalog}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{catalog}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .config(f"spark.sql.catalog.{catalog}.warehouse",    warehouse)
      .config("spark.sql.extensions",                       "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    # 1) Read the full table into a DataFrame
    df = spark.read.format("iceberg").load(full_table)

    # 2) Try reading the Iceberg partitions metadata via SQL
    try:
        meta_part = spark.sql(f"SELECT partition FROM {full_table}.partitions")
        # partition is a struct ‚Üí extract its field names
        part_struct = meta_part.schema["partition"].dataType
        partition_cols = [fld.name for fld in part_struct.fields]
        is_partitioned = True
    except AnalysisException:
        is_partitioned = False

    if is_partitioned:
        print(f"üîç Detected partition columns: {partition_cols}")

        # 3) Flatten & sort them descending
        select_expr = ", ".join(f"partition.{c} AS {c}" for c in partition_cols)
        flat = spark.sql(
            f"SELECT {select_expr} FROM {full_table}.partitions"
        ).distinct().orderBy(
            *[col(c).desc() for c in partition_cols]
        )

        part_values = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(part_values) > 2:
            to_compact = part_values[2:]
            print(f"üì¶ Skipping newest two partitions, compacting {len(to_compact)} older ones.")
            for vals in to_compact:
                # build a SQL-style filter, e.g. "day = '2024-06-01'"
                cond = " AND ".join(
                    f"{c} = {repr(v)}" for c, v in zip(partition_cols, vals)
                )
                print(f"   ‚Ä¢ Compacting WHERE {cond}")
                df.filter(expr(cond)) \
                  .repartition(target_num_files) \
                  .writeTo(full_table) \
                  .option("write.target-file-size-bytes", str(target_file_size)) \
                  .overwritePartitions()
        else:
            print("‚ö†Ô∏è Only two or fewer partitions found. Doing full-table compaction.")
            df.repartition(target_num_files) \
              .writeTo(full_table) \
              .option("write.target-file-size-bytes", str(target_file_size)) \
              .overwritePartitions()

    else:
        print("‚ÑπÔ∏è No Iceberg partitions detected ‚Üí performing full-table compaction.")
        df.repartition(target_num_files) \
          .writeTo(full_table) \
          .option("write.target-file-size-bytes", str(target_file_size)) \
          .overwritePartitions()

    print("‚úÖ Compaction complete.")
    spark.stop()

except Exception as e:
    print(f"‚ùå Job failed: {e}")
    sys.exit(1)
