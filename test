import sys
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

"""
Iceberg incremental compaction job
----------------------------------
* Compacts only partitions whose **time partition** is older than a configurable age (default 24‚ÄØh).
* Batches compaction by a limited number of partitions per run.
* Handles cases where the partition column name is unknown or hidden.
* Snapshot expiration & orphan cleanup is delayed until all **eligible** partitions (older than 24‚ÄØh) are compacted.
"""

ALL_ELIGIBLE_PARTITIONS_COMPACTED_FLAG = "/tmp/iceberg_old_partitions_compacted.flag"

# ---------------------------------------------------------------------------
# ENTRYPOINT
# ---------------------------------------------------------------------------

def main():
    # -------------------------------------------------------------------
    # 1. Resolve required CLI parameters; optional ones handled with get()
    # -------------------------------------------------------------------
    args = getResolvedOptions(
        sys.argv,
        [
            "file_size_for_optimization",
            "catalog_nm",
            "table_nm",
            "source_db",
        ]
    )

    FILE_SIZE_BYTES = int(args["file_size_for_optimization"]) * 1024 * 1024
    CATALOG         = args["catalog_nm"]
    DB              = args["source_db"]
    TABLE           = args["table_nm"]

    AGE_HR       = int(args.get("min_file_age_hours", 24))   # optional
    MAX_PARTS    = int(args.get("batch_size", 200))          # optional
    RETAIN_DAYS  = int(args.get("expire_snapshots_day", 7))  # optional

    FULL_TABLE = f"{CATALOG}.{DB}.{TABLE}"

    # -------------------------------------------------------------------
    # 2. Spark + Iceberg bootstrap
    # -------------------------------------------------------------------
    spark = (
        SparkSession.builder
        .appName("iceberg_incremental_compaction")
        .config(f"spark.sql.catalog.{CATALOG}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .enableHiveSupport()
        .getOrCreate()
    )
    GlueContext(SparkContext.getOrCreate())

    try:
        # ----------------------------------------------------------------
        # 3. Detect partition column via Iceberg metadata
        # ----------------------------------------------------------------
        print("Detecting partition columns via Iceberg metadata ‚Ä¶")
        meta = spark.sql(f"SELECT * FROM {FULL_TABLE}.partitions")
        partition_candidates = [f.name for f in meta.schema.fields if f.name not in ('file_count', 'record_count', 'last_updated_at')]
        if not partition_candidates:
            raise Exception("‚ùå No usable partition column found in .partitions metadata.")
        partition_col = partition_candidates[0]  # take the first available usable field  # assume first column is time-like

        # ----------------------------------------------------------------
        # 4. Determine cutoff & fetch eligible partitions
        # ----------------------------------------------------------------
        cutoff_ts  = datetime.utcnow() - timedelta(hours=AGE_HR)
        cutoff_str = cutoff_ts.strftime("%Y-%m-%d %H:%M:%S")
        print(f"Compacting partitions where {partition_col} < {cutoff_str} ‚Ä¶")

        part_query = (
            f"SELECT DISTINCT {partition_col} AS tp_val 
"
            f"FROM {FULL_TABLE}.partitions 
"
            f"WHERE {partition_col} < TIMESTAMP '{cutoff_str}'"
        )
        parts_df = spark.sql(part_query).orderBy(col("tp_val").asc()).limit(MAX_PARTS)
        parts    = parts_df.collect()
        print(f"Found {len(parts)} partitions older than cutoff (processing up to {MAX_PARTS}).")

        # ----------------------------------------------------------------
        # 5. Compact each selected partition
        # ----------------------------------------------------------------
        for p in parts:
            part_struct = p["partition"]
            cond = f"{partition_col} = '{p[partition_col]}'"
            print(f"Compacting partition: {cond}")
            (
                spark.read.format("iceberg").load(FULL_TABLE)
                .filter(cond)
                .writeTo(FULL_TABLE)
                .option("write.target-file-size-bytes", str(FILE_SIZE_BYTES))
                .overwritePartitions()
            )

        # ----------------------------------------------------------------
        # 6. Check how many eligible partitions remain
        # ----------------------------------------------------------------
        remaining_query = (
            f"SELECT COUNT(*) AS remaining FROM {FULL_TABLE}.partitions 
"
            f"WHERE {partition_col} < TIMESTAMP '{cutoff_str}'"
        )
        remaining = spark.sql(remaining_query).collect()[0]["remaining"]

        if remaining == 0:
            print("‚úÖ All eligible (older than 24‚ÄØh) partitions compacted ‚Äî writing flag.")
            with open(ALL_ELIGIBLE_PARTITIONS_COMPACTED_FLAG, "w") as f:
                f.write("done\n")
        else:
            print(f"üïí Still {remaining} partitions left to compact.")

        # ----------------------------------------------------------------
        # 7. Snapshot expiration & orphan cleanup (only after full compaction)
        # ----------------------------------------------------------------
        if remaining == 0:
            snaps = (
                spark.read.format("iceberg").load(f"{CATALOG}.{DB}.{TABLE}.snapshots")
                .filter("operation = 'overwrite'")
                .filter("element_at(summary, 'replace-partitions') = 'true'")
                .orderBy(col("committed_at").desc())
                .limit(1)
                .collect()
            )
            if snaps:
                print("‚úÖ Compaction snapshot detected ‚Üí Expiring old snapshots ‚Ä¶")
                exp_cutoff = (datetime.utcnow() - timedelta(days=RETAIN_DAYS)).strftime("%Y-%m-%d %H:%M:%S")
                spark.sql(
                    f"CALL {CATALOG}.system.expire_snapshots(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}',\n"
                    f"        retain_last => 5\n"
                    f")"
                )
                print("Snapshot expiration complete.")

                print("Removing orphan files ‚Ä¶")
                spark.sql(
                    f"CALL {CATALOG}.system.remove_orphan_files(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}'\n"
                    f")"
                )
                print("Orphan removal complete.")
        else:
            print("‚è≠ Skipping expiration ‚Äî not all eligible partitions compacted yet.")

    except Exception as exc:
        print("‚ùå Job failed:", exc)
        sys.exit(1)

    finally:
        spark.stop()
        print("Job finished ‚Äì OK.")


if __name__ == "__main__":
    main()
