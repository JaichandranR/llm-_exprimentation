import sys
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

def get_last_compacted_partition(spark, status_table, full_table):
    try:
        df = spark.read.format("iceberg").load(status_table)
        filtered_df = df.filter(df["table_name"] == full_table).select("last_compacted_index")
        if filtered_df.count() == 0:
            print(f"No compaction status found for table {full_table}. Starting from the beginning.")
            return 0
        last_index = filtered_df.collect()[0][0]
        return last_index
    except AnalysisException as e:
        print(f"Error accessing compaction status table: {e}")
        return 0

def all_partitions_compacted(spark, full_table, last_compacted_index):
    cutoff_time = datetime.utcnow() - timedelta(hours=24)
    eligible_partitions = spark.sql(f"SELECT * FROM {full_table}$partitions")
    total_eligible = eligible_partitions.count()
    return last_compacted_index >= total_eligible

def main():
    args = getResolvedOptions(sys.argv, [
        'file_size_for_optimization',
        'catalog_nm',
        'table_nm',
        'source_db',
        'skip_newest_partitions',
        'expire_snapshots_day'
    ])

    glue_job_name = "90177_common_data_compaction"
    file_size_for_optimization = int(args['file_size_for_optimization']) * 1024 * 1024
    catalog_nm = args['catalog_nm']
    table_nm = args['table_nm']
    source_db = args['source_db']
    skip_newest_partitions = int(args['skip_newest_partitions'])
    expire_snapshots_day = int(args['expire_snapshots_day'])

    full_table = f"{catalog_nm}.{source_db}.{table_nm}"
    status_table = f"{catalog_nm}.{source_db}.table_compaction_status"

    spark = (
        SparkSession.builder
            .appName(glue_job_name)
            .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
            .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
            .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
            .enableHiveSupport()
            .getOrCreate()
    )

    glueContext = GlueContext(SparkContext.getOrCreate())

    try:
        spark.sql(f"CREATE TABLE IF NOT EXISTS {status_table} (table_name STRING, last_compacted_index INT) USING iceberg")

        print(f"Loading partition metadata for table {full_table}...")
        partition_metadata = spark.sql(f"SELECT partition FROM {full_table}$partitions")

        from pyspark.sql.functions import regexp_extract

        partition_info = partition_metadata.withColumn(
            "partition_val", regexp_extract("partition", r"\\w+=(\\d+)", 1).cast("int")
        ).orderBy("partition_val")

        cutoff_time = datetime.utcnow() - timedelta(hours=24)
        partition_info_filtered = partition_info.limit(partition_info.count() - skip_newest_partitions)

        last_compacted_index = get_last_compacted_partition(spark, status_table, full_table)

        batch_size = 100
        num_partitions_compacted = 0

        for i, row in enumerate(partition_info_filtered.collect()[last_compacted_index:last_compacted_index + batch_size]):
            epoch_hour = row["partition_val"]
            partition_start = datetime(1970, 1, 1) + timedelta(hours=epoch_hour)
            partition_end = partition_start + timedelta(hours=1)

            print(f"Compacting partition hour: {epoch_hour} | {partition_start} to {partition_end}")

            spark.sql(f"""
                CALL {catalog_nm}.system.rewrite_data_files(
                  table => '{source_db}.{table_nm}',
                  where => "time >= TIMESTAMP '{partition_start}' AND time < TIMESTAMP '{partition_end}'",
                  options => map('target-file-size-bytes', '{file_size_for_optimization}', 'min-input-files', '1')
                )
            """)

            num_partitions_compacted += 1

        spark.sql(f"""
            MERGE INTO {status_table} t
            USING (SELECT '{full_table}' AS table_name, {last_compacted_index + num_partitions_compacted} AS last_compacted_index) s
            ON t.table_name = s.table_name
            WHEN MATCHED THEN UPDATE SET last_compacted_index = s.last_compacted_index
            WHEN NOT MATCHED THEN INSERT *
        """)

        if all_partitions_compacted(spark, full_table, last_compacted_index + num_partitions_compacted):
            print("Expiring snapshots...")
            cutoff = (datetime.utcnow() - timedelta(days=expire_snapshots_day)).strftime('%Y-%m-%d %H:%M:%S')
            spark.sql(f"""
                CALL {catalog_nm}.system.expire_snapshots(
                    table => '{source_db}.{table_nm}',
                    older_than => TIMESTAMP '{cutoff}'
                )
            """)

        print("Removing orphan files...")
        spark.sql(f"""
            CALL {catalog_nm}.system.remove_orphan_files(
                table => '{source_db}.{table_nm}',
                older_than => TIMESTAMP '{cutoff}'
            )
        """)

        spark.stop()
        print("Compaction + snapshot expiration + orphan cleanup done.")

    except Exception as e:
        print("Job failed:", e)
        sys.exit(1)

if __name__ == "__main__":
    main()
