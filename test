import sys
from datetime import datetime, timedelta
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

"""
Iceberg incremental compaction job
----------------------------------
* Compacts only partitions whose **time partition** is older than a configurable age (default 24‚ÄØh).
* Batches compaction by a limited number of partitions per run.
* Handles cases where partition column name is unknown or hidden.
* Snapshot expiration & orphan cleanup is delayed until all **eligible** partitions (older than 24h) are compacted.
"""

ALL_ELIGIBLE_PARTITIONS_COMPACTED_FLAG = "/tmp/iceberg_old_partitions_compacted.flag"

# ---------------------------------------------------------------------------
# ENTRYPOINT
# ---------------------------------------------------------------------------

def main():
    args = getResolvedOptions(
        sys.argv,
        [
            "file_size_for_optimization",
            "catalog_nm",
            "table_nm",
            "source_db"
        ]
    )

    FILE_SIZE_BYTES = int(args["file_size_for_optimization"]) * 1024 * 1024
    CATALOG = args["catalog_nm"]
    DB = args["source_db"]
    TABLE = args["table_nm"]
    AGE_HR = int(args.get("min_file_age_hours", 24))
    MAX_PARTS = int(args.get("batch_size", 200))
    RETAIN_DAYS = int(args.get("expire_snapshots_day", 7))

    FULL_TABLE = f"{CATALOG}.{DB}.{TABLE}"

    spark = (
        SparkSession.builder
        .appName("iceberg_incremental_compaction")
        .config(f"spark.sql.catalog.{CATALOG}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .enableHiveSupport()
        .getOrCreate()
    )
    GlueContext(SparkContext.getOrCreate())

    try:
        print("Inferring partition column...")
        part_cols_df = spark.sql(f"SHOW PARTITIONS {FULL_TABLE}")
        part_cols = part_cols_df.columns
        if not part_cols:
            raise Exception("No partition columns detected. Cannot proceed with incremental compaction.")
        partition_col = part_cols[0]  # assuming single partition column

        cutoff_ts = datetime.utcnow() - timedelta(hours=AGE_HR)
        cutoff_str = cutoff_ts.strftime("%Y-%m-%d %H:%M:%S")
        print(f"Compacting partitions where {partition_col} < {cutoff_str} ‚Ä¶")

        part_query = (
            f"SELECT DISTINCT partition.{partition_col} AS tp_val, partition \
             FROM {FULL_TABLE}.partitions \
             WHERE partition.{partition_col} < TIMESTAMP '{cutoff_str}'"
        )
        parts_df = spark.sql(part_query).orderBy(col("tp_val").asc()).limit(MAX_PARTS)
        parts = parts_df.collect()
        print(f"Found {len(parts)} partitions older than cutoff (processing up to {MAX_PARTS}).")

        for p in parts:
            part_struct = p["partition"]
            cond = " AND ".join([f"{k} = {repr(v)}" for k, v in part_struct.asDict().items()])
            print(f"Compacting partition: {cond}")
            (
                spark.read.format("iceberg").load(FULL_TABLE)
                .filter(cond)
                .writeTo(FULL_TABLE)
                .option("write.target-file-size-bytes", str(FILE_SIZE_BYTES))
                .overwritePartitions()
            )

        remaining_query = (
            f"SELECT COUNT(*) AS remaining FROM {FULL_TABLE}.partitions \
             WHERE partition.{partition_col} < TIMESTAMP '{cutoff_str}'"
        )
        remaining = spark.sql(remaining_query).collect()[0]["remaining"]

        if remaining == 0:
            print("‚úÖ All eligible (older than 24h) partitions compacted ‚Äî writing flag.")
            with open(ALL_ELIGIBLE_PARTITIONS_COMPACTED_FLAG, "w") as f:
                f.write("done\n")
        else:
            print(f"üïí Still {remaining} partitions left to compact.")

        if remaining == 0:
            snaps = (
                spark.read.format("iceberg").load(f"{CATALOG}.{DB}.{TABLE}.snapshots")
                .filter("operation = 'overwrite'")
                .filter("element_at(summary, 'replace-partitions') = 'true'")
                .orderBy(col("committed_at").desc())
                .limit(1)
                .collect()
            )
            if snaps:
                print("‚úÖ Compaction snapshot detected ‚Üí Expiring old snapshots ‚Ä¶")
                exp_cutoff = (datetime.utcnow() - timedelta(days=RETAIN_DAYS)).strftime("%Y-%m-%d %H:%M:%S")
                spark.sql(
                    f"CALL {CATALOG}.system.expire_snapshots(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}',\n"
                    f"        retain_last => 5\n"
                    f")"
                )
                print("Snapshot expiration complete.")

                print("Removing orphan files ‚Ä¶")
                spark.sql(
                    f"CALL {CATALOG}.system.remove_orphan_files(\n"
                    f"        table      => '{DB}.{TABLE}',\n"
                    f"        older_than => TIMESTAMP '{exp_cutoff}'\n"
                    f")"
                )
                print("Orphan removal complete.")
        else:
            print("‚è≠ Skipping expiration ‚Äî not all eligible partitions compacted yet.")

    except Exception as exc:
        print("‚ùå Job failed:", exc)
        sys.exit(1)
    finally:
        spark.stop()
        print("Job finished ‚Äì OK.")

if __name__ == "__main__":
    main()
