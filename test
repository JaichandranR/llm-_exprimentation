import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# ─── CONFIG ───────────────────────────────────────────────────────────────────
catalog_nm          = "cosmos_nonhcd_iceberg"
database_nm         = "common_data"
table_name          = "metadata_table"
partition_col       = "event_date"   # ← your time-based partition column
s3_warehouse_path   = "s3://app-id-90177-dep-id-114232-uu-id-pee895Fr5knp/"
full_table          = f"{catalog_nm}.{database_nm}.{table_name}"

# How many shuffle partitions / output files per compaction job
target_num_files    = 8

# ─── SPARK + ICEBERG SETUP ────────────────────────────────────────────────────
spark = (
    SparkSession.builder
      .appName("IcebergSelectiveCompaction")
      .config(f"spark.sql.catalog.{catalog_nm}",               "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl",  "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{catalog_nm}.io-impl",       "org.apache.iceberg.aws.s3.S3FileIO")
      .config(f"spark.sql.catalog.{catalog_nm}.warehouse",     s3_warehouse_path)
      .config("spark.sql.extensions",                          "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    # ─── read the table ─────────────────────────────────────────────────────────
    df = spark.read.format("iceberg").load(full_table)

    # ─── decide path based on whether partition_col exists in the schema ────────
    if partition_col in df.columns:
        # fetch distinct partition values, newest first
        parts_df       = df.select(partition_col).distinct().orderBy(col(partition_col).desc())
        part_values    = [r[partition_col] for r in parts_df.collect()]

        if len(part_values) > 2:
            print(f"Skipping the latest 2 partitions and compacting the other {len(part_values)-2} partitions.")
            for value in part_values[2:]:
                print(f" • compacting partition {partition_col} = {value}")
                df.filter(col(partition_col) == value) \
                  .repartition(target_num_files) \
                  .writeTo(full_table) \
                  .option("write.target-file-size-bytes", str(256*1024*1024)) \
                  .overwritePartitions()
        else:
            print("Only 2 or fewer partitions found; performing full-table compaction instead.")
            df.repartition(target_num_files) \
              .writeTo(full_table) \
              .option("write.target-file-size-bytes", str(256*1024*1024)) \
              .overwritePartitions()

    else:
        print(f"No column `{partition_col}` found → table is unpartitioned. Doing full-table compaction.")
        df.repartition(target_num_files) \
          .writeTo(full_table) \
          .option("write.target-file-size-bytes", str(256*1024*1024)) \
          .overwritePartitions()

    print("✅ Compaction job completed successfully.")

except Exception as e:
    print(f"❌ Compaction job failed: {e}")
    sys.exit(1)
