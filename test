from pyspark.sql import SparkSession
from datetime import datetime
import boto3

# Catalog/database/table definitions
catalog_nm = "cosmos_nonhcd_iceberg"
database_nm = "common_data"
table_nm = "90177_inventory"
partition_key = "table_name"

# Initialize Spark session
spark = (SparkSession.builder
    .appName("IcebergTableManagement")
    .config(f"spark.sql.catalog.{catalog_nm}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
    .config(f"spark.sql.catalog.{catalog_nm}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config(f"spark.sql.catalog.{catalog_nm}.warehouse", "s3://app-id-90177-dep-id-114232-uu-id-pee895fr5knp/")
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .getOrCreate())

# Sample data
json_objects = [
    {
        "table_name": "example_table",
        "snapshot_id": "12345",
        "timestamp_ms": 162525233515,
        "operation": "append",
        "manifest_list": "manifest_path"
    }
]
schema = "table_name STRING, snapshot_id STRING, timestamp_ms LONG, operation STRING, manifest_list STRING"

# Create DataFrame
df = spark.createDataFrame(json_objects, schema=schema)
table = f"{catalog_nm}.{database_nm}.{table_nm}"

# Check if table exists
table_exists = spark.sql(f"SHOW TABLES IN {database_nm}").filter(f"tableName = '{table_nm}'").count() > 0

if table_exists:
    df.createOrReplaceTempView("new_data")
    condition = "old.table_name = new.table_name AND old.snapshot_id = new.snapshot_id"
    spark.sql(f"""
        MERGE INTO {table} old
        USING new_data new
        ON {condition}
        WHEN NOT MATCHED THEN INSERT *
    """)
else:
    df.writeTo(table).using("iceberg")\
        .tableProperty("write.format.default", "parquet")\
        .partitionedBy(partition_key).createOrReplace()

print("‚úÖ Data written/merged into Iceberg table.")

# -----------------------------------------
# üöÄ Enhanced section: Get snapshot metadata
# -----------------------------------------

try:
    snapshots_df = spark.sql(f"SELECT * FROM `{table}$snapshots`")
    manifests_df = spark.sql(f"SELECT * FROM `{table}$manifests`")
    metadata_log_df = spark.sql(f"SELECT * FROM `{table}$metadata_log_entries`")

    latest_snapshot = snapshots_df.orderBy("committed_at", ascending=False).first()
    latest_metadata = metadata_log_df.orderBy("timestamp_ms", ascending=False).first()

    snapshot_info = {
        "snapshot_id": latest_snapshot["snapshot_id"],
        "committed_at": latest_snapshot["committed_at"],
        "operation": latest_snapshot["operation"],
        "manifest_list": latest_snapshot["manifest_list"],
        "metadata_file": latest_metadata["metadata_file"],
        "partition_spec_ids": manifests_df.select("partition_spec_id").distinct().rdd.flatMap(lambda x: x).collect(),
        "captured_at": datetime.utcnow().isoformat()
    }

    print("üì¶ Iceberg Snapshot Metadata:")
    for k, v in snapshot_info.items():
        print(f"{k}: {v}")

except Exception as e:
    print(f"‚ùå Failed to extract Iceberg snapshot metadata: {e}")
