import sys
from datetime import datetime, timedelta

from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr
from pyspark.sql.utils import AnalysisException
from awsglue.context import GlueContext

# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
catalog           = "cosmos_nonhcd_iceberg"
db                = "common_data"
tbl               = "metadata_table"
warehouse         = "s3://app-id-90177-dep-id-114232-uu-id-pee895Fr5knp/"
full_table        = f"{catalog}.{db}.{tbl}"
target_file_size  = 256 * 1024 * 1024  # 256 MB

# ‚îÄ‚îÄ‚îÄ SPARK / ICEBERG SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
spark = (
    SparkSession.builder
      .appName("GlueIcebergCompactionViaSQL_SizeBased")
      .config(f"spark.sql.catalog.{catalog}",              "org.apache.iceberg.spark.SparkCatalog")
      .config(f"spark.sql.catalog.{catalog}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
      .config(f"spark.sql.catalog.{catalog}.io-impl",      "org.apache.iceberg.aws.s3.S3FileIO")
      .config(f"spark.sql.catalog.{catalog}.warehouse",    warehouse)
      .config("spark.sql.extensions",                      "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()
)
glueContext = GlueContext(SparkContext.getOrCreate())

try:
    # 1) Read the full table into a DataFrame
    df = spark.read.format("iceberg").load(full_table)

    # 2) Detect partitioning
    try:
        meta_part   = spark.sql(f"SELECT partition FROM {full_table}.partitions")
        part_struct = meta_part.schema["partition"].dataType
        partition_cols = [fld.name for fld in part_struct.fields]
        is_partitioned = True
        print(f"üîç Detected partitions: {partition_cols}")
    except AnalysisException:
        is_partitioned = False
        print("‚ÑπÔ∏è No partitions detected")

    # 3) Size‚Äêbased compaction
    if is_partitioned:
        # figure out which partition values to compact
        select_expr = ", ".join(f"partition.{c} AS {c}" for c in partition_cols)
        flat = (
            spark.sql(f"SELECT {select_expr} FROM {full_table}.partitions")
                 .distinct()
                 .orderBy(*[col(c).desc() for c in partition_cols])
        )
        part_values = [tuple(r[c] for c in partition_cols) for r in flat.collect()]

        if len(part_values) > 2:
            to_compact = part_values[2:]
            print(f"üì¶ Skipping newest 2 partitions, compacting {len(to_compact)} older ones‚Ä¶")
            for vals in to_compact:
                cond = " AND ".join(f"{c} = {repr(v)}" for c, v in zip(partition_cols, vals))
                print("   ‚Ä¢ Compacting WHERE", cond)
                df.filter(expr(cond)) \
                  .writeTo(full_table) \
                  .option("write.target-file-size-bytes", str(target_file_size)) \
                  .overwritePartitions()
        else:
            print("‚ö†Ô∏è ‚â§2 partitions found; doing full‚Äêtable compaction")
            df.writeTo(full_table) \
              .option("write.target-file-size-bytes", str(target_file_size)) \
              .overwritePartitions()
    else:
        print("‚ÑπÔ∏è Unpartitioned table; doing full‚Äêtable compaction")
        df.writeTo(full_table) \
          .option("write.target-file-size-bytes", str(target_file_size)) \
          .overwritePartitions()

    print("‚úÖ Compaction complete.")
    spark.stop()

except Exception as e:
    print(f"‚ùå Job failed: {e}")
    sys.exit(1)
