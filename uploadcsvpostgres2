import pandas as pd
import requests
import json
from sqlalchemy import create_engine
import psycopg2
from ecdpawssession.session_builder import SessionBuilder

# Step 1: Postgres connection setup
USERNAME = ''  # Leave blank for IAM authentication
DOMAIN = 'naeast'  # ENTER DOMAIN
ROLE_ARN = 'arn:aws:iam::ROLE/90177-aurora-app-admin'  # ENTER ROLE ARN
ENDPOINT = 'proxy-postgres-nlb.elb.us-east-1.amazonaws.com'  # ENTER DB NLB PROXY
CLUSTER = 'aurora-cluster.cluster-c1wayscsgqcd.us-east-1.rds.amazonaws.com'  # ENTER PG Cluster
PORT = '6160'  # ENTER PORT
USER = 'llmdbAuroraAppAdmin'  # ENTER USER
REGION = 'us-east-1'  # ENTER REGION
DBNAME = 'llmdb'  # ENTER DB

# Generate RDS token for authentication
session = SessionBuilder.cli(
    role_arn=ROLE_ARN, 
    region=REGION, 
    username=USERNAME, 
    domain=DOMAIN, 
    store_password=True
).with_auto_renew().build()

client = session.Client('rds')
token = client.generate_db_auth_token(
    DBHostname=CLUSTER, 
    Port=PORT, 
    DBUsername=USER, 
    Region=REGION
)

# Establish database connection
def get_postgres_connection():
    try:
        conn = psycopg2.connect(
            host=ENDPOINT,
            user=USER,
            password=token,
            port=PORT,
            database=DBNAME
        )
        print("Database connection successful.")
        return conn
    except Exception as e:
        print(f"Database connection failed due to {e}")
        raise

# Step 2: Hosted API setup
API_URL = "http://your-eks-hosted-url"  # Replace with the actual URL of your hosted API
API_KEY = "3g7h1e9a5b0d2f81a4c6"  # Replace with your actual API key

# Function to call the hosted API to compute embeddings
def compute_embedding_with_api(text):
    headers = {
        "Content-Type": "application/json",
        "x-api-key": API_KEY
    }
    data = {
        "model": "MetaLlama318BInstruct",
        "prompt": f"<|begin_of_text|>{text}<|end_of_text|>",
        "options": {
            "seed": 1,
            "temperature": 0
        },
        "stream": False
    }
    response = requests.post(API_URL, data=json.dumps(data), headers=headers, timeout=1000)
    if response.status_code == 200:
        result = response.json()
        embedding = result.get("embedding", [])
        return embedding
    else:
        raise ValueError(f"Failed to compute embedding. API Response: {response.text}")

# Step 3: Populate application.mitre_embeddings table
def populate_mitre_embeddings_table(file_path, id_column, text_column, source):
    conn = get_postgres_connection()
    cursor = conn.cursor()

    try:
        # Read the CSV file
        df = pd.read_csv(file_path)
        print(f"Processing {len(df)} rows from {file_path}...")

        # Iterate through each row in the dataframe
        for _, row in df.iterrows():
            control_id = row[id_column]
            document = row[text_column]

            # Split the document into manageable chunks
            chunks = chunk_text(document)
            for chunk_id, chunk in enumerate(chunks):
                # Call the API to get embedding
                embedding = compute_embedding_with_api(chunk)

                # Prepare the SQL query
                insert_query = """
                    INSERT INTO application.mitre_embeddings (control_id, chunk_id, source, documents, embedding)
                    VALUES (%s, %s, %s, %s, %s)
                    ON CONFLICT (control_id, chunk_id)
                    DO UPDATE SET 
                        source = EXCLUDED.source,
                        documents = EXCLUDED.documents,
                        embedding = EXCLUDED.embedding;
                """
                cursor.execute(insert_query, (control_id, chunk_id, source, chunk, embedding))
        
        conn.commit()
        print(f"Embeddings from {file_path} successfully inserted into the database.")
    except Exception as e:
        print(f"Error while processing {file_path}: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()

# Helper function to chunk documents
def chunk_text(text, max_length=512):
    sentences = text.split(". ")  # Split into sentences
    chunks = []
    chunk = []
    length = 0

    for sentence in sentences:
        sentence_length = len(sentence.split())
        if length + sentence_length <= max_length:
            chunk.append(sentence)
            length += sentence_length
        else:
            chunks.append(". ".join(chunk))
            chunk = [sentence]
            length = sentence_length

    if chunk:
        chunks.append(". ".join(chunk))
    return chunks

# Step 4: Process all CSV files
def process_all_files():
    csv_files = [
        {"file": "MITRE_Mitigation.csv", "id_column": "MitigationID", "text_column": "MitigationDesc", "source": "mitre_mitigation"},
        {"file": "MITRE_SubTechnique.csv", "id_column": "SubTechniqueID", "text_column": "SubTechniqueDesc", "source": "mitre_subtechnique"},
        {"file": "MITRE_ID_Mapping.csv", "id_column": "TechniqueID", "text_column": "TechniqueID", "source": "mitre_id_mapping"},
        {"file": "MITRE_Tactic.csv", "id_column": "TacticID", "text_column": "TacticDescription", "source": "mitre_tactic"},
        {"file": "MITRE_Technique.csv", "id_column": "TechniqueID", "text_column": "TechniqueDesc", "source": "mitre_technique"}
    ]

    for csv_file in csv_files:
        populate_mitre_embeddings_table(
            file_path=csv_file["file"],
            id_column=csv_file["id_column"],
            text_column=csv_file["text_column"],
            source=csv_file["source"]
        )

# Run the script
if __name__ == "__main__":
    process_all_files()
