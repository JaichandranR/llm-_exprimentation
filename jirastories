STORY 1 ‚Äî High-Level Architecture Design
Summary:

Define high-level architecture for global lineage + DRD purge system

Description:

As a Foundation Service Stack Engineer, I want to develop a high-level architecture that outlines how manifest.json files, global dependency extraction, Trino/Mobius integration, Iceberg metadata, and Spark-based purge jobs work together within COSMOS.

The architecture document must clearly show the major components involved in the new design:

Team-specific dbt pipelines producing manifest.json

Shared S3 location for manifest collection

One-time custom metadata collector job that parses all manifests

Global Model Dependency Table (Iceberg)

Integration with Trino/Mobius for metadata validation

AWS Glue Spark job that performs DRD purging based on RCC + lineage

Optional on-prem Spark job for hybrid deployments

This document serves as the entry point for all engineering, governance, and architecture stakeholders.

Acceptance Criteria:

Diagram showing all system components

Describes data flow from dbt ‚Üí manifests ‚Üí dependency table ‚Üí purge

Includes AWS and on-prem execution paths

Reviewed by architecture team

Notes:

This story establishes the architecture needed for all remaining work.

‚úÖ STORY 2 ‚Äî Detailed Technical Design
Summary:

Produce detailed technical design for new lineage and purge system

Description:

As a Foundation Service Stack Engineer, I want to create a detailed design specification that includes:

manifest.json structure and how metadata will be extracted

full schema for GLOBAL_MODEL_DEPENDENCY_EDGES Iceberg table

logic for parsing parent/child relationships

handling of cross-project references

logic for detecting unpartitioned or unsafe tables

lineage-based purge eligibility rules (leaf-first sequencing)

interface for Spark DRD purge job

error handling and audit guidelines

This design document should answer all technical questions related to how metadata is extracted, stored, processed, validated, and used during purge execution.

Acceptance Criteria:

Design doc includes schemas, algorithms, workflows

Covers dependency extraction, metadata extraction, purge logic

Provides detailed mapping of manifest fields to internal model

Approved by governance + platform team

Notes:

This is the ‚ÄúBible‚Äù for all engineers implementing the system.

‚úÖ STORY 3 ‚Äî Architecture Review and Approval
Summary:

Conduct architecture and design review for lineage + purge framework

Description:

As a Foundation Service Stack Engineer, I want to review the full architecture and detailed design with all stakeholders (platform, governance, security, and team leads) to ensure alignment and sign-off.

Review sessions must cover:

Data flows

Dependency model

Manifest ingestion logic

Iceberg table design

Purge logic

On-prem and AWS execution use cases

Acceptance Criteria:

Review meeting completed

All feedback incorporated

Official approval recorded

Notes:

Required before implementation can start.

üü¶ EPIC ‚Äî Manifest Collection & Metadata/Dependency Extraction
‚úÖ STORY 4 ‚Äî Implement Manifest.json Collection
Summary:

Collect manifest.json files from all team pipelines

Description:

As a Foundation Service Stack Engineer, I must implement a standardized mechanism where all dbt teams publish their manifest.json files into a centralized S3 bucket.

This includes handling:

pipelines using Querybook, Kestra, Jules, dbt CLI, local jobs

different folder structures per team

versioning or overwriting of manifest.json

validation of upload success

detecting missing or outdated manifests

The collector job will use these manifests as the input to build the centralized dependency master table.

Acceptance Criteria:

All teams can publish manifest.json to a shared S3 root path

Supports different folder prefixes (team1/, team2/, etc.)

Logs upload success/failure

Validates manifest structure (optional JSON schema)

Notes:

This is the input layer of the entire system.

MERGED STORY (Replaces Story 5 + Story 6)
STORY ‚Äî Build Automated Job to Extract Model Metadata and Parent‚ÄìChild Dependencies from manifest.json
Summary:

Develop an automated solution to extract model metadata and parent‚Äìchild lineage relationships from manifest.json files.

Description (Merged & Expanded):

As a Foundation Service Stack Engineer, I want to build an automated, reusable job that parses each team‚Äôs manifest.json file and extracts both:

Model Metadata, such as:

model name

alias (table name)

schema

database/catalog

model type/materialization (table, view, incremental, external, snapshot)

tags (RCC codes, governance tags)

config properties (partition_by, unique_key, freshness configs, etc.)

file path, owner project name

full FQN of the output table

resource_type (model, source, seed)

Parent ‚Üí Child Dependency Edges, including:

ref() relationships

source() dependencies

seed ‚Üí model links

snapshot ‚Üí model dependencies

cross-project or cross-domain references

This unified automated job ensures we DO NOT rely on any manual extraction or manual mapping.
Given that dbt manifest.json is large, deeply nested, and varies across projects, manual lineage derivation is impossible and error-prone.

The job will generate structured objects representing:

model metadata records

dependency edge records

These outputs will feed into the GLOBAL_MODEL_DEPENDENCY_EDGES Iceberg table.

This solution must:

Run in batch mode for initial creation

Run in incremental mode when new manifests arrive

Handle malformed or partially-populated manifests gracefully

This is the core component of the new DRD lineage governance framework.

Acceptance Criteria:

Automated job implemented using Python/Spark/Glue (or agreed technology).

Parses all manifest.json fields required for metadata extraction.

Extracts all dependencies from ref(), source(), seed, snapshot.

Normalizes metadata + lineage into structured output datasets.

No manual intervention is required to derive relationships.

Handles multiple team manifest structures and naming conventions.

Logs invalid manifests or missing fields.

Supports incremental updates when manifests change.

Outputs compatible structures for loading into Iceberg master table.

Accurately represents cross-project lineage.

Processing verified across sample manifests from all teams.

Additional Notes:

This story becomes the heart of your ‚Äúone-time metadata collector‚Äù logic.

Metadata and dependencies must be extracted together because they are tightly coupled in dbt‚Äôs internal node graph.

This merged approach simplifies your Epics, reduces redundancy, and aligns with how dbt‚Äôs manifest.json is designed.

Downstream purge logic depends 100% on the accuracy of this extraction.


‚úÖ STORY 7 ‚Äî Implement Incremental Manifest-Based Lineage Update
Summary:

Update lineage only when manifest changes

Description:

As a Foundation Service Stack Engineer, I want to detect when a team publishes a new or updated manifest.json and process only that file rather than reprocessing all manifests globally.

This reduces costs and ensures lineage freshness.

The job must compare:

manifest hash, or

last modified timestamp

and update only the affected edges and metadata.

Acceptance Criteria:

Detects updated manifests

Processes only changed manifests

No duplicate edge rows

Logs which manifests triggered updates

Notes:

Helps keep the global lineage table scalable.

‚úÖ STORY 8 ‚Äî Create Global Dependency Iceberg Table
Summary:

Create Iceberg table for storing dependency edges

Description:

As a Foundation Service Stack Engineer, I want to create a centralized Iceberg table that holds parent-child dependencies extracted from all team manifests.

This table becomes the global source of truth for:

lineage queries

purge ordering

downstream dependency validation

safety assessments

The schema must include:

parent_model

parent_project

child_model

child_project

relation_type

fully qualified table names (optional)

timestamps for auditing

Acceptance Criteria:

Iceberg table created with correct schema

Supports inserts and merges

Accessible via Trino

Notes:

Will be heavily used by Spark purge job.

‚úÖ STORY 9 ‚Äî Load Dependency Edges into Iceberg
Summary:

Load extracted dependency edges into Iceberg table

Description:

As a Foundation Service Stack Engineer, I want to implement batch and incremental logic for inserting dependency edges into the Iceberg table, ensuring deduplication and consistency.

This includes:

initial full load

incremental loads based on new manifests

merging changes (replace changed edges, remove stale ones)

Acceptance Criteria:

Batch and incremental loads supported

Duplicate edges prevented

Stale edges removed when manifest changes

Logging of load success/failure

Notes:

This finalizes the metadata foundation for DRD purge.

üü¶ EPIC ‚Äî Spark DRD Purge Engine
‚úÖ STORY 10 ‚Äî Build Spark DRD Purge Job
Summary:

Build Spark job for performing DRD deletions on Iceberg tables

Description:

As a Foundation Service Stack Engineer, I want to create a Spark job (AWS Glue + on-prem Spark compatible) that deletes Iceberg table rows older than the retention periods defined by RCC-based metadata tags.

This job must be simple, reads the dependency table and source table metadata, then performs Iceberg DELETEs.

Acceptance Criteria:

Deletes based on retention_days input

Supports partitioned deletions

Creates purge audit records

Configurable for AWS and on-prem

Notes:

This is the core execution engine for DRD purging.

‚úÖ STORY 11 ‚Äî Implement Safety Rules (Partition + Dependencies)
Summary:

Add validation to prevent unsafe purging

Description:

As a Foundation Service Stack Engineer, I want strict safety checks before purging any table. These include:

Blocking purge for unpartitioned tables

Blocking purge if a table has active downstream dependencies

Blocking purge if partition is ‚Äúhot‚Äù (currently being written to)

Logging all violations

Acceptance Criteria:

Safety rules implemented

Unpartitioned tables flagged

Dependent tables prevented from purging

Active partition logic validated

Notes:

Prevents data loss and integrity issues.

‚úÖ STORY 12 ‚Äî Lineage-Based Purge Ordering (Leaf-first)
Summary:

Compute leaf-first purge ordering from dependency table

Description:

As a Foundation Service Stack Engineer, I want to determine purge order using global dependency edges so that parent tables are only purged after all children have been validated or purged.

Algorithm:

Identify leaf models (no dependents).

Purge eligible leaf models.

Recompute leaf nodes.

Continue upward until no eligible models remain.

Acceptance Criteria:

Algorithm implemented

Correctly identifies leaf models

Works across multiple projects

Logs decision-making process

Notes:

Critical for dependency-safe purging.

üü¶ EPIC ‚Äî On-Prem Spark Support
‚úÖ STORY 13 ‚Äî Implement On-Prem Spark Purge Job
Summary:

Create portable version of purge job for on-prem Spark

Description:

As a Foundation Service Stack Engineer, I want the Spark purge job to run identically on-prem using Spark-submit, allowing hybrid customers and internal data centers to perform DRD purging.

Acceptance Criteria:

Same logic as AWS Glue version

Supports local or Hive/Nessie catalog

Config-driven runtime fields

‚úÖ STORY 14 ‚Äî Add Config-Based Catalog/Schema for On-Prem Purge
Summary:

Enable config-driven catalog settings for on-prem

Description:

As a Foundation Service Stack Engineer, I want purge job settings for catalog type, catalog URI, schema, and credentials to be configurable through external files so that on-prem and AWS environments can use the same codebase.

Acceptance Criteria:

Reads YAML/JSON config

No hardcoded catalog/schema

Supports secure secret injection

‚úÖ STORY 15 ‚Äî Validate On-Prem End-to-End Purge
Summary:

Test purge logic on on-prem Iceberg tables

Description:

As a Foundation Service Stack Engineer, I want to validate purge behavior on an on-prem Spark environment by purging sample Iceberg tables and verifying data correctness.

Acceptance Criteria:

Test data created

Purge executed

Results validated against expected outcomes

Logs compared

üü¶ EPIC ‚Äî CI/CD Integration
‚úÖ STORY 16 ‚Äî CI/CD Hook to Publish Manifest.json
Summary:

Automatically upload manifest.json after dbt runs

Description:

As a Foundation Service Stack Engineer, I want dbt pipelines (across teams) to automatically publish manifest.json files to the central S3 path after each successful dbt compile/run so that metadata remains up-to-date.

Acceptance Criteria:

CI/CD job implemented

Manifest uploaded reliably

Failure alerts sent to team

‚úÖ STORY 17 ‚Äî Trigger Lineage Refresh on Manifest Upload
Summary:

Trigger metadata refresh on new manifest arrival

Description:

As a Foundation Service Stack Engineer, I want lineage extraction to automatically run when a new manifest.json is uploaded to the S3 bucket so that the global dependency table stays current without manual intervention.

Acceptance Criteria:

S3 event or schedule trigger set

Incremental job runs only for updated manifests

Successful update logged
