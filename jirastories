STORY 1 ‚Äî High-Level Architecture Design
Summary:

Define high-level architecture for global lineage + DRD purge system

Description:

As a Foundation Service Stack Engineer, I want to develop a high-level architecture that outlines how manifest.json files, global dependency extraction, Trino/Mobius integration, Iceberg metadata, and Spark-based purge jobs work together within COSMOS.

The architecture document must clearly show the major components involved in the new design:

Team-specific dbt pipelines producing manifest.json

Shared S3 location for manifest collection

One-time custom metadata collector job that parses all manifests

Global Model Dependency Table (Iceberg)

Integration with Trino/Mobius for metadata validation

AWS Glue Spark job that performs DRD purging based on RCC + lineage

Optional on-prem Spark job for hybrid deployments

This document serves as the entry point for all engineering, governance, and architecture stakeholders.

Acceptance Criteria:

Diagram showing all system components

Describes data flow from dbt ‚Üí manifests ‚Üí dependency table ‚Üí purge

Includes AWS and on-prem execution paths

Reviewed by architecture team

Notes:

This story establishes the architecture needed for all remaining work.

‚úÖ STORY 2 ‚Äî Detailed Technical Design
Summary:

Produce detailed technical design for new lineage and purge system

Description:

As a Foundation Service Stack Engineer, I want to create a detailed design specification that includes:

manifest.json structure and how metadata will be extracted

full schema for GLOBAL_MODEL_DEPENDENCY_EDGES Iceberg table

logic for parsing parent/child relationships

handling of cross-project references

logic for detecting unpartitioned or unsafe tables

lineage-based purge eligibility rules (leaf-first sequencing)

interface for Spark DRD purge job

error handling and audit guidelines

This design document should answer all technical questions related to how metadata is extracted, stored, processed, validated, and used during purge execution.

Acceptance Criteria:

Design doc includes schemas, algorithms, workflows

Covers dependency extraction, metadata extraction, purge logic

Provides detailed mapping of manifest fields to internal model

Approved by governance + platform team

Notes:

This is the ‚ÄúBible‚Äù for all engineers implementing the system.

‚úÖ STORY 3 ‚Äî Architecture Review and Approval
Summary:

Conduct architecture and design review for lineage + purge framework

Description:

As a Foundation Service Stack Engineer, I want to review the full architecture and detailed design with all stakeholders (platform, governance, security, and team leads) to ensure alignment and sign-off.

Review sessions must cover:

Data flows

Dependency model

Manifest ingestion logic

Iceberg table design

Purge logic

On-prem and AWS execution use cases

Acceptance Criteria:

Review meeting completed

All feedback incorporated

Official approval recorded

Notes:

Required before implementation can start.

üü¶ EPIC ‚Äî Manifest Collection & Metadata/Dependency Extraction
‚úÖ STORY 4 ‚Äî Implement Manifest.json Collection
Summary:

Collect manifest.json files from all team pipelines

Description:

As a Foundation Service Stack Engineer, I must implement a standardized mechanism where all dbt teams publish their manifest.json files into a centralized S3 bucket.

This includes handling:

pipelines using Querybook, Kestra, Jules, dbt CLI, local jobs

different folder structures per team

versioning or overwriting of manifest.json

validation of upload success

detecting missing or outdated manifests

The collector job will use these manifests as the input to build the centralized dependency master table.

Acceptance Criteria:

All teams can publish manifest.json to a shared S3 root path

Supports different folder prefixes (team1/, team2/, etc.)

Logs upload success/failure

Validates manifest structure (optional JSON schema)

Notes:

This is the input layer of the entire system.

MERGED STORY (Replaces Story 5 + Story 6)
STORY ‚Äî Build Automated Job to Extract Model Metadata and Parent‚ÄìChild Dependencies from manifest.json
Summary:

Develop an automated solution to extract model metadata and parent‚Äìchild lineage relationships from manifest.json files.

Description (Merged & Expanded):

As a Foundation Service Stack Engineer, I want to build an automated, reusable job that parses each team‚Äôs manifest.json file and extracts both:

Model Metadata, such as:

model name

alias (table name)

schema

database/catalog

model type/materialization (table, view, incremental, external, snapshot)

tags (RCC codes, governance tags)

config properties (partition_by, unique_key, freshness configs, etc.)

file path, owner project name

full FQN of the output table

resource_type (model, source, seed)

Parent ‚Üí Child Dependency Edges, including:

ref() relationships

source() dependencies

seed ‚Üí model links

snapshot ‚Üí model dependencies

cross-project or cross-domain references

This unified automated job ensures we DO NOT rely on any manual extraction or manual mapping.
Given that dbt manifest.json is large, deeply nested, and varies across projects, manual lineage derivation is impossible and error-prone.

The job will generate structured objects representing:

model metadata records

dependency edge records

These outputs will feed into the GLOBAL_MODEL_DEPENDENCY_EDGES Iceberg table.

This solution must:

Run in batch mode for initial creation

Run in incremental mode when new manifests arrive

Handle malformed or partially-populated manifests gracefully

This is the core component of the new DRD lineage governance framework.

Acceptance Criteria:

Automated job implemented using Python/Spark/Glue (or agreed technology).

Parses all manifest.json fields required for metadata extraction.

Extracts all dependencies from ref(), source(), seed, snapshot.

Normalizes metadata + lineage into structured output datasets.

No manual intervention is required to derive relationships.

Handles multiple team manifest structures and naming conventions.

Logs invalid manifests or missing fields.

Supports incremental updates when manifests change.

Outputs compatible structures for loading into Iceberg master table.

Accurately represents cross-project lineage.

Processing verified across sample manifests from all teams.

Additional Notes:

This story becomes the heart of your ‚Äúone-time metadata collector‚Äù logic.

Metadata and dependencies must be extracted together because they are tightly coupled in dbt‚Äôs internal node graph.

This merged approach simplifies your Epics, reduces redundancy, and aligns with how dbt‚Äôs manifest.json is designed.

Downstream purge logic depends 100% on the accuracy of this extraction.


‚úÖ STORY 7 ‚Äî Implement Incremental Manifest-Based Lineage Update
Summary:

Update lineage only when manifest changes

Description:

As a Foundation Service Stack Engineer, I want to detect when a team publishes a new or updated manifest.json and process only that file rather than reprocessing all manifests globally.

This reduces costs and ensures lineage freshness.

The job must compare:

manifest hash, or

last modified timestamp

and update only the affected edges and metadata.

Acceptance Criteria:

Detects updated manifests

Processes only changed manifests

No duplicate edge rows

Logs which manifests triggered updates

Notes:

Helps keep the global lineage table scalable.

‚úÖ STORY 8 ‚Äî Create Global Dependency Iceberg Table
Summary:

Create Iceberg table for storing dependency edges

Description:

As a Foundation Service Stack Engineer, I want to create a centralized Iceberg table that holds parent-child dependencies extracted from all team manifests.

This table becomes the global source of truth for:

lineage queries

purge ordering

downstream dependency validation

safety assessments

The schema must include:

parent_model

parent_project

child_model

child_project

relation_type

fully qualified table names (optional)

timestamps for auditing

Acceptance Criteria:

Iceberg table created with correct schema

Supports inserts and merges

Accessible via Trino

Notes:

Will be heavily used by Spark purge job.

‚úÖ STORY 9 ‚Äî Load Dependency Edges into Iceberg
Summary:

Load extracted dependency edges into Iceberg table

Description:

As a Foundation Service Stack Engineer, I want to implement batch and incremental logic for inserting dependency edges into the Iceberg table, ensuring deduplication and consistency.

This includes:

initial full load

incremental loads based on new manifests

merging changes (replace changed edges, remove stale ones)

Acceptance Criteria:

Batch and incremental loads supported

Duplicate edges prevented

Stale edges removed when manifest changes

Logging of load success/failure

Notes:

This finalizes the metadata foundation for DRD purge.

üü¶ EPIC ‚Äî Spark DRD Purge Engine
‚úÖMERGED STORY ‚Äî Build Spark DRD Purge Engine with Lineage-Based Leaf-First Purge Ordering
Summary:

Develop a unified Spark-based DRD purge engine that performs Iceberg DELETE operations using RCC retention rules and executes purge operations in a lineage-aware, leaf-first order derived from the Global Dependency Table.

Description (Merged & Expanded):

As a Foundation Service Stack Engineer, I want to develop a single Spark-based purge engine (AWS Glue + on-prem compatible) that reads the Global Dependency Table, identifies safe purge order using leaf-first lineage traversal, and executes DRD purge operations based on RCC retention logic.

This unified Spark job must:

1. Load Global Dependency Table

Read parent‚Äìchild relationships from GLOBAL_MODEL_DEPENDENCY_EDGES

Build an in-memory DAG representation of model/table relationships

Determine which tables are leaf nodes (no downstream dependencies)

2. Compute Lineage-Based Leaf-First Purge Ordering

Identify purgeable models by selecting leaf tables

Purge leaf tables first, ensuring no child depends on them

After purging a leaf, recompute new leaf candidates

Continue until eligible models are exhausted

Ensure cross-team, cross-project lineage is respected

Prevent purging parent tables prematurely

3. Apply RCC Retention Logic

For each table selected for purge:

Determine retention period (from metadata/tags)

Determine partition column (if available)

Identify partitions or records older than retention cutoff

4. Perform Actual Iceberg Deletes

Execute Iceberg DELETE statements

Purge only eligible partitions or rows

Use Spark DataFrame syntax or Iceberg SQL

Ensure idempotent behavior

5. Include All Safety Validations

Block purge if:

Table is unpartitioned (if unsupported for deletes)

Table has active ‚Äúhot‚Äù partitions

Table has dependent models not yet purged

Table is missing retention configuration

6. Generate Audit + Logging

Write purge results (before/after counts) to audit table

Capture lineage reasoning for purge steps (e.g., ‚Äúpurged as a leaf‚Äù)

Log safety failures and exceptions

7. Portable Across Environments

Code must run:

As AWS Glue Spark job

As On-Prem Spark-submit job

Runtime config must control catalog, schema, retention settings

This is the core execution engine of the entire DRD governance solution.

Acceptance Criteria:

Spark job loads dependency graph from Iceberg

Purge order is computed using correct leaf-first, lineage-aware traversal

Retention logic applied correctly per table

Only safe candidate tables are purged

Iceberg deletes executed successfully (partition-level or row-level)

Unpartitioned tables are blocked with clear logs

Downstream dependencies prevent premature purge

Active ‚Äúhot‚Äù partitions are protected

Audit records created for each purge operation

Logs show purge order, dependency checks, safety checks, and outcomes

Job proven to run in:

AWS Glue

On-prem Spark environment

End-to-end integration tested with sample datasets
üü¶ EPIC ‚Äî On-Prem Spark Support
‚úÖ STORY 13 ‚Äî Implement On-Prem Spark Purge Job
Summary:

Create portable version of purge job for on-prem Spark

Description:

As a Foundation Service Stack Engineer, I want the Spark purge job to run identically on-prem using Spark-submit, allowing hybrid customers and internal data centers to perform DRD purging.

Acceptance Criteria:

Same logic as AWS Glue version

Supports local or Hive/Nessie catalog

Config-driven runtime fields

‚úÖ STORY 14 ‚Äî Add Config-Based Catalog/Schema for On-Prem Purge
Summary:

Enable config-driven catalog settings for on-prem

Description:

As a Foundation Service Stack Engineer, I want purge job settings for catalog type, catalog URI, schema, and credentials to be configurable through external files so that on-prem and AWS environments can use the same codebase.

Acceptance Criteria:

Reads YAML/JSON config

No hardcoded catalog/schema

Supports secure secret injection

‚úÖ STORY 15 ‚Äî Validate On-Prem End-to-End Purge
Summary:

Test purge logic on on-prem Iceberg tables

Description:

As a Foundation Service Stack Engineer, I want to validate purge behavior on an on-prem Spark environment by purging sample Iceberg tables and verifying data correctness.

Acceptance Criteria:

Test data created

Purge executed

Results validated against expected outcomes

Logs compared

üü¶ EPIC ‚Äî CI/CD Integration
‚úÖ STORY 16 ‚Äî CI/CD Hook to Publish Manifest.json
Summary:

Automatically upload manifest.json after dbt runs

Description:

As a Foundation Service Stack Engineer, I want dbt pipelines (across teams) to automatically publish manifest.json files to the central S3 path after each successful dbt compile/run so that metadata remains up-to-date.

Acceptance Criteria:

CI/CD job implemented

Manifest uploaded reliably

Failure alerts sent to team

‚úÖ STORY 17 ‚Äî Trigger Lineage Refresh on Manifest Upload
Summary:

Trigger metadata refresh on new manifest arrival

Description:

As a Foundation Service Stack Engineer, I want lineage extraction to automatically run when a new manifest.json is uploaded to the S3 bucket so that the global dependency table stays current without manual intervention.

Acceptance Criteria:

S3 event or schedule trigger set

Incremental job runs only for updated manifests

Successful update logged

STORY ‚Äî Perform End-to-End Testing Across the Global Lineage and DRD Purge Framework
Summary:

Execute comprehensive end-to-end testing across manifest ingestion, dependency extraction, global lineage table construction, and Spark purge execution to validate the full DRD governance workflow.

Description (Expanded):

As a Foundation Service Stack Engineer, I want to conduct thorough end-to-end testing across the entire Global Lineage & DRD Purge Governance Framework to ensure all components work together correctly, safely, and reliably across multiple data domains.

This story covers the validation of the complete real-world flow:

1. Manifest Ingestion

Confirm each dbt project's manifest.json is successfully published to S3

Validate ingestion logic handles multiple teams and environments

Test incremental updates when manifests change

2. Metadata & Dependency Extraction Job

Validate the automated manifest parser extracts:

model metadata

parent‚Äìchild relationships

Verify correctness across models, sources, seeds, snapshots

Test cross-project lineage extraction

3. Global Dependency Table Population

Validate Iceberg table is correctly written

Confirm no duplicates, missing edges, or malformed rows

Validate consistent results across batch and incremental loads

4. Spark DRD Purge Engine

Validate purge ordering respects lineage

Confirm leaf-first sequencing

Validate safety rules:

prevents purge of dependent tables

blocks unpartitioned tables

protects active/hot partitions

Confirm correct application of retention logic

Verify Iceberg DELETE behavior across sample datasets

5. Audit & Logging Validation

Validate purge audit table captures:

before/after record counts

purge timestamps

table/partition purged

lineage reasoning

Review purge logs for errors, warnings, and sequencing

6. Multi-Team Integration Scenarios

Create and test scenarios such as:

Team A ‚Üí Team B model dependency

Shared tables used by multiple teams

Purge eligibility differences (e.g., 30-day table feeding 90-day table)

Tables with zero downstream dependencies

Tables with circular or unexpected lineage patterns

Out-of-sync manifests

7. AWS and On-Prem Validation

Run the purge job in AWS Glue

Run the same job via on-prem Spark-submit

Validate behavior is consistent across environments

8. Final E2E Governance Validation

Demonstrate that only safe tables are purged

Demonstrate that unsafe tables are blocked with clear error messages

Confirm the system behaves correctly in:

first run (cold start)

subsequent incremental runs

partial manifest updates

partition boundary tests

Acceptance Criteria:

End-to-end run completes successfully across:

manifest ingestion

metadata extraction

dependency table updates

Spark lineage-aware purge execution

audit logging

All test scenarios documented with results

Purge failures reflect correct safety checks

Purge execution order matches lineage DAG

Cross-project dependencies validated

Retention logic verified for partitioned tables

Audit table reviewed and verified

On-prem + AWS execution validated

Test evidence recorded and shared with governance + platform teams

Additional Notes:

This is a major validation story and should occur after implementing ingestion, extraction, dependency table, and purge engine.

Multiple sample test datasets may need to be created to simulate real dbt environments.

Output of this story is required for governance sign-off and production readiness.


Ingest and Register Iceberg Tables Provisioned Outside dbt (Manual or External Sources)
Summary:

Implement a solution to discover, ingest, and register Iceberg tables created outside of dbt (manual tables, external producers, legacy ETL outputs) and include them in the global lineage & DRD purge governance framework.

Description (Expanded):

As a Foundation Service Stack Engineer, I want to ensure that Iceberg tables not represented in any dbt manifest.json ‚Äî such as manually created tables, legacy ingestion tables, third-party producer tables, or tables created via Glue/EMR/Trino jobs ‚Äî are also included in the global lineage and DRD governance process.

These external/non-dbt tables do not appear in dbt manifests and therefore are invisible to the metadata collector job unless explicitly discovered and registered.

This story introduces a mechanism to:

1. Automatically Discover External Iceberg Tables

Query Iceberg catalog(s) directly

Identify tables not present in any dbt project manifest

Detect new tables added outside dbt

2. Register External Tables in the Global Dependency Framework

Insert metadata entries for these tables

Mark them as ‚Äúexternal‚Äù or ‚Äúnon-dbt-managed‚Äù

Ensure they appear in the lineage DAG (with no parents unless specified)

3. Include External Tables in DRD Purge Eligibility

Apply retention rules (either default or metadata-driven)

Ensure safety checks apply (unpartitioned, dependent tables)

Ensure Spark purge engine can operate on them

4. Support Manual Imports / One-Time Registrations

Allow Ops or Governance teams to manually register tables

Useful for:

Ad-hoc tables

Temporary ingestion layers

Legacy historic datasets

Provide an easy interface to record:

full FQNs

schemas

retention expectations

optional dependencies

5. Prevent Governance Blind Spots

No table in COSMOS should escape DRD purge governance

No ‚Äúunknown‚Äù tables should bypass retention or lineage validation

Critical for compliance, RCC rules, and data retention auditability

Acceptance Criteria:

A mechanism is implemented to discover Iceberg tables across specified catalogs.

Tables not found in any dbt manifest.json are flagged as ‚Äúexternal‚Äù or ‚Äúnon-dbt.‚Äù

Metadata for these external tables is registered in the global dependency/metadata store.

External tables appear in:

lineage graph (as isolated entities unless dependencies are manually added)

purge eligibility lists

safety validation workflows

Spark purge engine can process or block them appropriately.

Manual registration workflow (CLI script or SQL interface) documented and validated.

Clear logging and governance reporting on:

external tables discovered

newly registered tables

tables missing retention configs

Governance team can override or assign retention rules for external tables.
