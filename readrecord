import boto3
import sys
import traceback
from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ConnectTimeoutError, ClientError
from pyspark.sql import SparkSession

def list_glue_databases_and_tables(glue_client, database_name, table_name, catalog_id):
    try:
        # Retrieve the list of databases
        dbs = glue_client.get_databases(CatalogId=catalog_id)

        # Print the names of the databases
        print("Databases in AWS Glue Data Catalog:")
        for db in dbs['DatabaseList']:
            print(db['Name'])

        # Retrieve the list of tables in the specified database
        tables = glue_client.get_tables(DatabaseName=database_name, CatalogId=catalog_id)

        # Print the names of the tables
        print(f"\nTables in AWS Glue Database '{database_name}':")
        for table in tables['TableList']:
            print(table['Name'])

        print("\nFetching first 10 rows from table:", table_name)

    except NoCredentialsError:
        print("[ERROR] AWS credentials not found. Please configure your credentials.")
    except PartialCredentialsError:
        print("[ERROR] Incomplete AWS credentials. Please check your configuration.")
    except ConnectTimeoutError:
        print("[ERROR] Connection timed out. Please check your network connectivity.")
    except ClientError as e:
        print(f"[ERROR] AWS Client error: {e}")
    except Exception as e:
        print(f"[ERROR] Unexpected error: {str(e)}")
        print(traceback.format_exc())  # Log full stack trace

def fetch_first_10_rows(database_name, table_name, catalog_id):
    """ Fetch the first 10 rows from an AWS Glue Iceberg table using PySpark """

    try:
        # Initialize Spark Session with Iceberg Support
        spark = SparkSession.builder \
            .appName("GlueIcebergQuery") \
            .config("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog") \
            .config("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog") \
            .config("spark.sql.catalog.glue_catalog.warehouse", "s3://your-glue-warehouse-bucket/") \
            .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
            .getOrCreate()

        # Define Iceberg table path
        iceberg_table = f"glue_catalog.{database_name}.{table_name}"

        print(f"\nFetching first 10 rows from Iceberg table: {iceberg_table}")

        # Read the first 10 rows
        df = spark.read.format("iceberg").load(iceberg_table)

        # Show first 10 records
        df.show(10, truncate=False)

    except KeyError as e:
        print(f"[ERROR] KeyError encountered: {str(e)}")
        print(traceback.format_exc())
    except ValueError as e:
        print(f"[ERROR] ValueError encountered: {str(e)}")
        print(traceback.format_exc())
    except AttributeError as e:
        print(f"[ERROR] AttributeError encountered: {str(e)}")
        print(traceback.format_exc())
    except Exception as e:
        print(f"[ERROR] An unexpected error occurred while fetching records: {str(e)}")
        print(traceback.format_exc())

def main():
    role_arn = 'arn:aws:iam::691272445312:role/lancholers/services/AWSGlueServiceRole-90177-tf'
    session_name = 'GlueJobRunnerSession-123'
    region_name = 'us-east-1'
    catalog_id = '809616259930'
    database_name = 'common_data_dev'
    table_name = '87674_verum_application'

    try:
        # Initialize AWS Glue Client
        glue_client = boto3.client('glue', region_name=region_name)

        # Step 1: List Databases & Tables (Original Code)
        list_glue_databases_and_tables(glue_client, database_name, table_name, catalog_id)

        # Step 2: Fetch First 10 Rows (Using PySpark with Iceberg Support)
        fetch_first_10_rows(database_name, table_name, catalog_id)

    except Exception as e:
        print(f"[ERROR] An error occurred in main(): {str(e)}")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()
