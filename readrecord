import boto3
import sys
from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ConnectTimeoutError, ClientError
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

def list_glue_databases_and_tables(glue_client, database_name, table_name, catalog_id):
    try:
        # Retrieve the list of databases
        dbs = glue_client.get_databases(CatalogId=catalog_id)

        # Print the names of the databases
        print("Databases in AWS Glue Data Catalog:")
        for db in dbs['DatabaseList']:
            print(db['Name'])

        # Retrieve the list of tables in the specified database
        tables = glue_client.get_tables(DatabaseName=database_name, CatalogId=catalog_id)

        # Print the names of the tables
        print(f"\nTables in AWS Glue Database '{database_name}':")
        for table in tables['TableList']:
            print(table['Name'])

        print("\nFetching first 10 rows from table:", table_name)

    except NoCredentialsError:
        print("AWS credentials not found. Please configure your credentials.")
    except PartialCredentialsError:
        print("Incomplete AWS credentials. Please check your configuration.")
    except ConnectTimeoutError:
        print("Connection timed out. Please check your network connectivity.")
    except ClientError as e:
        print(f"Client error: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

def fetch_first_10_rows(database_name, table_name):
    """ Fetch the first 10 rows from an AWS Glue table using PySpark """

    try:
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init("FetchFirst10RowsJob", {})

        # Load table data
        datasource = glueContext.create_dynamic_frame.from_catalog(database=database_name, table_name=table_name)

        # Convert to Spark DataFrame
        df = datasource.toDF()

        # Show first 10 records
        print("\nFirst 10 records from table:", table_name)
        df.show(10)

        job.commit()

    except Exception as e:
        print(f"An error occurred while fetching records: {e}")

def main():
    role_arn = 'arn:aws:iam::691272445312:role/lancholers/services/AWSGlueServiceRole-90177-tf'
    session_name = 'GlueJobRunnerSession-123'
    region_name = 'us-east-1'
    catalog_id = '809616259930'
    database_name = 'common_data_dev'
    table_name = '87674_verum_application'

    # Initialize AWS Glue Client
    glue_client = boto3.client('glue', region_name=region_name)

    # Step 1: List Databases & Tables (Original Code)
    list_glue_databases_and_tables(glue_client, database_name, table_name, catalog_id)

    # Step 2: Fetch First 10 Rows (New PySpark Integration)
    fetch_first_10_rows(database_name, table_name)

if __name__ == "__main__":
    main()
